[
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "resources",
    "section": "",
    "text": "Rodrigues’ Reproducible Analytical Pipelines in R\nSamantha’s quarto website tutorial\nSamantha’s quarto blog posts tutorial\nMastering Shiny, by Hadley Wickham\nHappy Git and GitHub for the useR, by Jenny Bryan\nR for Data Science, by Hadley Wickham\nHitchhiker’s Guide to Python, by Kenneth Reitz & Tanya Schlusser\nData wrangling essentials: comparisons in JavaScript, Python, SQL, R, and Excel, by Allison Horst & Paul Buffa\nW3Schools, particularly for their HTML & CSS tutorials\nJayde’s parameterized reports with quarto\nMine’s quarto manuscripts"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Portfolio",
    "section": "",
    "text": "R package development\nTaking on package development inculcates in one best practices in programming: using a secluded development environment made possible by {renv} package and Rstudio’s project initialization functionality, documenting code, shared data and publishing user guides to get people started with the software. Package development workflow included {fussen}, {roxygen2}, and {devtools}\nThe packaged is named pbwrangler, it’s goal to document functions used in reading, processing and writing field experimental data in potato breeding.\n\n\nShiny web app\n{shiny} is undoubtedly a go to tool for building a web app that runs in production or just for presenting a proof of concept. I developed a small app as proof that I understand the underlying framework to be able to build an app that can be used in production.\n\n\nReproducible analytical pipelines in R\nThis was a ‘do it yourself too’ as I was reading an online version of Rodrigues’ reproducible piplines text . It reinforced my understanding of package development powered by {fusen}, reproducibility of package versions using {renv}, reproducible pipelines with {targets}, building and sharing docker containers in dockerhub and github, and continuous integration/development (CI/CD) using github actions. A branch with CI/CD running a docker container can be found here.\n\n\nEnd-to-end Machine Learning (MLflow + Docker + Google cloud)\nThis is an account of my learning journey aided by this tutorial to grasp the nitty-gritties of buliding, logging, saving and serving machine learning models. The code available at this repo and a write-up is here.\n\n\nWeb scraping: getting data from the internet\nI set out to understand how to scrape data using Python. I explored beautifulsoup, requests, scrapingBee API, and scrapy to scrape data from Google news and a product listing page. I wrote a piece about it too."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "Hello!\nYou’ve made it to my landing page! I am a student enrolled in a Master of Science degree in Statistics and data science, specializing in biostatistics. I use R, SAS, and Python for statistical analysis and visualizations. My interest is statistical computing applied to spatial statistics, infectious diseases modelling and Bayesian data analysis.\n\neducation\n\n\nMS in Biostatistics, 2020 - Ongoing|Hasselt University\n\n\n\nBS in Applied Statistics, 2015|Maseno University\n\n\n\n\n\nexperience\n\n\nBiometrician, 2024-present|International Potato Center (CIP)\n\n\nResearch Assistant, 2022-2023|Karolinska Institutet\n\n\n\nData Manager, 2022-2022|Kenya Medical Research Institute\n\n\n\nData Manager, 2018-2020|KEMRI - Wellcome Trust Research Programme"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "posts",
    "section": "",
    "text": "Web Scraping with Python\n\n\n\nQuarto\n\nPython\n\nbeautifulsoup\n\nscrapy\n\nscrapingBee\n\n\n\nScraping data from the web using {beautifulsoup4}, {requests}, {ScrapingBee}, and {Scrapy}\n\n\n\n\n\nAug 3, 2025\n\n\nBasil Okola\n\n\n\n\n\n\n\n\n\n\n\n\nEnd-to-end Machine Learning (MLflow + Docker + Google Cloud)\n\n\n\nQuarto\n\nPython\n\nMLflow\n\n\n\nMachine learning Workflow using MLflow locally and pushing image to google cloud\n\n\n\n\n\nAug 2, 2025\n\n\nBasil Okola\n\n\n\n\n\n\n\n\n\n\n\n\nR package development guide\n\n\n\nQuarto\n\nR\n\npackages\n\n\n\nWriting R packages using devtools, roxygen2 and usethis packages\n\n\n\n\n\nDec 16, 2023\n\n\nBasil Okola\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-12-16-R-package dev/index.html",
    "href": "posts/2023-12-16-R-package dev/index.html",
    "title": "R package development guide",
    "section": "",
    "text": "These are quick step-by-step guides I wrote when going through Andy’s fundamentas of package development notes.\n\nInitialize package dev files with devtools::create_package() which creates mandatory file for the package\nEnable git tracking:\n\nconfigure: usethis::use_git_configure()\ncommit: usethis::use_git()\n\nFill in sections of the DESCRIPTION file. {roxygen2} is used to actualize this.\nEdit.Rprofile() to set options you’ll use in the documentation such as author details. This uses usethis::edit_r_profile()\nCreate a separate R file for each function you’ll be including in the package. Use use_r(\"/path-to-file/R/function-name.R\")\nLoad the functions with devtools::load_all()\nCheck the loaded files with check() which runs R CMD checks to catch errors/warnings/notes that need addressing.\nAdd files you don’t wish to include in the package build-in .Rbuildignore file.\nEnable roxygen2 to be used for package documentation: project options -&gt; Build Tools -&gt; check to generate documentation with roxygen or better devtools::document() which generates NAMESPACE automatically\nAutomate external function imports with usethis::use_import_from(): example usethis::use_import_from(“utils”, “install.packages”)\nDocument functions: put the cursor inside the R function definition and ctrl+shift+alt+R to insert the roxygen skeleton. The workflow here is after documenting -&gt; load_all() -&gt; document() -&gt; check()\nData files go to /data dir. It should be of .rda format\nExternal (non .rda format) data go to /inst/extdata/. Document them in the R file e.g. data.R and store it in /R. load_all() then document()\nCall use_package_doc() to add a dummy .R file that will prompt roxygen to generate basic package-level documentation. I noticed doing this erased imports in {package-name}-package.R file. Add recommended imports (see 10) and check()\nInstall your package with devtools::install()\nAttach your package as with other packages by calling library()\nTesting: Using the edit code -&gt; load_all() -&gt; experiment iteration can be unsustainable if you come back to your code months after development. You should write formal tests supported by {testthat} package.\nSet up formal testing of your package with usethis::use_testthat(). Creates a folder /tests. Don’t edit tests/testhat.R\nCall usethis::use_test() e.g., use_test(\"install_load_packages.R\") to edit tests for functions living in a particular R file in R/.\ntest() or ctr + shift + T runs all tests in your test/ directory. The workflow updates to load_all() -&gt; test() -&gt; document() -&gt; check(). Tests should be small and run quickly.\nDependencies, add imports in DESCRIPTION with use_package().\nAdd README with use_readme_rmd()\nRender readme.rmd with build_readme()\nUse continuous integration with use_github_action() then build_readme() again 25 Build a website for your package with use_pkgdown_github_pages() then document().\n\n\n\n\nCitationBibTeX citation:@online{okola2023,\n  author = {Okola, Basil},\n  title = {R Package Development Guide},\n  date = {2023-12-16},\n  url = {https://bokola.github.io/posts/2023-12-16-R-package dev/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nOkola, Basil. 2023. “R Package Development Guide.” December\n16, 2023. https://bokola.github.io/posts/2023-12-16-R-package\ndev/."
  },
  {
    "objectID": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html",
    "href": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html",
    "title": "End-to-end Machine Learning (MLflow + Docker + Google Cloud)",
    "section": "",
    "text": "BibTeX citation:@online{okola2025,\n  author = {Okola, Basil},\n  title = {End-to-End {Machine} {Learning} {(MLflow} + {Docker} +\n    {Google} {Cloud)}},\n  date = {2025-08-02},\n  url = {https://bokola.github.io/posts/2025-08-02-End-to-end-ML-with-MLflow/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nOkola, Basil. 2025. “End-to-End Machine Learning (MLflow + Docker\n+ Google Cloud).” August 2, 2025. https://bokola.github.io/posts/2025-08-02-End-to-end-ML-with-MLflow/.\nThis is an account of my learning journey aided by this tutorial to grasp the nitty-gritties of buliding, logging, saving and serving machine learning models. First is a description of the development environment used."
  },
  {
    "objectID": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#development-environment",
    "href": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#development-environment",
    "title": "End-to-end Machine Learning (MLflow + Docker + Google Cloud)",
    "section": "Development Environment",
    "text": "Development Environment\nI am running Debian 24.04 LTS, and Pycharm IDE calling Python 3.12 within a .venv virtual environment. Since the model used is a Tensorflow neural network, I had to follow cuda documentation in setting up necessary drives. You also need to start mlflow ui local server by running mlflow ui --port 5000 in the terminal, install dependenices pip install mlflow[extras] hyperopt tensorflow scikit-learn pandas numpy, and set environment variable export MLFLOW_TRACKING_URI=http://localhost:5000."
  },
  {
    "objectID": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-1-data-preparation",
    "href": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-1-data-preparation",
    "title": "End-to-end Machine Learning (MLflow + Docker + Google Cloud)",
    "section": "Step 1 : Data Preparation",
    "text": "Step 1 : Data Preparation\nThe tutorial uses wine quality classification data.\n#prepare data\n\nimport os\nfrom dotenv import load_dotenv\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport tensorflow as tf\nfrom tensorflow import keras\nimport mlflow\nfrom mlflow.models import infer_signature\nfrom hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n\n#test prediction\nimport requests\nimport json\n\n#environment variables\nload_dotenv(\".env\")\n\nMLFLOW_TRACKING_URI=os.getenv(\"MLFLOW_TRACKING_URI\")\nXLA_FLAGS=os.getenv('XLA_FLAGS')\n\n#load data\ndata = pd.read_csv(\n    \"https://raw.githubusercontent.com/mlflow/mlflow/master/tests/datasets/winequality-white.csv\",\n    sep=\";\",\n)\n\ntrain, test = train_test_split(data, test_size=0.2, random_state=12)\ntrain_x = train.drop([\"quality\"], axis=1).values\ntrain_y = train[[\"quality\"]].values.ravel()\ntest_x = test.drop([\"quality\"], axis=1).values\ntest_y = test[[\"quality\"]].values.ravel()\n\n#further split training data for validation\ntrain_x, valid_x, train_y, valid_y = train_test_split(train_x, train_y, test_size=0.2, random_state=12)\n\n#Create model signature for deployment\nsignature = infer_signature(train_x, train_y)"
  },
  {
    "objectID": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#define-model-architecture",
    "href": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#define-model-architecture",
    "title": "End-to-end Machine Learning (MLflow + Docker + Google Cloud)",
    "section": "Define Model architecture",
    "text": "Define Model architecture\ndef create_and_train_model(learning_rate, momentum, epochs=10):\n    \"\"\"\n    Create and train a neural network with specified hyperparameters.\n\n    Returns:\n        dict: Training results including model and metrics\n    \"\"\"\n    # Normalize input features for better training stability\n    mean = np.mean(train_x, axis=0)\n    var = np.var(train_x, axis=0)\n\n    # Define model architecture\n    model = keras.Sequential(\n        [\n            keras.Input([train_x.shape[1]]),\n            keras.layers.Normalization(mean=mean, variance=var),\n            keras.layers.Dense(64, activation=\"relu\"),\n            keras.layers.Dropout(0.2),  # Add regularization\n            keras.layers.Dense(32, activation=\"relu\"),\n            keras.layers.Dense(1),\n        ]\n    )\n\n    # Compile with specified hyperparameters\n    model.compile(\n        optimizer=keras.optimizers.SGD(learning_rate=learning_rate, momentum=momentum),\n        loss=\"mean_squared_error\",\n        metrics=[keras.metrics.RootMeanSquaredError()],\n    )\n\n    # Train with early stopping for efficiency\n    early_stopping = keras.callbacks.EarlyStopping(\n        patience=3, restore_best_weights=True\n    )\n\n    # Train the model\n    history = model.fit(\n        train_x,\n        train_y,\n        validation_data=(valid_x, valid_y),\n        epochs=epochs,\n        batch_size=64,\n        callbacks=[early_stopping],\n        verbose=0,  # Reduce output for cleaner logs\n    )\n\n    # Evaluate on validation set\n    val_loss, val_rmse = model.evaluate(valid_x, valid_y, verbose=0)\n\n    return {\n        \"model\": model,\n        \"val_rmse\": val_rmse,\n        \"val_loss\": val_loss,\n        \"history\": history,\n        \"epochs_trained\": len(history.history[\"loss\"]),\n    }\n    ```\n\n## Step 3: Set up parameter optimization\n\ndef objective(params): ““” Objective function for hyperparameter optimization. This function will be called by Hyperopt for each trial. ““” with mlflow.start_run(nested=True): # Log hyperparameters being tested mlflow.log_params( { “learning_rate”: params[“learning_rate”], “momentum”: params[“momentum”], “optimizer”: “SGD”, “architecture”: “64-32-1”, } )\n    # Train model with current hyperparameters\n    result = create_and_train_model(\n        learning_rate=params[\"learning_rate\"],\n        momentum=params[\"momentum\"],\n        epochs=15,\n    )\n\n    # Log training results\n    mlflow.log_metrics(\n        {\n            \"val_rmse\": result[\"val_rmse\"],\n            \"val_loss\": result[\"val_loss\"],\n            \"epochs_trained\": result[\"epochs_trained\"],\n        }\n    )\n\n    # Log the trained model\n    mlflow.tensorflow.log_model(result[\"model\"], name=\"model\", signature=signature)\n\n    # Log training curves as artifacts\n    import matplotlib.pyplot as plt\n\n    plt.figure(figsize=(12, 4))\n\n    plt.subplot(1, 2, 1)\n    plt.plot(result[\"history\"].history[\"loss\"], label=\"Training Loss\")\n    plt.plot(result[\"history\"].history[\"val_loss\"], label=\"Validation Loss\")\n    plt.title(\"Model Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n\n    plt.subplot(1, 2, 2)\n    plt.plot(\n        result[\"history\"].history[\"root_mean_squared_error\"], label=\"Training RMSE\"\n    )\n    plt.plot(\n        result[\"history\"].history[\"val_root_mean_squared_error\"],\n        label=\"Validation RMSE\",\n    )\n    plt.title(\"Model RMSE\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"RMSE\")\n    plt.legend()\n\n    plt.tight_layout()\n    plt.savefig(\"training_curves.png\")\n    mlflow.log_artifact(\"training_curves.png\")\n    plt.close()\n\n    # Return loss for Hyperopt (it minimizes)\n    return {\"loss\": result[\"val_rmse\"], \"status\": STATUS_OK}"
  },
  {
    "objectID": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-5-analyze-results-in-the-mlflow-ui",
    "href": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-5-analyze-results-in-the-mlflow-ui",
    "title": "End-to-end Machine Learning (MLflow + Docker + Google Cloud)",
    "section": "Step 5: Analyze Results in the MLflow UI",
    "text": "Step 5: Analyze Results in the MLflow UI\n\nNavigate to your experiment → click on “wine-quality-optimization\nAdd key columns: click “columns and add:\n\nMetrics | val_rmse\nParameters | learning_rate\nParameters | momentum\n\nInterprete the visualization: blue lines - better performing runs; red lines - worse performing runs\nAlso take a look at the training curves:\n\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n# Specify the path to your image file\nimage_path = \"val_rmse.png\"\n\n# Read the image\nimg = mpimg.imread(image_path)\n\n# Display the image\nplt.imshow(img)\nplt.axis('off')  # Hide the axes\nplt.show()\n\n\n\n\n\n\n\n\n\n# Specify the path to your image file\nimage_path = \"training_curves.png\"\n\n# Read the image\nimg = mpimg.imread(image_path)\n\n# Display the image\nplt.imshow(img)\nplt.axis('off')  # Hide the axes\nplt.show()"
  },
  {
    "objectID": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-6-register-your-best-model",
    "href": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-6-register-your-best-model",
    "title": "End-to-end Machine Learning (MLflow + Docker + Google Cloud)",
    "section": "Step 6: Register your best model",
    "text": "Step 6: Register your best model\nTo find the best run: in the table view, click on the run with the lowest val_rmse then navigate to model artifacts and scroll to the “Artifacts” section. then register the model:\n- Go to \"Models\" tab in MLflow UI\n\n- Click on your registered model\n\n- Transition to \"Staging\" stage for testing\n\n- Add tags and descriptions as needed"
  },
  {
    "objectID": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-7-deploy-the-best-model",
    "href": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-7-deploy-the-best-model",
    "title": "End-to-end Machine Learning (MLflow + Docker + Google Cloud)",
    "section": "Step 7: Deploy the best model",
    "text": "Step 7: Deploy the best model\nTest your model with a REST API\n# Serve the model (choose the version number you registered)\nmlflow models serve -m \"models:/wine-quality-predictor/1\" --port 5002\nTest your deployment\n# Test with a sample wine\ncurl -X POST http://localhost:5002/invocations \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"dataframe_split\": {\n      \"columns\": [\n        \"fixed acidity\", \"volatile acidity\", \"citric acid\", \"residual sugar\",\n        \"chlorides\", \"free sulfur dioxide\", \"total sulfur dioxide\", \"density\",\n        \"pH\", \"sulphates\", \"alcohol\"\n      ],\n      \"data\": [[7.0, 0.27, 0.36, 20.7, 0.045, 45, 170, 1.001, 3.0, 0.45, 8.8]]\n    }\n  }'\n\n\nYou could also test with Python\n\nimport requests\nimport json\n\n# Prepare test data\ntest_wine = {\n    \"dataframe_split\": {\n        \"columns\": [\n            \"fixed acidity\",\n            \"volatile acidity\",\n            \"citric acid\",\n            \"residual sugar\",\n            \"chlorides\",\n            \"free sulfur dioxide\",\n            \"total sulfur dioxide\",\n            \"density\",\n            \"pH\",\n            \"sulphates\",\n            \"alcohol\",\n        ],\n        \"data\": [[7.0, 0.27, 0.36, 20.7, 0.045, 45, 170, 1.001, 3.0, 0.45, 8.8]],\n    }\n}\n\n# Make prediction request\nresponse = requests.post(\n    \"http://localhost:5002/invocations\",\n    headers={\"Content-Type\": \"application/json\"},\n    data=json.dumps(test_wine),\n)\n\nprediction = response.json()\nprint(f\"Predicted wine quality: {prediction['predictions'][0]:.2f}\")"
  },
  {
    "objectID": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-8-build-a-production-docker-container",
    "href": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-8-build-a-production-docker-container",
    "title": "End-to-end Machine Learning (MLflow + Docker + Google Cloud)",
    "section": "Step 8: Build a production Docker container",
    "text": "Step 8: Build a production Docker container\n# Build Docker image\nmlflow models build-docker \\\n  --model-uri \"models:/wine-quality-predictor/1\" \\\n  --name \"wine-quality-api\"\nTest your container:\n# Run the container\ndocker run -p 5003:8080 wine-quality-api\n\n# Test in another terminal\ncurl -X POST http://localhost:5003/invocations \\\n-H \"Content-Type: application/json\" \\\n-d '{\n  \"dataframe_split\": {\n    \"columns\": [\"fixed acidity\",\"volatile acidity\",\"citric acid\",\"residual sugar\",\"chlorides\",\"free sulfur dioxide\",\"total sulfur dioxide\",\"density\",\"pH\",\"sulphates\",\"alcohol\"],\n    \"data\": [[7.0, 0.27, 0.36, 20.7, 0.045, 45, 170, 1.001, 3.0, 0.45, 8.8]]\n  }\n}'"
  },
  {
    "objectID": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-9-deploy-to-google-cloud",
    "href": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-9-deploy-to-google-cloud",
    "title": "End-to-end Machine Learning (MLflow + Docker + Google Cloud)",
    "section": "Step 9: Deploy to Google cloud",
    "text": "Step 9: Deploy to Google cloud\n\nAuthentication and project set up\n$ gcloud auth login\nConfigure Docker for gcp $ gcloud auth configure-docker\nset project $ gcloud config set project PROJECT_ID\nIAM roles\n\nArtifacr registry Administrator\nroles/artifactregistry.createOnPushRepoAdmin\nStorage Administrator\n\nExport the credentials export GOOGLE_APPLICATION_CREDENTIALS=\"/path/to/your-service-account-file.json\"\nTag the docker image\n$ docker tag IMAGE_NAME gcr.io/PROJECT_ID/IMAGE_NAME:TAG\nPush the docker image to Google Cloud Container Registry $ docker push gcr.io/PROJECT_ID/IMAGE_NAME:TAG\n\n\n# Specify the path to your image file\nimage_path = \"gc-artifact-registry.png\"\n\n# Read the image\nimg = mpimg.imread(image_path)\n\n# Display the image\nplt.imshow(img)\nplt.axis('off')  # Hide the axes\nplt.show()\n\n\n\n\n\n\n\n\nsee"
  },
  {
    "objectID": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-2-define-model-architecture",
    "href": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-2-define-model-architecture",
    "title": "End-to-end Machine Learning (MLflow + Docker + Google Cloud)",
    "section": "Step 2: Define Model architecture",
    "text": "Step 2: Define Model architecture\n\ndef create_and_train_model(learning_rate, momentum, epochs=10):\n    \"\"\"\n    Create and train a neural network with specified hyperparameters.\n\n    Returns:\n        dict: Training results including model and metrics\n    \"\"\"\n    #Normalize input features for better training stability\n    mean = np.mean(train_x, axis=0)\n    var = np.var(train_x, axis=0)\n\n    #Define model architecture\n    model = keras.Sequential(\n        [\n            keras.Input([train_x.shape[1]]),\n            keras.layers.Normalization(mean=mean, variance=var),\n            keras.layers.Dense(64, activation=\"relu\"),\n            keras.layers.Dropout(0.2),  # Add regularization\n            keras.layers.Dense(32, activation=\"relu\"),\n            keras.layers.Dense(1),\n        ]\n    )\n\n    #Compile with specified hyperparameters\n    model.compile(\n        optimizer=keras.optimizers.SGD(learning_rate=learning_rate, momentum=momentum),\n        loss=\"mean_squared_error\",\n        metrics=[keras.metrics.RootMeanSquaredError()],\n    )\n\n    #Train with early stopping for efficiency\n    early_stopping = keras.callbacks.EarlyStopping(\n        patience=3, restore_best_weights=True\n    )\n\n    #Train the model\n    history = model.fit(\n        train_x,\n        train_y,\n        validation_data=(valid_x, valid_y),\n        epochs=epochs,\n        batch_size=64,\n        callbacks=[early_stopping],\n        verbose=0,  # Reduce output for cleaner logs\n    )\n\n    #Evaluate on validation set\n    val_loss, val_rmse = model.evaluate(valid_x, valid_y, verbose=0)\n\n    return {\n        \"model\": model,\n        \"val_rmse\": val_rmse,\n        \"val_loss\": val_loss,\n        \"history\": history,\n        \"epochs_trained\": len(history.history[\"loss\"]),\n    }"
  },
  {
    "objectID": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-3-set-up-parameter-optimization",
    "href": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-3-set-up-parameter-optimization",
    "title": "End-to-end Machine Learning (MLflow + Docker + Google Cloud)",
    "section": "Step 3: Set up parameter optimization",
    "text": "Step 3: Set up parameter optimization\n\ndef objective(params):\n    \"\"\"\n    Objective function for hyperparameter optimization.\n    This function will be called by Hyperopt for each trial.\n    \"\"\"\n    with mlflow.start_run(nested=True):\n        #Log hyperparameters being tested\n        mlflow.log_params(\n            {\n                \"learning_rate\": params[\"learning_rate\"],\n                \"momentum\": params[\"momentum\"],\n                \"optimizer\": \"SGD\",\n                \"architecture\": \"64-32-1\",\n            }\n        )\n\n        #Train model with current hyperparameters\n        result = create_and_train_model(\n            learning_rate=params[\"learning_rate\"],\n            momentum=params[\"momentum\"],\n            epochs=15,\n        )\n\n        #Log training results\n        mlflow.log_metrics(\n            {\n                \"val_rmse\": result[\"val_rmse\"],\n                \"val_loss\": result[\"val_loss\"],\n                \"epochs_trained\": result[\"epochs_trained\"],\n            }\n        )\n\n        #Log the trained model\n        mlflow.tensorflow.log_model(result[\"model\"], name=\"model\", signature=signature)\n\n        #Log training curves as artifacts\n        import matplotlib.pyplot as plt\n\n        plt.figure(figsize=(12, 4))\n\n        plt.subplot(1, 2, 1)\n        plt.plot(result[\"history\"].history[\"loss\"], label=\"Training Loss\")\n        plt.plot(result[\"history\"].history[\"val_loss\"], label=\"Validation Loss\")\n        plt.title(\"Model Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.legend()\n\n        plt.subplot(1, 2, 2)\n        plt.plot(\n            result[\"history\"].history[\"root_mean_squared_error\"], label=\"Training RMSE\"\n        )\n        plt.plot(\n            result[\"history\"].history[\"val_root_mean_squared_error\"],\n            label=\"Validation RMSE\",\n        )\n        plt.title(\"Model RMSE\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"RMSE\")\n        plt.legend()\n\n        plt.tight_layout()\n        plt.savefig(\"training_curves.png\")\n        mlflow.log_artifact(\"training_curves.png\")\n        plt.close()\n\n        #Return loss for Hyperopt (it minimizes)\n        return {\"loss\": result[\"val_rmse\"], \"status\": STATUS_OK}\n\n\n#Define search space for hyperparameters\nsearch_space = {\n    \"learning_rate\": hp.loguniform(\"learning_rate\", np.log(1e-5), np.log(1e-1)),\n    \"momentum\": hp.uniform(\"momentum\", 0.0, 0.9),\n}\n\nprint(\"Search space defined:\")\nprint(\"- Learning rate: 1e-5 to 1e-1 (log-uniform)\")\nprint(\"- Momentum: 0.0 to 0.9 (uniform)\")"
  },
  {
    "objectID": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-4-run-the-hyperparameter-optimization",
    "href": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-4-run-the-hyperparameter-optimization",
    "title": "End-to-end Machine Learning (MLflow + Docker + Google Cloud)",
    "section": "Step 4: Run the hyperparameter optimization",
    "text": "Step 4: Run the hyperparameter optimization\n\n#Create or set experiment\nexperiment_name = \"wine-quality-optimization\"\nmlflow.set_experiment(experiment_name)\n\nprint(f\"Starting hyperparameter optimization experiment: {experiment_name}\")\nprint(\"This will run 15 trials to find optimal hyperparameters...\")\n\nwith mlflow.start_run(run_name=\"hyperparameter-sweep\"):\n    #Log experiment metadata\n    mlflow.log_params(\n        {\n            \"optimization_method\": \"Tree-structured Parzen Estimator (TPE)\",\n            \"max_evaluations\": 15,\n            \"objective_metric\": \"validation_rmse\",\n            \"dataset\": \"wine-quality\",\n            \"model_type\": \"neural_network\",\n        }\n    )\n\n    #Run optimization\n    trials = Trials()\n    best_params = fmin(\n        fn=objective,\n        space=search_space,\n        algo=tpe.suggest,\n        max_evals=15,\n        trials=trials,\n        verbose=True,\n    )\n\n    #Find and log best results\n    best_trial = min(trials.results, key=lambda x: x[\"loss\"])\n    best_rmse = best_trial[\"loss\"]\n\n    #Log optimization results\n    mlflow.log_params(\n        {\n            \"best_learning_rate\": best_params[\"learning_rate\"],\n            \"best_momentum\": best_params[\"momentum\"],\n        }\n    )\n    mlflow.log_metrics(\n        {\n            \"best_val_rmse\": best_rmse,\n            \"total_trials\": len(trials.trials),\n            \"optimization_completed\": 1,\n        }\n    )"
  },
  {
    "objectID": "posts/2025-08-03-Web-scraping-python/index.html",
    "href": "posts/2025-08-03-Web-scraping-python/index.html",
    "title": "Web Scraping with Python",
    "section": "",
    "text": "BibTeX citation:@online{okola2025,\n  author = {Okola, Basil},\n  title = {Web {Scraping} with {Python}},\n  date = {2025-08-03},\n  url = {https://bokola.github.io/posts/2025-08-03-Web-scraping-python/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nOkola, Basil. 2025. “Web Scraping with Python.” August 3,\n2025. https://bokola.github.io/posts/2025-08-03-Web-scraping-python/.\nThis is a web scrapping task to find conflict/war related news articles from the internet. There is been quite a lot of that considering the Russia/Ukraine conflict, The South Sudan conflict, and the Palestine/Israel conflict just to mention the most reported cases."
  },
  {
    "objectID": "posts/2025-08-03-Web-scraping-python/index.html#using-beautifulsoup-requests",
    "href": "posts/2025-08-03-Web-scraping-python/index.html#using-beautifulsoup-requests",
    "title": "Web Scraping with Python",
    "section": "Using Beautifulsoup + requests",
    "text": "Using Beautifulsoup + requests\n\nUnderstanding website’s structure\nPrior to scraping inspect the HTML source code of the web page to identify the elements you want to scrape\n\n\nSet up your develpment environment\nCreate a virtual environment, follow prompts per your IDE. For VScode I pressed Ctrl+Shift+Pthen searched Python: Create Environment A beginner web scraper in Python is advised to start with requests and beautifulsoup4 librarires which is what we will use. \nimport requests\nfrom bs4 import BeautifulSoup\n\nbaseurl = \"https://news.ycombinator.com\"\nuser = \"\"\npassd = \"\"\n\ns = requests.Session()\ndata = {\"goto\": \"news\", \"acct\": user, \"pw\": passd}\nr = s.post(f'{baseurl}', data=data)\n\nsoup = BeautifulSoup(r.text, 'html.parser')\nif soup.find(id='logout') is not None:\n    print(\"Successfully logged in\")\nelse:\n    print(\"Authentication error\")\n\n\nInspect HTML element\nEach post is wrapped in a &lt;tr&gt; tag with the class athing\n\nimport matplotlib.pyplot as plt\n# import matplotlib\n# matplotlib.use('TkAgg') # Use a non-interactive backend for saving figures\n\n\nimport matplotlib.image as mpimg\nimage_path = \"hacker-element.png\"\n\nimage = mpimg.imread(image_path)\nplt.imshow(image)\nplt.axis('off')  # Hide axes\nplt.show()\n\n\n\n\n\n\n\n\n\n\nScrape with requests + beautifulsoup4\n\nimport requests\nfrom bs4 import BeautifulSoup\n\nr = requests.get(\"https://news.ycombinator.com/\")\nsoup = BeautifulSoup(r.text, 'html.parser')\nlinks = soup.find_all('tr', class_='athing')\n\nformatted_links = []\nfor link in links:\n    data = {\n        'id': link['id'],\n        'title': link.find_all(\"td\")[2].a.text,\n        'url': link.find_all(\"td\")[2].a['href'],\n        'rank': int(link.find_all(\"td\")[0].span.text.replace('.', ''))\n    }\n    formatted_links.append(data)\n\n\n    formatted_links.append(data)\n    \nprint(formatted_links)\n\n[{'id': '44777760', 'title': 'Persona vectors: Monitoring and controlling character traits in language models', 'url': 'https://www.anthropic.com/research/persona-vectors', 'rank': 1}, {'id': '44777760', 'title': 'Persona vectors: Monitoring and controlling character traits in language models', 'url': 'https://www.anthropic.com/research/persona-vectors', 'rank': 1}, {'id': '44778936', 'title': 'Modern Node.js Patterns', 'url': 'https://kashw1n.com/blog/nodejs-2025/', 'rank': 2}, {'id': '44778936', 'title': 'Modern Node.js Patterns', 'url': 'https://kashw1n.com/blog/nodejs-2025/', 'rank': 2}, {'id': '44777869', 'title': 'UN report finds UN reports are not widely read', 'url': 'https://www.reuters.com/world/un-report-finds-united-nations-reports-are-not-widely-read-2025-08-01/', 'rank': 3}, {'id': '44777869', 'title': 'UN report finds UN reports are not widely read', 'url': 'https://www.reuters.com/world/un-report-finds-united-nations-reports-are-not-widely-read-2025-08-01/', 'rank': 3}, {'id': '44775563', 'title': \"If you're remote, ramble\", 'url': 'https://stephango.com/ramblings', 'rank': 4}, {'id': '44775563', 'title': \"If you're remote, ramble\", 'url': 'https://stephango.com/ramblings', 'rank': 4}, {'id': '44778764', 'title': 'ChatGPT chats were indexed then removed from search but still remain online', 'url': 'https://growtika.com/chatgpt-shared-chats-seo-indexing-privacy-leak/', 'rank': 5}, {'id': '44778764', 'title': 'ChatGPT chats were indexed then removed from search but still remain online', 'url': 'https://growtika.com/chatgpt-shared-chats-seo-indexing-privacy-leak/', 'rank': 5}, {'id': '44736025', 'title': 'Helsinki records zero traffic deaths for full year', 'url': 'https://www.helsinkitimes.fi/finland/finland-news/domestic/27539-helsinki-records-zero-traffic-deaths-for-full-year.html', 'rank': 6}, {'id': '44736025', 'title': 'Helsinki records zero traffic deaths for full year', 'url': 'https://www.helsinkitimes.fi/finland/finland-news/domestic/27539-helsinki-records-zero-traffic-deaths-for-full-year.html', 'rank': 6}, {'id': '44774104', 'title': 'Twenty Eighth International Obfuscated C Code Contest', 'url': 'https://www.ioccc.org/2024/index.html', 'rank': 7}, {'id': '44774104', 'title': 'Twenty Eighth International Obfuscated C Code Contest', 'url': 'https://www.ioccc.org/2024/index.html', 'rank': 7}, {'id': '44745441', 'title': \"2,500-year-old Siberian 'ice mummy' had intricate tattoos, imaging reveals\", 'url': 'https://www.bbc.com/news/articles/c4gzx0zm68vo', 'rank': 8}, {'id': '44745441', 'title': \"2,500-year-old Siberian 'ice mummy' had intricate tattoos, imaging reveals\", 'url': 'https://www.bbc.com/news/articles/c4gzx0zm68vo', 'rank': 8}, {'id': '44776434', 'title': 'The Fulbright Program: Chock Full of Bright Ideas', 'url': 'https://bastian.rieck.me/blog/2025/fulbright/', 'rank': 9}, {'id': '44776434', 'title': 'The Fulbright Program: Chock Full of Bright Ideas', 'url': 'https://bastian.rieck.me/blog/2025/fulbright/', 'rank': 9}, {'id': '44777965', 'title': 'Converge (YC S23) well-capitalized New York startup seeks product developers', 'url': 'https://www.runconverge.com/careers', 'rank': 10}, {'id': '44777965', 'title': 'Converge (YC S23) well-capitalized New York startup seeks product developers', 'url': 'https://www.runconverge.com/careers', 'rank': 10}, {'id': '44778087', 'title': 'Yosemite embodies the long war over US national park privatization', 'url': 'https://theconversation.com/yosemite-embodies-the-long-war-over-us-national-park-privatization-261133', 'rank': 11}, {'id': '44778087', 'title': 'Yosemite embodies the long war over US national park privatization', 'url': 'https://theconversation.com/yosemite-embodies-the-long-war-over-us-national-park-privatization-261133', 'rank': 11}, {'id': '44738228', 'title': 'The Subway Game (1980)', 'url': 'https://www.gricer.com/subway_game/subway_game.html', 'rank': 12}, {'id': '44738228', 'title': 'The Subway Game (1980)', 'url': 'https://www.gricer.com/subway_game/subway_game.html', 'rank': 12}, {'id': '44775830', 'title': 'How to make almost anything (2019)', 'url': 'https://fab.cba.mit.edu/classes/863.19/CBA/people/dsculley/index.html', 'rank': 13}, {'id': '44775830', 'title': 'How to make almost anything (2019)', 'url': 'https://fab.cba.mit.edu/classes/863.19/CBA/people/dsculley/index.html', 'rank': 13}, {'id': '44774567', 'title': 'A Real PowerBook: The Macintosh Application Environment on a Pa-RISC Laptop', 'url': 'http://oldvcr.blogspot.com/2025/08/a-real-powerbook-macintosh-application.html', 'rank': 14}, {'id': '44774567', 'title': 'A Real PowerBook: The Macintosh Application Environment on a Pa-RISC Laptop', 'url': 'http://oldvcr.blogspot.com/2025/08/a-real-powerbook-macintosh-application.html', 'rank': 14}, {'id': '44775485', 'title': \"EHRs: The hidden distraction in your doctor's office\", 'url': 'https://spectrum.ieee.org/electronic-health-records', 'rank': 15}, {'id': '44775485', 'title': \"EHRs: The hidden distraction in your doctor's office\", 'url': 'https://spectrum.ieee.org/electronic-health-records', 'rank': 15}, {'id': '44769039', 'title': 'Telo MT1', 'url': 'https://www.telotrucks.com/', 'rank': 16}, {'id': '44769039', 'title': 'Telo MT1', 'url': 'https://www.telotrucks.com/', 'rank': 16}, {'id': '44746621', 'title': '6 weeks of Claude Code', 'url': 'https://blog.puzzmo.com/posts/2025/07/30/six-weeks-of-claude-code/', 'rank': 17}, {'id': '44746621', 'title': '6 weeks of Claude Code', 'url': 'https://blog.puzzmo.com/posts/2025/07/30/six-weeks-of-claude-code/', 'rank': 17}, {'id': '44748226', 'title': 'Watching the World in a Dark Room: The Early Modern Camera Obscura', 'url': 'https://publicdomainreview.org/essay/the-early-modern-camera-obscura', 'rank': 18}, {'id': '44748226', 'title': 'Watching the World in a Dark Room: The Early Modern Camera Obscura', 'url': 'https://publicdomainreview.org/essay/the-early-modern-camera-obscura', 'rank': 18}, {'id': '44737738', 'title': 'Flourishing chemosynthetic life at the greatest depths of hadal trenches', 'url': 'https://www.nature.com/articles/s41586-025-09317-z', 'rank': 19}, {'id': '44737738', 'title': 'Flourishing chemosynthetic life at the greatest depths of hadal trenches', 'url': 'https://www.nature.com/articles/s41586-025-09317-z', 'rank': 19}, {'id': '44771808', 'title': 'Lina Khan points to Figma IPO as vindication of M&A scrutiny', 'url': 'https://techcrunch.com/2025/08/02/lina-khan-points-to-figma-ipo-as-vindication-for-ma-scrutiny/', 'rank': 20}, {'id': '44771808', 'title': 'Lina Khan points to Figma IPO as vindication of M&A scrutiny', 'url': 'https://techcrunch.com/2025/08/02/lina-khan-points-to-figma-ipo-as-vindication-for-ma-scrutiny/', 'rank': 20}, {'id': '44747000', 'title': 'A 3D model of the human airways via a digital light processing bioprinter', 'url': 'https://analyticalsciencejournals.onlinelibrary.wiley.com/doi/10.1002/bit.29013', 'rank': 21}, {'id': '44747000', 'title': 'A 3D model of the human airways via a digital light processing bioprinter', 'url': 'https://analyticalsciencejournals.onlinelibrary.wiley.com/doi/10.1002/bit.29013', 'rank': 21}, {'id': '44777055', 'title': 'This Old SGI: notes and memoirs on the Silicon Graphics 4D series (1996)', 'url': 'https://archive.irixnet.org/thisoldsgi/', 'rank': 22}, {'id': '44777055', 'title': 'This Old SGI: notes and memoirs on the Silicon Graphics 4D series (1996)', 'url': 'https://archive.irixnet.org/thisoldsgi/', 'rank': 22}, {'id': '44773998', 'title': 'The Algebra Gatekeepers', 'url': 'https://www.educationprogress.org/p/the-algebra-gatekeepers', 'rank': 23}, {'id': '44773998', 'title': 'The Algebra Gatekeepers', 'url': 'https://www.educationprogress.org/p/the-algebra-gatekeepers', 'rank': 23}, {'id': '44747786', 'title': 'My bytecode optimizer beats Copilot by 2x', 'url': 'https://deviantabstraction.com/2025/07/29/how-my-bytecode-optimizer-beats-copilot-by-2x/', 'rank': 24}, {'id': '44747786', 'title': 'My bytecode optimizer beats Copilot by 2x', 'url': 'https://deviantabstraction.com/2025/07/29/how-my-bytecode-optimizer-beats-copilot-by-2x/', 'rank': 24}, {'id': '44739944', 'title': 'Micron rolls out 276-layer SSD trio for speed, scale, and stability', 'url': 'https://blocksandfiles.com/2025/07/30/micron-three-276-layer-ssds/', 'rank': 25}, {'id': '44739944', 'title': 'Micron rolls out 276-layer SSD trio for speed, scale, and stability', 'url': 'https://blocksandfiles.com/2025/07/30/micron-three-276-layer-ssds/', 'rank': 25}, {'id': '44779169', 'title': 'The Dollar Is Dead', 'url': 'https://mathmeetsmoney.substack.com/p/the-dollar-is-dead', 'rank': 26}, {'id': '44779169', 'title': 'The Dollar Is Dead', 'url': 'https://mathmeetsmoney.substack.com/p/the-dollar-is-dead', 'rank': 26}, {'id': '44746583', 'title': 'PixiEditor 2.0 – A FOSS universal 2D graphics editor', 'url': 'https://pixieditor.net/blog/2025/07/30/20-release/', 'rank': 27}, {'id': '44746583', 'title': 'PixiEditor 2.0 – A FOSS universal 2D graphics editor', 'url': 'https://pixieditor.net/blog/2025/07/30/20-release/', 'rank': 27}, {'id': '44766962', 'title': 'At a Loss for Words: A flawed idea is teaching kids to be poor readers (2019)', 'url': 'https://www.apmreports.org/episode/2019/08/22/whats-wrong-how-schools-teach-reading', 'rank': 28}, {'id': '44766962', 'title': 'At a Loss for Words: A flawed idea is teaching kids to be poor readers (2019)', 'url': 'https://www.apmreports.org/episode/2019/08/22/whats-wrong-how-schools-teach-reading', 'rank': 28}, {'id': '44730544', 'title': 'Online Collection of Keygen Music', 'url': 'https://keygenmusic.tk', 'rank': 29}, {'id': '44730544', 'title': 'Online Collection of Keygen Music', 'url': 'https://keygenmusic.tk', 'rank': 29}, {'id': '44736854', 'title': 'Build Your Own Minisforum N5 Inspired Mini NAS: A Comprehensive Guide', 'url': 'https://jackharvest.com/index.php/2025/07/27/build-your-own-minisforum-n5-inspired-mini-nas-a-comprehensive-guide/', 'rank': 30}, {'id': '44736854', 'title': 'Build Your Own Minisforum N5 Inspired Mini NAS: A Comprehensive Guide', 'url': 'https://jackharvest.com/index.php/2025/07/27/build-your-own-minisforum-n5-inspired-mini-nas-a-comprehensive-guide/', 'rank': 30}]\n\n\n\n\nStore data as .csv\n\nimport csv\n\nfile = 'hacker_news_posts.csv'\nwith open(file, 'w', newline=\"\") as f:\n    writer = csv.DictWriter(f, fieldnames=['id', 'title', 'url', 'rank'])\n    writer.writeheader()\n    for row in formatted_links:\n        writer.writerow(row)\n\n\n\nStore data in PostgreSQL\n\nStep 1: Installing PostgreSQL\nFollow the PostgreSQL download page for downloads and installation\n\n\nStep 2: Creating a Database Table\nFirst you’ll need a table\n\n#start service\nsudo systemctl start postgresql.service\n\n#log in as a superuser\nsudo -i -u postgres\n\nCREATE DATABASE scrape_demo;\n\n\nCREATE TABLE \"hn_links\" (\n    \"id\" INTEGER NOT NULL,\n    \"title\" VARCHAR NOT NULL,\n    \"url\" VARCHAR NOT NULL,\n    \"rank\" INTEGER NOT NULL\n);\n\n\nStep 3: Install Psycopg2 to Connect to PostgreSQL\npip install psycopg2\nEstablish connection to the database\nEnsure you set password for postgres user, which logs without a password by default\n\nsudo -u postgres psql\n\npostgres=# ALTER USER postgres PASSWORD 'myPassword';\nALTER ROLE\n\n\nimport psycopg2\nimport os\nimport dotenv\ndotenv.load_dotenv()\n\np = os.getenv(\"pass\")\ntable_name = \"hn_links\"\ncsv_path = \"hacker_news_posts.csv\"\n\n\ncon = psycopg2.connect(host=\"127.0.0.1\", port=\"5432\", user=\"postgres\", password = p,database=\"scrape_demo\")\n\n# Get a database cursor\ncur = con.cursor()\n\nr = requests.get('https://news.ycombinator.com')\nsoup = BeautifulSoup(r.text, 'html.parser')\nlinks = soup.findAll('tr', class_='athing')\n\nfor link in links:\n    cur.execute(\"\"\"\n        INSERT INTO hn_links (id, title, url, rank)\n        VALUES (%s, %s, %s, %s)\n        \"\"\",\n        (\n            link['id'],\n            link.find_all('td')[2].a.text,\n            link.find_all('td')[2].a['href'],\n            int(link.find_all('td')[0].span.text.replace('.', ''))\n        )\n    )\n\n# Commit the data\ncon.commit()\n\n# Close our database connections\ncur.close()\ncon.close()\n\n/tmp/ipykernel_137379/1184676145.py:18: DeprecationWarning: Call to deprecated method findAll. (Replaced by find_all) -- Deprecated since version 4.0.0.\n  links = soup.findAll('tr', class_='athing')"
  },
  {
    "objectID": "posts/2025-08-03-Web-scraping-python/index.html#using-scapingbee-python-client",
    "href": "posts/2025-08-03-Web-scraping-python/index.html#using-scapingbee-python-client",
    "title": "Web Scraping with Python",
    "section": "Using ScapingBee Python Client",
    "text": "Using ScapingBee Python Client\nScrapingBee is a subscription API providing a way to bypass any website’s anti-scraping measures.\n\nfrom scrapingbee import ScrapingBeeClient\nimport json\nimport pandas as pd\n\ndotenv.load_dotenv()\nkey = os.getenv(\"spring_bee_api_key\")\n\nsb_client = ScrapingBeeClient(api_key=key)\nurl = \"https://www.aljazeera.com/\"\n\n\n\nclient = ScrapingBeeClient(api_key=key)\n\ndef google_news_headlines_api(country_code='US'):\n\n    extract_rules = {\n        \"news\": {\n        \"selector\": \"article\",\n        \"type\": \"list\",\n            \"output\": {\n                \"title\": \".gPFEn,.JtKRv\",\n                \"source\": \".vr1PYe\",\n                \"time\": \"time@datetime\",\n                \"author\": \".bInasb\",\n                \"link\": \".WwrzSb@href\"\n            }\n        }\n    }\n\n    js_scenario = {\n        \"instructions\":[\n            {\"evaluate\":\"document.querySelectorAll('.WwrzSb').forEach( (e) =&gt; e.href = e.href );\"}\n        ]\n    }\n\n    response =  client.get(\n        f'https://news.google.com/topics/CAAqJggKIiBDQkFTRWdvSUwyMHZNRFZxYUdjU0FtVnVHZ0pWVXlnQVAB?&gl={country_code}',\n        params={ \n            \"custom_google\": \"true\",\n            \"wait_for\": \".bInasb\",\n            \"extract_rules\": extract_rules,\n            \"js_scenario\": js_scenario, \n        },\n        retries=2\n    )\n\n    if response.text.startswith('{\"message\":\"Invalid api key:'):\n        return f\"Oops! It seems you may have missed adding your API KEY or you are using an incorrect key.\\nGet your free API KEY and 1000 free scraping credits by signing up to our platform here: https://app.scrapingbee.com/account/register\"\n    else:\n        def get_info():\n            if len(response.json()['news']) == 0:\n                return \"FAILED TO RETRIEVE NEWS\"\n            else:\n                return \"SUCCESS\"\n\n        return pd.DataFrame({\n            'count': len(response.json()['news']),\n            'news_extracts': response.json()['news'],\n            'info': f\"{response.status_code} {get_info()}\",\n        })\n#country_code: Set the news location; US, IN, etc.\ndf = google_news_headlines_api(country_code='US')\n\nprint(df.iloc[:10])\n\n   count                                      news_extracts         info\n0    262  {'title': 'Trump Live Updates: White House Def...  200 SUCCESS\n1    262  {'title': 'White House officials rush to defen...  200 SUCCESS\n2    262  {'title': 'Larry Summers says Trump's accusati...  200 SUCCESS\n3    262  {'title': 'Erika McEntarfer, The Economist Tru...  200 SUCCESS\n4    262  {'title': 'Eric Holder backs Democratic respon...  200 SUCCESS\n5    262  {'title': 'Will Texas be as 'gerrymandered' as...  200 SUCCESS\n6    262  {'title': 'Texas House panel passes new redist...  200 SUCCESS\n7    262  {'title': 'Eric Holder on Why He Reversed Cour...  200 SUCCESS\n8    262  {'title': 'Earthquake in Bergen County shakes ...  200 SUCCESS\n9    262  {'title': 'Earthquake Jolts New Jersey and New...  200 SUCCESS"
  },
  {
    "objectID": "posts/2025-08-03-Web-scraping-python/index.html#web-scraping-with-scrapy",
    "href": "posts/2025-08-03-Web-scraping-python/index.html#web-scraping-with-scrapy",
    "title": "Web Scraping with Python",
    "section": "Web scraping with Scrapy",
    "text": "Web scraping with Scrapy\nScrapy is a web scraping framework using an event-driven networking infrastracture built around an asynchronous network engine that allows for more efficiency and scalability. It is made of a crawler that handles low-level logic, and a spider that is provider by the user to help the crawler generate request, parse and retrieve data.\nIn this section we use scrapy to scrape product listings available at web-scraping.dev, but first some house-keeping.\nTo install scrapy run pip install scrapy or better still add scrapy to your project’s requirements.txt and run pip install -r requirements.txt. Start a scrapy project by running scrapy startproject &lt;project-name&gt; &lt;project-directory&gt; in terminal. Some scrapy commands below:\n\n!scrapy --help\n\nScrapy 2.13.3 - active project: webscrapingdev\n\nUsage:\n  scrapy &lt;command&gt; [options] [args]\n\nAvailable commands:\n  bench         Run quick benchmark test\n  check         Check spider contracts\n  crawl         Run a spider\n  edit          Edit spider\n  fetch         Fetch a URL using the Scrapy downloader\n  genspider     Generate new spider using pre-defined templates\n  list          List available spiders\n  parse         Parse URL (using its spider) and print the results\n  runspider     Run a self-contained spider (without creating a project)\n  settings      Get settings values\n  shell         Interactive scraping console\n  startproject  Create new project\n  version       Print Scrapy version\n  view          Open URL in browser, as seen by Scrapy\n\nUse \"scrapy &lt;command&gt; -h\" to see more info about a command\n\n\n\nCreating a spider\nrun scrapy genspider &lt;name&gt; &lt;host-to-scrape&gt;\n\n!scrapy genspider products web-scraping.dev\n\nSpider 'products' already exists in module:\n  webscrapingdev.spiders.products\n\n\n\n!scrapy list\n!tree\n\n\nproducts\n\n.\n\n├── article-class.png\n\n├── hacker-element.png\n\n├── hacker_news_posts.csv\n\n├── LICENSE\n\n├── producthunt.json\n\n├── README.md\n\n├── requirements.txt\n\n├── results.json\n\n├── scrapy.cfg\n\n├── webscrapingdev\n\n│   ├── __init__.py\n\n│   ├── items.py\n\n│   ├── middlewares.py\n\n│   ├── pipelines.py\n\n│   ├── __pycache__\n\n│   │   ├── __init__.cpython-312.pyc\n\n│   │   └── settings.cpython-312.pyc\n\n│   ├── settings.py\n\n│   └── spiders\n\n│       ├── __init__.py\n\n│       ├── products.py\n\n│       └── __pycache__\n\n│           ├── __init__.cpython-312.pyc\n\n│           └── products.cpython-312.pyc\n\n├── web-scrap.ipynb\n\n├── web-scrap.py\n\n├── web-scrap.qmd\n\n└── web-scrap.quarto_ipynb\n\n\n\n5 directories, 24 files\n\n\n\n\nif you open the generated spider - products.py, you’ll find the following\nimport scrapy\n\n\nclass ProductsSpider(scrapy.Spider):\n    name = \"products\"\n    allowed_domains = [\"web-scraping.dev\"]\n    start_urls = [\"https://web-scraping.dev\"]\n\n    def parse(self, response):\n        pass\n\n\nname is used as a reference to the spider for scrapy commands like crawl` - this would run the scraper\nallowed_domains is a safety feauture restricting this spider to crawl only particular domains.\nstart_urls indicates the spider starting point while parse() is the first callback to execute above instructions.\n\n\n\nAdding crawling logic\nWe want our start_urls to be some topic directories e.g., https://www.producthunt.com/topics/developer-tools and our parse() callback method to find all product links and schedule them to be scrapped:\n# /spiders/products.py\nimport scrapy\nfrom scrapy.http import Response, Request\n\n\nclass ProductsSpider(scrapy.Spider):\n    name = 'products'\n    allowed_domains = ['web-scraping.dev']\n    start_urls = [\n        'https://web-scraping.dev/products',\n    ]\n\n    def parse(self, response: Response):\n        product_urls = response.xpath(\n            \"//div[@class='row product']/div/h3/a/@href\"\n        ).getall()\n        for url in product_urls:\n            yield Request(url, callback=self.parse_product)\n        # or shortcut in scrapy &gt;2.0\n        # yield from response.follow_all(product_urls, callback=self.parse_product)\n    \n    def parse_product(self, response: Response):\n        print(response)\n\n\n\nAdding Parsing Logic\nPopulate parse_product()\n# /spiders/products.py\n...\n\n    def parse_product(self, response: Response):\n        yield {\n            \"title\": response.xpath(\"//h3[contains(@class, 'product-title')]/text()\").get(),\n            \"price\": response.xpath(\"//span[contains(@class, 'product-price')]/text()\").get(),\n            \"image\": response.xpath(\"//div[contains(@class, 'product-image')]/img/@src\").get(),\n            \"description\": response.xpath(\"//p[contains(@class, 'description')]/text()\").get()\n        }\n\n\n\nBasic Settings\nAdjust recommended settings:\n# settings.py\n# will ignore /robots.txt rules that might prevent scraping\nROBOTSTXT_OBEY = False\n# will cache all request to /httpcache directory which makes running spiders in development much quicker\n# tip: to refresh cache just delete /httpcache directory\nHTTPCACHE_ENABLED = True\n# while developing we want to see debug logs\nLOG_LEVEL = \"DEBUG\" # or \"INFO\" in production\n\n# to avoid basic bot detection we want to set some basic headers\nDEFAULT_REQUEST_HEADERS = {\n    # we should use headers\n    'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\",\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\n\nRunning Spiders\nEither through the scrapy command or explicitly calling scrapy using a Python script.\n\n!scrapy crawl products\n\n2025-08-03 23:05:30 [scrapy.utils.log] INFO: Scrapy 2.13.3 started (bot: webscrapingdev)\n2025-08-03 23:05:30 [scrapy.utils.log] INFO: Versions:\n{'lxml': '6.0.0',\n 'libxml2': '2.14.4',\n 'cssselect': '1.3.0',\n 'parsel': '1.10.0',\n 'w3lib': '2.3.1',\n 'Twisted': '25.5.0',\n 'Python': '3.12.3 (main, Jun 18 2025, 17:59:45) [GCC 13.3.0]',\n 'pyOpenSSL': '25.1.0 (OpenSSL 3.5.1 1 Jul 2025)',\n 'cryptography': '45.0.5',\n 'Platform': 'Linux-6.14.0-27-generic-x86_64-with-glibc2.39'}\n2025-08-03 23:05:30 [scrapy.addons] INFO: Enabled addons:\n[]\n2025-08-03 23:05:30 [asyncio] DEBUG: Using selector: EpollSelector\n2025-08-03 23:05:30 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n2025-08-03 23:05:30 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n2025-08-03 23:05:30 [scrapy.extensions.telnet] INFO: Telnet Password: 54a208c5e6d0eeb0\n2025-08-03 23:05:30 [scrapy.middleware] INFO: Enabled extensions:\n['scrapy.extensions.corestats.CoreStats',\n 'scrapy.extensions.telnet.TelnetConsole',\n 'scrapy.extensions.memusage.MemoryUsage',\n 'scrapy.extensions.feedexport.FeedExporter',\n 'scrapy.extensions.logstats.LogStats']\n2025-08-03 23:05:30 [scrapy.crawler] INFO: Overridden settings:\n{'BOT_NAME': 'webscrapingdev',\n 'CONCURRENT_REQUESTS_PER_DOMAIN': 1,\n 'DOWNLOAD_DELAY': 1,\n 'FEED_EXPORT_ENCODING': 'utf-8',\n 'HTTPCACHE_ENABLED': True,\n 'NEWSPIDER_MODULE': 'webscrapingdev.spiders',\n 'SPIDER_MODULES': ['webscrapingdev.spiders']}\n2025-08-03 23:05:30 [scrapy.middleware] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n 'scrapy.downloadermiddlewares.stats.DownloaderStats',\n 'scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware']\n2025-08-03 23:05:30 [scrapy.middleware] INFO: Enabled spider middlewares:\n['scrapy.spidermiddlewares.start.StartSpiderMiddleware',\n 'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n2025-08-03 23:05:30 [scrapy.middleware] INFO: Enabled item pipelines:\n[]\n2025-08-03 23:05:30 [scrapy.core.engine] INFO: Spider opened\n2025-08-03 23:05:30 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2025-08-03 23:05:30 [scrapy.extensions.httpcache] DEBUG: Using filesystem cache storage in /home/basil-owiti/web-scrap/.scrapy/httpcache\n2025-08-03 23:05:30 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n2025-08-03 23:05:30 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://web-scraping.dev/products&gt; (referer: None) ['cached']\n2025-08-03 23:05:30 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://web-scraping.dev/product/5&gt; (referer: https://web-scraping.dev/products) ['cached']\n2025-08-03 23:05:30 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://web-scraping.dev/product/4&gt; (referer: https://web-scraping.dev/products) ['cached']\n2025-08-03 23:05:30 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://web-scraping.dev/product/3&gt; (referer: https://web-scraping.dev/products) ['cached']\n2025-08-03 23:05:30 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://web-scraping.dev/product/2&gt; (referer: https://web-scraping.dev/products) ['cached']\n2025-08-03 23:05:30 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://web-scraping.dev/product/1&gt; (referer: https://web-scraping.dev/products) ['cached']\n2025-08-03 23:05:30 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 https://web-scraping.dev/product/5&gt;\n{'title': 'Blue Energy Potion', 'price': '$4.99', 'image': 'https://web-scraping.dev/assets/products/blue-potion.webp', 'description': \"Ignite your gaming sessions with our 'Blue Energy Potion', a premium energy drink crafted for dedicated gamers. Inspired by the classic video game potions, this energy drink provides a much-needed boost to keep you focused and energized. It's more than just an energy drink - it's an ode to the gaming culture, packaged in an aesthetically pleasing potion-like bottle that'll make you feel like you're in your favorite game world. Drink up and game on!\"}\n2025-08-03 23:05:30 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 https://web-scraping.dev/product/4&gt;\n{'title': 'Red Energy Potion', 'price': '$4.99', 'image': 'https://web-scraping.dev/assets/products/red-potion.webp', 'description': \"Elevate your game with our 'Red Potion', an extraordinary energy drink that's as enticing as it is effective. This fiery red potion delivers an explosive berry flavor and an energy kick that keeps you at the top of your game. Are you ready to level up?\"}\n2025-08-03 23:05:30 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 https://web-scraping.dev/product/3&gt;\n{'title': 'Teal Energy Potion', 'price': '$4.99', 'image': 'https://web-scraping.dev/assets/products/teal-potion.webp', 'description': \"Experience a surge of vitality with our 'Teal Potion', an exceptional energy drink designed for the gaming community. With its intriguing teal color and a flavor that keeps you asking for more, this potion is your best companion during those long gaming nights. Every sip is an adventure - let the quest begin!\"}\n2025-08-03 23:05:30 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 https://web-scraping.dev/product/2&gt;\n{'title': 'Dark Red Energy Potion', 'price': '$4.99', 'image': 'https://web-scraping.dev/assets/products/darkred-potion.webp', 'description': \"Unleash the power within with our 'Dark Red Potion', an energy drink as intense as the games you play. Its deep red color and bold cherry cola flavor are as inviting as they are invigorating. Bring out the best in your gaming performance, and unlock your full potential.\"}\n2025-08-03 23:05:30 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 https://web-scraping.dev/product/1&gt;\n{'title': 'Box of Chocolate Candy', 'price': '$9.99 ', 'image': 'https://web-scraping.dev/assets/products/orange-chocolate-box-small-1.webp', 'description': \"Indulge your sweet tooth with our Box of Chocolate Candy. Each box contains an assortment of rich, flavorful chocolates with a smooth, creamy filling. Choose from a variety of flavors including zesty orange and sweet cherry. Whether you're looking for the perfect gift or just want to treat yourself, our Box of Chocolate Candy is sure to satisfy.\"}\n2025-08-03 23:05:30 [scrapy.core.engine] INFO: Closing spider (finished)\n2025-08-03 23:05:30 [scrapy.extensions.feedexport] INFO: Stored json feed (5 items) in: producthunt.json\n2025-08-03 23:05:30 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n{'downloader/request_bytes': 2187,\n 'downloader/request_count': 6,\n 'downloader/request_method_count/GET': 6,\n 'downloader/response_bytes': 129325,\n 'downloader/response_count': 6,\n 'downloader/response_status_count/200': 6,\n 'elapsed_time_seconds': 0.273339,\n 'feedexport/success_count/FileFeedStorage': 1,\n 'finish_reason': 'finished',\n 'finish_time': datetime.datetime(2025, 8, 3, 20, 5, 30, 975962, tzinfo=datetime.timezone.utc),\n 'httpcache/hit': 6,\n 'item_scraped_count': 5,\n 'items_per_minute': None,\n 'log_count/DEBUG': 15,\n 'log_count/INFO': 11,\n 'memusage/max': 169533440,\n 'memusage/startup': 169533440,\n 'request_depth_max': 1,\n 'response_received_count': 6,\n 'responses_per_minute': None,\n 'scheduler/dequeued': 6,\n 'scheduler/dequeued/memory': 6,\n 'scheduler/enqueued': 6,\n 'scheduler/enqueued/memory': 6,\n 'start_time': datetime.datetime(2025, 8, 3, 20, 5, 30, 702623, tzinfo=datetime.timezone.utc)}\n2025-08-03 23:05:30 [scrapy.core.engine] INFO: Spider closed (finished)\n\n\n\n\nSaving results\n\n!scrapy crawl products --output results.json\n\n2025-08-03 23:05:31 [scrapy.utils.log] INFO: Scrapy 2.13.3 started (bot: webscrapingdev)\n2025-08-03 23:05:31 [scrapy.utils.log] INFO: Versions:\n{'lxml': '6.0.0',\n 'libxml2': '2.14.4',\n 'cssselect': '1.3.0',\n 'parsel': '1.10.0',\n 'w3lib': '2.3.1',\n 'Twisted': '25.5.0',\n 'Python': '3.12.3 (main, Jun 18 2025, 17:59:45) [GCC 13.3.0]',\n 'pyOpenSSL': '25.1.0 (OpenSSL 3.5.1 1 Jul 2025)',\n 'cryptography': '45.0.5',\n 'Platform': 'Linux-6.14.0-27-generic-x86_64-with-glibc2.39'}\n2025-08-03 23:05:31 [scrapy.addons] INFO: Enabled addons:\n[]\n2025-08-03 23:05:31 [asyncio] DEBUG: Using selector: EpollSelector\n2025-08-03 23:05:31 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n2025-08-03 23:05:31 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n2025-08-03 23:05:31 [scrapy.extensions.telnet] INFO: Telnet Password: 3b6189cc1a000885\n2025-08-03 23:05:31 [scrapy.middleware] INFO: Enabled extensions:\n['scrapy.extensions.corestats.CoreStats',\n 'scrapy.extensions.telnet.TelnetConsole',\n 'scrapy.extensions.memusage.MemoryUsage',\n 'scrapy.extensions.feedexport.FeedExporter',\n 'scrapy.extensions.logstats.LogStats']\n2025-08-03 23:05:31 [scrapy.crawler] INFO: Overridden settings:\n{'BOT_NAME': 'webscrapingdev',\n 'CONCURRENT_REQUESTS_PER_DOMAIN': 1,\n 'DOWNLOAD_DELAY': 1,\n 'FEED_EXPORT_ENCODING': 'utf-8',\n 'HTTPCACHE_ENABLED': True,\n 'NEWSPIDER_MODULE': 'webscrapingdev.spiders',\n 'SPIDER_MODULES': ['webscrapingdev.spiders']}\n2025-08-03 23:05:31 [scrapy.middleware] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n 'scrapy.downloadermiddlewares.stats.DownloaderStats',\n 'scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware']\n2025-08-03 23:05:31 [scrapy.middleware] INFO: Enabled spider middlewares:\n['scrapy.spidermiddlewares.start.StartSpiderMiddleware',\n 'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n2025-08-03 23:05:31 [scrapy.middleware] INFO: Enabled item pipelines:\n[]\n2025-08-03 23:05:31 [scrapy.core.engine] INFO: Spider opened\n2025-08-03 23:05:31 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2025-08-03 23:05:31 [scrapy.extensions.httpcache] DEBUG: Using filesystem cache storage in /home/basil-owiti/web-scrap/.scrapy/httpcache\n2025-08-03 23:05:31 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n2025-08-03 23:05:31 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://web-scraping.dev/products&gt; (referer: None) ['cached']\n2025-08-03 23:05:31 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://web-scraping.dev/product/5&gt; (referer: https://web-scraping.dev/products) ['cached']\n2025-08-03 23:05:31 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://web-scraping.dev/product/4&gt; (referer: https://web-scraping.dev/products) ['cached']\n2025-08-03 23:05:31 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://web-scraping.dev/product/3&gt; (referer: https://web-scraping.dev/products) ['cached']\n2025-08-03 23:05:31 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://web-scraping.dev/product/2&gt; (referer: https://web-scraping.dev/products) ['cached']\n2025-08-03 23:05:31 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://web-scraping.dev/product/1&gt; (referer: https://web-scraping.dev/products) ['cached']\n2025-08-03 23:05:31 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 https://web-scraping.dev/product/5&gt;\n{'title': 'Blue Energy Potion', 'price': '$4.99', 'image': 'https://web-scraping.dev/assets/products/blue-potion.webp', 'description': \"Ignite your gaming sessions with our 'Blue Energy Potion', a premium energy drink crafted for dedicated gamers. Inspired by the classic video game potions, this energy drink provides a much-needed boost to keep you focused and energized. It's more than just an energy drink - it's an ode to the gaming culture, packaged in an aesthetically pleasing potion-like bottle that'll make you feel like you're in your favorite game world. Drink up and game on!\"}\n2025-08-03 23:05:31 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 https://web-scraping.dev/product/4&gt;\n{'title': 'Red Energy Potion', 'price': '$4.99', 'image': 'https://web-scraping.dev/assets/products/red-potion.webp', 'description': \"Elevate your game with our 'Red Potion', an extraordinary energy drink that's as enticing as it is effective. This fiery red potion delivers an explosive berry flavor and an energy kick that keeps you at the top of your game. Are you ready to level up?\"}\n2025-08-03 23:05:31 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 https://web-scraping.dev/product/3&gt;\n{'title': 'Teal Energy Potion', 'price': '$4.99', 'image': 'https://web-scraping.dev/assets/products/teal-potion.webp', 'description': \"Experience a surge of vitality with our 'Teal Potion', an exceptional energy drink designed for the gaming community. With its intriguing teal color and a flavor that keeps you asking for more, this potion is your best companion during those long gaming nights. Every sip is an adventure - let the quest begin!\"}\n2025-08-03 23:05:31 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 https://web-scraping.dev/product/2&gt;\n{'title': 'Dark Red Energy Potion', 'price': '$4.99', 'image': 'https://web-scraping.dev/assets/products/darkred-potion.webp', 'description': \"Unleash the power within with our 'Dark Red Potion', an energy drink as intense as the games you play. Its deep red color and bold cherry cola flavor are as inviting as they are invigorating. Bring out the best in your gaming performance, and unlock your full potential.\"}\n2025-08-03 23:05:31 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 https://web-scraping.dev/product/1&gt;\n{'title': 'Box of Chocolate Candy', 'price': '$9.99 ', 'image': 'https://web-scraping.dev/assets/products/orange-chocolate-box-small-1.webp', 'description': \"Indulge your sweet tooth with our Box of Chocolate Candy. Each box contains an assortment of rich, flavorful chocolates with a smooth, creamy filling. Choose from a variety of flavors including zesty orange and sweet cherry. Whether you're looking for the perfect gift or just want to treat yourself, our Box of Chocolate Candy is sure to satisfy.\"}\n2025-08-03 23:05:31 [scrapy.core.engine] INFO: Closing spider (finished)\n2025-08-03 23:05:31 [scrapy.extensions.feedexport] INFO: Stored json feed (5 items) in: results.json\n2025-08-03 23:05:31 [scrapy.extensions.feedexport] INFO: Stored json feed (5 items) in: producthunt.json\n2025-08-03 23:05:31 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n{'downloader/request_bytes': 2187,\n 'downloader/request_count': 6,\n 'downloader/request_method_count/GET': 6,\n 'downloader/response_bytes': 129325,\n 'downloader/response_count': 6,\n 'downloader/response_status_count/200': 6,\n 'elapsed_time_seconds': 0.270542,\n 'feedexport/success_count/FileFeedStorage': 2,\n 'finish_reason': 'finished',\n 'finish_time': datetime.datetime(2025, 8, 3, 20, 5, 31, 888646, tzinfo=datetime.timezone.utc),\n 'httpcache/hit': 6,\n 'item_scraped_count': 5,\n 'items_per_minute': None,\n 'log_count/DEBUG': 15,\n 'log_count/INFO': 12,\n 'memusage/max': 169545728,\n 'memusage/startup': 169545728,\n 'request_depth_max': 1,\n 'response_received_count': 6,\n 'responses_per_minute': None,\n 'scheduler/dequeued': 6,\n 'scheduler/dequeued/memory': 6,\n 'scheduler/enqueued': 6,\n 'scheduler/enqueued/memory': 6,\n 'start_time': datetime.datetime(2025, 8, 3, 20, 5, 31, 618104, tzinfo=datetime.timezone.utc)}\n2025-08-03 23:05:31 [scrapy.core.engine] INFO: Spider closed (finished)\n\n\n\n!tree\n\n\n.\n\n├── article-class.png\n\n├── hacker-element.png\n\n├── hacker_news_posts.csv\n\n├── LICENSE\n\n├── producthunt.json\n\n├── README.md\n\n├── requirements.txt\n\n├── results.json\n\n├── scrapy.cfg\n\n├── webscrapingdev\n\n│   ├── __init__.py\n\n│   ├── items.py\n\n│   ├── middlewares.py\n\n│   ├── pipelines.py\n\n│   ├── __pycache__\n\n│   │   ├── __init__.cpython-312.pyc\n\n│   │   └── settings.cpython-312.pyc\n\n│   ├── settings.py\n\n│   └── spiders\n\n│       ├── __init__.py\n\n│       ├── products.py\n\n│       └── __pycache__\n\n│           ├── __init__.cpython-312.pyc\n\n│           └── products.cpython-312.pyc\n\n├── web-scrap.ipynb\n\n├── web-scrap.py\n\n├── web-scrap.qmd\n\n└── web-scrap.quarto_ipynb\n\n\n\n5 directories, 24 files\n\n\n\n\n\nimport json\njson_file = 'results.json'\nwith open(json_file) as f:\n    j_obj = json.load(f)\n\n\njson_fmt = json.dumps(j_obj, indent=2)\nprint(json_fmt)\n\n[\n  {\n    \"title\": \"Blue Energy Potion\",\n    \"price\": \"$4.99\",\n    \"image\": \"https://web-scraping.dev/assets/products/blue-potion.webp\",\n    \"description\": \"Ignite your gaming sessions with our 'Blue Energy Potion', a premium energy drink crafted for dedicated gamers. Inspired by the classic video game potions, this energy drink provides a much-needed boost to keep you focused and energized. It's more than just an energy drink - it's an ode to the gaming culture, packaged in an aesthetically pleasing potion-like bottle that'll make you feel like you're in your favorite game world. Drink up and game on!\"\n  },\n  {\n    \"title\": \"Red Energy Potion\",\n    \"price\": \"$4.99\",\n    \"image\": \"https://web-scraping.dev/assets/products/red-potion.webp\",\n    \"description\": \"Elevate your game with our 'Red Potion', an extraordinary energy drink that's as enticing as it is effective. This fiery red potion delivers an explosive berry flavor and an energy kick that keeps you at the top of your game. Are you ready to level up?\"\n  },\n  {\n    \"title\": \"Teal Energy Potion\",\n    \"price\": \"$4.99\",\n    \"image\": \"https://web-scraping.dev/assets/products/teal-potion.webp\",\n    \"description\": \"Experience a surge of vitality with our 'Teal Potion', an exceptional energy drink designed for the gaming community. With its intriguing teal color and a flavor that keeps you asking for more, this potion is your best companion during those long gaming nights. Every sip is an adventure - let the quest begin!\"\n  },\n  {\n    \"title\": \"Dark Red Energy Potion\",\n    \"price\": \"$4.99\",\n    \"image\": \"https://web-scraping.dev/assets/products/darkred-potion.webp\",\n    \"description\": \"Unleash the power within with our 'Dark Red Potion', an energy drink as intense as the games you play. Its deep red color and bold cherry cola flavor are as inviting as they are invigorating. Bring out the best in your gaming performance, and unlock your full potential.\"\n  },\n  {\n    \"title\": \"Box of Chocolate Candy\",\n    \"price\": \"$9.99 \",\n    \"image\": \"https://web-scraping.dev/assets/products/orange-chocolate-box-small-1.webp\",\n    \"description\": \"Indulge your sweet tooth with our Box of Chocolate Candy. Each box contains an assortment of rich, flavorful chocolates with a smooth, creamy filling. Choose from a variety of flavors including zesty orange and sweet cherry. Whether you're looking for the perfect gift or just want to treat yourself, our Box of Chocolate Candy is sure to satisfy.\"\n  }\n]"
  }
]