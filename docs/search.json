[
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "resources",
    "section": "",
    "text": "Rodrigues’ Reproducible Analytical Pipelines in R\nSamantha’s quarto website tutorial\nSamantha’s quarto blog posts tutorial\nMastering Shiny, by Hadley Wickham\nHappy Git and GitHub for the useR, by Jenny Bryan\nR for Data Science, by Hadley Wickham\nHitchhiker’s Guide to Python, by Kenneth Reitz & Tanya Schlusser\nData wrangling essentials: comparisons in JavaScript, Python, SQL, R, and Excel, by Allison Horst & Paul Buffa\nW3Schools, particularly for their HTML & CSS tutorials\nJayde’s parameterized reports with quarto\nMine’s quarto manuscripts"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Portfolio",
    "section": "",
    "text": "R package development\nTaking on package development inculcates in one best practices in programming: using a secluded development environment made possible by {renv} package and Rstudio’s project initialization functionality, documenting code, shared data and publishing user guides to get people started with the software. Package development workflow included {fussen}, {roxygen2}, and {devtools}\nThe packaged is named pbwrangler, it’s goal to document functions used in reading, processing and writing field experimental data in potato breeding.\n\n\nShiny web app\n{shiny} is undoubtedly a go to tool for building a web app that runs in production or just for presenting a proof of concept. I have been reading about using {golem} in packaging and modularizing shiny applications for better project level management (being a package you able to document, check, and test every function), and other reproducubility benefits (for instance docker images). I was able to modularize parts of a COVID-19 dashboard initially developed by a team from LSHTM, into a golem-framework application called COVID19Dash. It was challenging navigating a large codebase and breaking it into parts, troubleshooting instances of some lines of code not running in an isolated environment, the kind we work with during package package development and docker image building/serving.\nPrior to this, I had also developed a small app as proof that I understand the underlying framework to be able to build an app that can be used in production.\n\n\nReproducible analytical pipelines in R\nThis was a ‘do it yourself too’ as I was reading an online version of Rodrigues’ reproducible piplines text . It reinforced my understanding of package development powered by {fusen}, reproducibility of package versions using {renv}, reproducible pipelines with {targets}, building and sharing docker containers in dockerhub and github, and continuous integration/development (CI/CD) using github actions. A branch with CI/CD running a docker container can be found here.\n\n\nEnd-to-end Machine Learning (MLflow + Docker + Google cloud)\nThis is an account of my learning journey aided by this tutorial to grasp the nitty-gritties of building, logging, saving and serving machine learning models. The code available at this repo and a write-up is here.\n\n\nWeb scraping: getting data from the internet\nI set out to understand how to scrape data using Python. I explored beautifulsoup, requests, scrapingBee API, and scrapy to scrape data from Google news and a product listing page. I wrote a piece about it too. I have also scheduled this to run daily using github actions.\n\n\nProject-based Data Engineering\nThis is an accompanying “do-it-yourself” as I go through data engineering material from DuckDb. It is my initiative to learn how to to build data pipelines with Python, SQL & DuckDB. The first part is about ingestion, involving reading public data from Google Cloud, writing it locally as .csv or to MotherDuck database. Github for project materials and a post\n\n\nELT pipeline with dbt, snowflake, and dagster\nCreated an ELT pipeline that uses a dbt project to read and write tables to a snowflake warehouse database, then orchestrated the workflow with dagster. Github repo."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "Hello!\nYou’ve made it to my landing page! I am a student enrolled in a Master of Science degree in Statistics and data science, specializing in biostatistics. I use R, SAS, and Python for statistical analysis and visualizations. My interest is statistical computing applied to spatial statistics, infectious diseases modelling and Bayesian data analysis.\n\neducation\n\n\nMS in Biostatistics, 2020 - Ongoing|Hasselt University\n\n\n\nBS in Applied Statistics, 2015|Maseno University\n\n\n\n\n\nexperience\n\n\nBiometrician, 2024-present|International Potato Center (CIP)\n\n\nResearch Assistant, 2022-2023|Karolinska Institutet\n\n\n\nData Manager, 2022-2022|Kenya Medical Research Institute\n\n\n\nData Manager, 2018-2020|KEMRI - Wellcome Trust Research Programme"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "posts",
    "section": "",
    "text": "Surveys: Design and Analysis\n\n\n\nbiostatistics\n\nsurvey design\n\n\n\n\n\n\n\n\n\nJan 15, 2026\n\n\nBasil Okola\n\n\n\n\n\n\n\n\n\n\n\n\nAdaptive Methods for Clinical Trials\n\n\n\nbiostatistics\n\nadaptive designs\n\ninterim analysis\n\nsimulations\n\n\n\nA simulation approach to adaptive clinical trial designs\n\n\n\n\n\nJan 12, 2026\n\n\nBasil Okola\n\n\n\n\n\n\n\n\n\n\n\n\nInteractive maps (R)\n\n\n\nggplot2\n\nplotly\n\necharts4r\n\nggiraph\n\nwidgetframe\n\n\n\n\n\n\n\n\n\nOct 30, 2025\n\n\nBasil Okola\n\n\n\n\n\n\n\n\n\n\n\n\nPublication ready visualization in R\n\n\n\nggplot2\n\ncowplot\n\n\n\n\n\n\n\n\n\nOct 29, 2025\n\n\nBasil Okola\n\n\n\n\n\n\n\n\n\n\n\n\nEnd-to-end Data Engineering with Python DuckDB, Google Cloud, dbt\n\n\n\nQuarto\n\nPython\n\nbeautifulsoup\n\nscrapy\n\nscrapingBee\n\n\n\nPart 1: Data Ingestion\n\n\n\n\n\nAug 8, 2025\n\n\nBasil Okola\n\n\n\n\n\n\n\n\n\n\n\n\nWeb Scraping with Python\n\n\n\nQuarto\n\nPython\n\nbeautifulsoup\n\nscrapy\n\nscrapingBee\n\n\n\nScraping data from the web using {beautifulsoup4}, {requests}, {ScrapingBee}, and {Scrapy}\n\n\n\n\n\nAug 3, 2025\n\n\nBasil Okola\n\n\n\n\n\n\n\n\n\n\n\n\nEnd-to-end Machine Learning (MLflow + Docker + Google Cloud)\n\n\n\nQuarto\n\nPython\n\nMLflow\n\n\n\nMachine learning Workflow using MLflow locally and pushing image to google cloud\n\n\n\n\n\nAug 2, 2025\n\n\nBasil Okola\n\n\n\n\n\n\n\n\n\n\n\n\nR package development guide\n\n\n\nQuarto\n\nR\n\npackages\n\n\n\nWriting R packages using devtools, roxygen2 and usethis packages\n\n\n\n\n\nDec 16, 2023\n\n\nBasil Okola\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-12-16-R-package dev/index.html",
    "href": "posts/2023-12-16-R-package dev/index.html",
    "title": "R package development guide",
    "section": "",
    "text": "These are quick step-by-step guides I wrote when going through Andy’s fundamentas of package development notes.\n\nInitialize package dev files with devtools::create_package() which creates mandatory file for the package\nEnable git tracking:\n\nconfigure: usethis::use_git_configure()\ncommit: usethis::use_git()\n\nFill in sections of the DESCRIPTION file. {roxygen2} is used to actualize this.\nEdit.Rprofile() to set options you’ll use in the documentation such as author details. This uses usethis::edit_r_profile()\nCreate a separate R file for each function you’ll be including in the package. Use use_r(\"/path-to-file/R/function-name.R\")\nLoad the functions with devtools::load_all()\nCheck the loaded files with check() which runs R CMD checks to catch errors/warnings/notes that need addressing.\nAdd files you don’t wish to include in the package build-in .Rbuildignore file.\nEnable roxygen2 to be used for package documentation: project options -&gt; Build Tools -&gt; check to generate documentation with roxygen or better devtools::document() which generates NAMESPACE automatically\nAutomate external function imports with usethis::use_import_from(): example usethis::use_import_from(“utils”, “install.packages”)\nDocument functions: put the cursor inside the R function definition and ctrl+shift+alt+R to insert the roxygen skeleton. The workflow here is after documenting -&gt; load_all() -&gt; document() -&gt; check()\nData files go to /data dir. It should be of .rda format\nExternal (non .rda format) data go to /inst/extdata/. Document them in the R file e.g. data.R and store it in /R. load_all() then document()\nCall use_package_doc() to add a dummy .R file that will prompt roxygen to generate basic package-level documentation. I noticed doing this erased imports in {package-name}-package.R file. Add recommended imports (see 10) and check()\nInstall your package with devtools::install()\nAttach your package as with other packages by calling library()\nTesting: Using the edit code -&gt; load_all() -&gt; experiment iteration can be unsustainable if you come back to your code months after development. You should write formal tests supported by {testthat} package.\nSet up formal testing of your package with usethis::use_testthat(). Creates a folder /tests. Don’t edit tests/testhat.R\nCall usethis::use_test() e.g., use_test(\"install_load_packages.R\") to edit tests for functions living in a particular R file in R/.\ntest() or ctr + shift + T runs all tests in your test/ directory. The workflow updates to load_all() -&gt; test() -&gt; document() -&gt; check(). Tests should be small and run quickly.\nDependencies, add imports in DESCRIPTION with use_package().\nAdd README with use_readme_rmd()\nRender readme.rmd with build_readme()\nUse continuous integration with use_github_action() then build_readme() again 25 Build a website for your package with use_pkgdown_github_pages() then document().\n\n\n\n\nCitationBibTeX citation:@online{okola2023,\n  author = {Okola, Basil},\n  title = {R Package Development Guide},\n  date = {2023-12-16},\n  url = {https://bokola.github.io/posts/2023-12-16-R-package dev/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nOkola, Basil. 2023. “R Package Development Guide.” December\n16, 2023. https://bokola.github.io/posts/2023-12-16-R-package\ndev/."
  },
  {
    "objectID": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html",
    "href": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html",
    "title": "End-to-end Machine Learning (MLflow + Docker + Google Cloud)",
    "section": "",
    "text": "BibTeX citation:@online{okola2025,\n  author = {Okola, Basil},\n  title = {End-to-End {Machine} {Learning} {(MLflow} + {Docker} +\n    {Google} {Cloud)}},\n  date = {2025-08-02},\n  url = {https://bokola.github.io/posts/2025-08-02-End-to-end-ML-with-MLflow/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nOkola, Basil. 2025. “End-to-End Machine Learning (MLflow + Docker\n+ Google Cloud).” August 2, 2025. https://bokola.github.io/posts/2025-08-02-End-to-end-ML-with-MLflow/.\nThis is an account of my learning journey aided by this tutorial to grasp the nitty-gritties of buliding, logging, saving and serving machine learning models. First is a description of the development environment used."
  },
  {
    "objectID": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#development-environment",
    "href": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#development-environment",
    "title": "End-to-end Machine Learning (MLflow + Docker + Google Cloud)",
    "section": "Development Environment",
    "text": "Development Environment\nI am running Debian 24.04 LTS, and Pycharm IDE calling Python 3.12 within a .venv virtual environment. Since the model used is a Tensorflow neural network, I had to follow cuda documentation in setting up necessary drives. You also need to start mlflow ui local server by running mlflow ui --port 5000 in the terminal, install dependenices pip install mlflow[extras] hyperopt tensorflow scikit-learn pandas numpy, and set environment variable export MLFLOW_TRACKING_URI=http://localhost:5000."
  },
  {
    "objectID": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-1-data-preparation",
    "href": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-1-data-preparation",
    "title": "End-to-end Machine Learning (MLflow + Docker + Google Cloud)",
    "section": "Step 1 : Data Preparation",
    "text": "Step 1 : Data Preparation\nThe tutorial uses wine quality classification data.\n#prepare data\n\nimport os\nfrom dotenv import load_dotenv\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport tensorflow as tf\nfrom tensorflow import keras\nimport mlflow\nfrom mlflow.models import infer_signature\nfrom hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n\n#test prediction\nimport requests\nimport json\n\n#environment variables\nload_dotenv(\".env\")\n\nMLFLOW_TRACKING_URI=os.getenv(\"MLFLOW_TRACKING_URI\")\nXLA_FLAGS=os.getenv('XLA_FLAGS')\n\n#load data\ndata = pd.read_csv(\n    \"https://raw.githubusercontent.com/mlflow/mlflow/master/tests/datasets/winequality-white.csv\",\n    sep=\";\",\n)\n\ntrain, test = train_test_split(data, test_size=0.2, random_state=12)\ntrain_x = train.drop([\"quality\"], axis=1).values\ntrain_y = train[[\"quality\"]].values.ravel()\ntest_x = test.drop([\"quality\"], axis=1).values\ntest_y = test[[\"quality\"]].values.ravel()\n\n#further split training data for validation\ntrain_x, valid_x, train_y, valid_y = train_test_split(train_x, train_y, test_size=0.2, random_state=12)\n\n#Create model signature for deployment\nsignature = infer_signature(train_x, train_y)"
  },
  {
    "objectID": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#define-model-architecture",
    "href": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#define-model-architecture",
    "title": "End-to-end Machine Learning (MLflow + Docker + Google Cloud)",
    "section": "Define Model architecture",
    "text": "Define Model architecture\ndef create_and_train_model(learning_rate, momentum, epochs=10):\n    \"\"\"\n    Create and train a neural network with specified hyperparameters.\n\n    Returns:\n        dict: Training results including model and metrics\n    \"\"\"\n    # Normalize input features for better training stability\n    mean = np.mean(train_x, axis=0)\n    var = np.var(train_x, axis=0)\n\n    # Define model architecture\n    model = keras.Sequential(\n        [\n            keras.Input([train_x.shape[1]]),\n            keras.layers.Normalization(mean=mean, variance=var),\n            keras.layers.Dense(64, activation=\"relu\"),\n            keras.layers.Dropout(0.2),  # Add regularization\n            keras.layers.Dense(32, activation=\"relu\"),\n            keras.layers.Dense(1),\n        ]\n    )\n\n    # Compile with specified hyperparameters\n    model.compile(\n        optimizer=keras.optimizers.SGD(learning_rate=learning_rate, momentum=momentum),\n        loss=\"mean_squared_error\",\n        metrics=[keras.metrics.RootMeanSquaredError()],\n    )\n\n    # Train with early stopping for efficiency\n    early_stopping = keras.callbacks.EarlyStopping(\n        patience=3, restore_best_weights=True\n    )\n\n    # Train the model\n    history = model.fit(\n        train_x,\n        train_y,\n        validation_data=(valid_x, valid_y),\n        epochs=epochs,\n        batch_size=64,\n        callbacks=[early_stopping],\n        verbose=0,  # Reduce output for cleaner logs\n    )\n\n    # Evaluate on validation set\n    val_loss, val_rmse = model.evaluate(valid_x, valid_y, verbose=0)\n\n    return {\n        \"model\": model,\n        \"val_rmse\": val_rmse,\n        \"val_loss\": val_loss,\n        \"history\": history,\n        \"epochs_trained\": len(history.history[\"loss\"]),\n    }\n    ```\n\n## Step 3: Set up parameter optimization\n\ndef objective(params): ““” Objective function for hyperparameter optimization. This function will be called by Hyperopt for each trial. ““” with mlflow.start_run(nested=True): # Log hyperparameters being tested mlflow.log_params( { “learning_rate”: params[“learning_rate”], “momentum”: params[“momentum”], “optimizer”: “SGD”, “architecture”: “64-32-1”, } )\n    # Train model with current hyperparameters\n    result = create_and_train_model(\n        learning_rate=params[\"learning_rate\"],\n        momentum=params[\"momentum\"],\n        epochs=15,\n    )\n\n    # Log training results\n    mlflow.log_metrics(\n        {\n            \"val_rmse\": result[\"val_rmse\"],\n            \"val_loss\": result[\"val_loss\"],\n            \"epochs_trained\": result[\"epochs_trained\"],\n        }\n    )\n\n    # Log the trained model\n    mlflow.tensorflow.log_model(result[\"model\"], name=\"model\", signature=signature)\n\n    # Log training curves as artifacts\n    import matplotlib.pyplot as plt\n\n    plt.figure(figsize=(12, 4))\n\n    plt.subplot(1, 2, 1)\n    plt.plot(result[\"history\"].history[\"loss\"], label=\"Training Loss\")\n    plt.plot(result[\"history\"].history[\"val_loss\"], label=\"Validation Loss\")\n    plt.title(\"Model Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n\n    plt.subplot(1, 2, 2)\n    plt.plot(\n        result[\"history\"].history[\"root_mean_squared_error\"], label=\"Training RMSE\"\n    )\n    plt.plot(\n        result[\"history\"].history[\"val_root_mean_squared_error\"],\n        label=\"Validation RMSE\",\n    )\n    plt.title(\"Model RMSE\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"RMSE\")\n    plt.legend()\n\n    plt.tight_layout()\n    plt.savefig(\"training_curves.png\")\n    mlflow.log_artifact(\"training_curves.png\")\n    plt.close()\n\n    # Return loss for Hyperopt (it minimizes)\n    return {\"loss\": result[\"val_rmse\"], \"status\": STATUS_OK}"
  },
  {
    "objectID": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-5-analyze-results-in-the-mlflow-ui",
    "href": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-5-analyze-results-in-the-mlflow-ui",
    "title": "End-to-end Machine Learning (MLflow + Docker + Google Cloud)",
    "section": "Step 5: Analyze Results in the MLflow UI",
    "text": "Step 5: Analyze Results in the MLflow UI\n\nNavigate to your experiment → click on “wine-quality-optimization\nAdd key columns: click “columns and add:\n\nMetrics | val_rmse\nParameters | learning_rate\nParameters | momentum\n\nInterprete the visualization: blue lines - better performing runs; red lines - worse performing runs\nAlso take a look at the training curves:\n\n\nfrom PIL import Image\nfrom IPython.display import display\n# Specify the path to your image file\nimage_path = \"val_rmse.png\"\n\n# Read the image\nimg = Image.open(image_path)\n\n# Display the image\ndisplay(img)\n\n\n\n\n\n\n\n\n\nfrom PIL import Image\nfrom IPython.display import display\n\n# Specify the path to your image file\nimage_path = \"training_curves.png\"\n\n# Read the image\nimg = Image.open(image_path)\n\n# Display the image\ndisplay(img)"
  },
  {
    "objectID": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-6-register-your-best-model",
    "href": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-6-register-your-best-model",
    "title": "End-to-end Machine Learning (MLflow + Docker + Google Cloud)",
    "section": "Step 6: Register your best model",
    "text": "Step 6: Register your best model\nTo find the best run: in the table view, click on the run with the lowest val_rmse then navigate to model artifacts and scroll to the “Artifacts” section. then register the model:\n- Go to \"Models\" tab in MLflow UI\n\n- Click on your registered model\n\n- Transition to \"Staging\" stage for testing\n\n- Add tags and descriptions as needed"
  },
  {
    "objectID": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-7-deploy-the-best-model",
    "href": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-7-deploy-the-best-model",
    "title": "End-to-end Machine Learning (MLflow + Docker + Google Cloud)",
    "section": "Step 7: Deploy the best model",
    "text": "Step 7: Deploy the best model\nTest your model with a REST API\n# Serve the model (choose the version number you registered)\nmlflow models serve -m \"models:/wine-quality-predictor/1\" --port 5002\nTest your deployment\n# Test with a sample wine\ncurl -X POST http://localhost:5002/invocations \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"dataframe_split\": {\n      \"columns\": [\n        \"fixed acidity\", \"volatile acidity\", \"citric acid\", \"residual sugar\",\n        \"chlorides\", \"free sulfur dioxide\", \"total sulfur dioxide\", \"density\",\n        \"pH\", \"sulphates\", \"alcohol\"\n      ],\n      \"data\": [[7.0, 0.27, 0.36, 20.7, 0.045, 45, 170, 1.001, 3.0, 0.45, 8.8]]\n    }\n  }'\n\n\nYou could also test with Python\n\nimport requests\nimport json\n\n# Prepare test data\ntest_wine = {\n    \"dataframe_split\": {\n        \"columns\": [\n            \"fixed acidity\",\n            \"volatile acidity\",\n            \"citric acid\",\n            \"residual sugar\",\n            \"chlorides\",\n            \"free sulfur dioxide\",\n            \"total sulfur dioxide\",\n            \"density\",\n            \"pH\",\n            \"sulphates\",\n            \"alcohol\",\n        ],\n        \"data\": [[7.0, 0.27, 0.36, 20.7, 0.045, 45, 170, 1.001, 3.0, 0.45, 8.8]],\n    }\n}\n\n# Make prediction request\nresponse = requests.post(\n    \"http://localhost:5002/invocations\",\n    headers={\"Content-Type\": \"application/json\"},\n    data=json.dumps(test_wine),\n)\n\nprediction = response.json()\nprint(f\"Predicted wine quality: {prediction['predictions'][0]:.2f}\")"
  },
  {
    "objectID": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-8-build-a-production-docker-container",
    "href": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-8-build-a-production-docker-container",
    "title": "End-to-end Machine Learning (MLflow + Docker + Google Cloud)",
    "section": "Step 8: Build a production Docker container",
    "text": "Step 8: Build a production Docker container\n# Build Docker image\nmlflow models build-docker \\\n  --model-uri \"models:/wine-quality-predictor/1\" \\\n  --name \"wine-quality-api\"\nTest your container:\n# Run the container\ndocker run -p 5003:8080 wine-quality-api\n\n# Test in another terminal\ncurl -X POST http://localhost:5003/invocations \\\n-H \"Content-Type: application/json\" \\\n-d '{\n  \"dataframe_split\": {\n    \"columns\": [\"fixed acidity\",\"volatile acidity\",\"citric acid\",\"residual sugar\",\"chlorides\",\"free sulfur dioxide\",\"total sulfur dioxide\",\"density\",\"pH\",\"sulphates\",\"alcohol\"],\n    \"data\": [[7.0, 0.27, 0.36, 20.7, 0.045, 45, 170, 1.001, 3.0, 0.45, 8.8]]\n  }\n}'"
  },
  {
    "objectID": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-9-deploy-to-google-cloud",
    "href": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-9-deploy-to-google-cloud",
    "title": "End-to-end Machine Learning (MLflow + Docker + Google Cloud)",
    "section": "Step 9: Deploy to Google cloud",
    "text": "Step 9: Deploy to Google cloud\n\nAuthentication and project set up\n$ gcloud auth login\nConfigure Docker for gcp $ gcloud auth configure-docker\nset project $ gcloud config set project PROJECT_ID\nIAM roles\n\nArtifacr registry Administrator\nroles/artifactregistry.createOnPushRepoAdmin\nStorage Administrator\n\nExport the credentials export GOOGLE_APPLICATION_CREDENTIALS=\"/path/to/your-service-account-file.json\"\nTag the docker image\n$ docker tag IMAGE_NAME gcr.io/PROJECT_ID/IMAGE_NAME:TAG\nPush the docker image to Google Cloud Container Registry $ docker push gcr.io/PROJECT_ID/IMAGE_NAME:TAG\n\n\nfrom PIL import Image\nfrom IPython.display import display\n# Specify the path to your image file\nimage_path = \"gc-artifact-registry.png\"\n\n# Read the image\nimg = Image.open(image_path)\n\n# Display the image\ndisplay(img)\n\n\n\n\n\n\n\n\nsee"
  },
  {
    "objectID": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-2-define-model-architecture",
    "href": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-2-define-model-architecture",
    "title": "End-to-end Machine Learning (MLflow + Docker + Google Cloud)",
    "section": "Step 2: Define Model architecture",
    "text": "Step 2: Define Model architecture\n\ndef create_and_train_model(learning_rate, momentum, epochs=10):\n    \"\"\"\n    Create and train a neural network with specified hyperparameters.\n\n    Returns:\n        dict: Training results including model and metrics\n    \"\"\"\n    #Normalize input features for better training stability\n    mean = np.mean(train_x, axis=0)\n    var = np.var(train_x, axis=0)\n\n    #Define model architecture\n    model = keras.Sequential(\n        [\n            keras.Input([train_x.shape[1]]),\n            keras.layers.Normalization(mean=mean, variance=var),\n            keras.layers.Dense(64, activation=\"relu\"),\n            keras.layers.Dropout(0.2),  # Add regularization\n            keras.layers.Dense(32, activation=\"relu\"),\n            keras.layers.Dense(1),\n        ]\n    )\n\n    #Compile with specified hyperparameters\n    model.compile(\n        optimizer=keras.optimizers.SGD(learning_rate=learning_rate, momentum=momentum),\n        loss=\"mean_squared_error\",\n        metrics=[keras.metrics.RootMeanSquaredError()],\n    )\n\n    #Train with early stopping for efficiency\n    early_stopping = keras.callbacks.EarlyStopping(\n        patience=3, restore_best_weights=True\n    )\n\n    #Train the model\n    history = model.fit(\n        train_x,\n        train_y,\n        validation_data=(valid_x, valid_y),\n        epochs=epochs,\n        batch_size=64,\n        callbacks=[early_stopping],\n        verbose=0,  # Reduce output for cleaner logs\n    )\n\n    #Evaluate on validation set\n    val_loss, val_rmse = model.evaluate(valid_x, valid_y, verbose=0)\n\n    return {\n        \"model\": model,\n        \"val_rmse\": val_rmse,\n        \"val_loss\": val_loss,\n        \"history\": history,\n        \"epochs_trained\": len(history.history[\"loss\"]),\n    }"
  },
  {
    "objectID": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-3-set-up-parameter-optimization",
    "href": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-3-set-up-parameter-optimization",
    "title": "End-to-end Machine Learning (MLflow + Docker + Google Cloud)",
    "section": "Step 3: Set up parameter optimization",
    "text": "Step 3: Set up parameter optimization\n\ndef objective(params):\n    \"\"\"\n    Objective function for hyperparameter optimization.\n    This function will be called by Hyperopt for each trial.\n    \"\"\"\n    with mlflow.start_run(nested=True):\n        #Log hyperparameters being tested\n        mlflow.log_params(\n            {\n                \"learning_rate\": params[\"learning_rate\"],\n                \"momentum\": params[\"momentum\"],\n                \"optimizer\": \"SGD\",\n                \"architecture\": \"64-32-1\",\n            }\n        )\n\n        #Train model with current hyperparameters\n        result = create_and_train_model(\n            learning_rate=params[\"learning_rate\"],\n            momentum=params[\"momentum\"],\n            epochs=15,\n        )\n\n        #Log training results\n        mlflow.log_metrics(\n            {\n                \"val_rmse\": result[\"val_rmse\"],\n                \"val_loss\": result[\"val_loss\"],\n                \"epochs_trained\": result[\"epochs_trained\"],\n            }\n        )\n\n        #Log the trained model\n        mlflow.tensorflow.log_model(result[\"model\"], name=\"model\", signature=signature)\n\n        #Log training curves as artifacts\n        import matplotlib.pyplot as plt\n\n        plt.figure(figsize=(12, 4))\n\n        plt.subplot(1, 2, 1)\n        plt.plot(result[\"history\"].history[\"loss\"], label=\"Training Loss\")\n        plt.plot(result[\"history\"].history[\"val_loss\"], label=\"Validation Loss\")\n        plt.title(\"Model Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.legend()\n\n        plt.subplot(1, 2, 2)\n        plt.plot(\n            result[\"history\"].history[\"root_mean_squared_error\"], label=\"Training RMSE\"\n        )\n        plt.plot(\n            result[\"history\"].history[\"val_root_mean_squared_error\"],\n            label=\"Validation RMSE\",\n        )\n        plt.title(\"Model RMSE\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"RMSE\")\n        plt.legend()\n\n        plt.tight_layout()\n        plt.savefig(\"training_curves.png\")\n        mlflow.log_artifact(\"training_curves.png\")\n        plt.close()\n\n        #Return loss for Hyperopt (it minimizes)\n        return {\"loss\": result[\"val_rmse\"], \"status\": STATUS_OK}\n\n\n#Define search space for hyperparameters\nsearch_space = {\n    \"learning_rate\": hp.loguniform(\"learning_rate\", np.log(1e-5), np.log(1e-1)),\n    \"momentum\": hp.uniform(\"momentum\", 0.0, 0.9),\n}\n\nprint(\"Search space defined:\")\nprint(\"- Learning rate: 1e-5 to 1e-1 (log-uniform)\")\nprint(\"- Momentum: 0.0 to 0.9 (uniform)\")"
  },
  {
    "objectID": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-4-run-the-hyperparameter-optimization",
    "href": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-4-run-the-hyperparameter-optimization",
    "title": "End-to-end Machine Learning (MLflow + Docker + Google Cloud)",
    "section": "Step 4: Run the hyperparameter optimization",
    "text": "Step 4: Run the hyperparameter optimization\n\n#Create or set experiment\nexperiment_name = \"wine-quality-optimization\"\nmlflow.set_experiment(experiment_name)\n\nprint(f\"Starting hyperparameter optimization experiment: {experiment_name}\")\nprint(\"This will run 15 trials to find optimal hyperparameters...\")\n\nwith mlflow.start_run(run_name=\"hyperparameter-sweep\"):\n    #Log experiment metadata\n    mlflow.log_params(\n        {\n            \"optimization_method\": \"Tree-structured Parzen Estimator (TPE)\",\n            \"max_evaluations\": 15,\n            \"objective_metric\": \"validation_rmse\",\n            \"dataset\": \"wine-quality\",\n            \"model_type\": \"neural_network\",\n        }\n    )\n\n    #Run optimization\n    trials = Trials()\n    best_params = fmin(\n        fn=objective,\n        space=search_space,\n        algo=tpe.suggest,\n        max_evals=15,\n        trials=trials,\n        verbose=True,\n    )\n\n    #Find and log best results\n    best_trial = min(trials.results, key=lambda x: x[\"loss\"])\n    best_rmse = best_trial[\"loss\"]\n\n    #Log optimization results\n    mlflow.log_params(\n        {\n            \"best_learning_rate\": best_params[\"learning_rate\"],\n            \"best_momentum\": best_params[\"momentum\"],\n        }\n    )\n    mlflow.log_metrics(\n        {\n            \"best_val_rmse\": best_rmse,\n            \"total_trials\": len(trials.trials),\n            \"optimization_completed\": 1,\n        }\n    )"
  },
  {
    "objectID": "posts/2025-08-03-Web-scraping-python/index.html",
    "href": "posts/2025-08-03-Web-scraping-python/index.html",
    "title": "Web Scraping with Python",
    "section": "",
    "text": "BibTeX citation:@online{okola2025,\n  author = {Okola, Basil},\n  title = {Web {Scraping} with {Python}},\n  date = {2025-08-03},\n  url = {https://bokola.github.io/posts/2025-08-03-Web-scraping-python/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nOkola, Basil. 2025. “Web Scraping with Python.” August 3,\n2025. https://bokola.github.io/posts/2025-08-03-Web-scraping-python/.\nThis is a web scrapping task to find conflict/war related news articles from the internet. There is been quite a lot of that considering the Russia/Ukraine conflict, The South Sudan conflict, and the Palestine/Israel conflict just to mention the most reported cases."
  },
  {
    "objectID": "posts/2025-08-03-Web-scraping-python/index.html#using-beautifulsoup-requests",
    "href": "posts/2025-08-03-Web-scraping-python/index.html#using-beautifulsoup-requests",
    "title": "Web Scraping with Python",
    "section": "Using Beautifulsoup + requests",
    "text": "Using Beautifulsoup + requests\n\nUnderstanding website’s structure\nPrior to scraping inspect the HTML source code of the web page to identify the elements you want to scrape\n\n\nSet up your develpment environment\nCreate a virtual environment, follow prompts per your IDE. For VScode I pressed Ctrl+Shift+Pthen searched Python: Create Environment A beginner web scraper in Python is advised to start with requests and beautifulsoup4 librarires which is what we will use. \nimport requests\nfrom bs4 import BeautifulSoup\n\nbaseurl = \"https://news.ycombinator.com\"\nuser = \"\"\npassd = \"\"\n\ns = requests.Session()\ndata = {\"goto\": \"news\", \"acct\": user, \"pw\": passd}\nr = s.post(f'{baseurl}', data=data)\n\nsoup = BeautifulSoup(r.text, 'html.parser')\nif soup.find(id='logout') is not None:\n    print(\"Successfully logged in\")\nelse:\n    print(\"Authentication error\")\n\n\nInspect HTML element\nEach post is wrapped in a &lt;tr&gt; tag with the class athing\n\nfrom PIL import Image\nfrom IPython.display import display\n\nimport matplotlib.image as mpimg\nimage_path = \"hacker-element.png\"\n\nimage = Image.open(image_path)\ndisplay(image)\n\n\n\n\n\n\n\n\n\n\nScrape with requests + beautifulsoup4\n\nimport requests\nfrom bs4 import BeautifulSoup\n\nr = requests.get(\"https://news.ycombinator.com/\")\nsoup = BeautifulSoup(r.text, 'html.parser')\nlinks = soup.find_all('tr', class_='athing')\n\nformatted_links = []\nfor link in links:\n    data = {\n        'id': link['id'],\n        'title': link.find_all(\"td\")[2].a.text,\n        'url': link.find_all(\"td\")[2].a['href'],\n        'rank': int(link.find_all(\"td\")[0].span.text.replace('.', ''))\n    }\n    formatted_links.append(data)\n\n\n    formatted_links.append(data)\n    \nprint(formatted_links)\n\n[{'id': '44754697', 'title': 'New quantum state of matter found at interface of exotic materials', 'url': 'https://phys.org/news/2025-07-quantum-state-interface-exotic-materials.html', 'rank': 1}, {'id': '44754697', 'title': 'New quantum state of matter found at interface of exotic materials', 'url': 'https://phys.org/news/2025-07-quantum-state-interface-exotic-materials.html', 'rank': 1}, {'id': '44778936', 'title': 'Modern Node.js Patterns', 'url': 'https://kashw1n.com/blog/nodejs-2025/', 'rank': 2}, {'id': '44778936', 'title': 'Modern Node.js Patterns', 'url': 'https://kashw1n.com/blog/nodejs-2025/', 'rank': 2}, {'id': '44780353', 'title': 'So you want to parse a PDF?', 'url': 'https://eliot-jones.com/2025/8/pdf-parsing-xref', 'rank': 3}, {'id': '44780353', 'title': 'So you want to parse a PDF?', 'url': 'https://eliot-jones.com/2025/8/pdf-parsing-xref', 'rank': 3}, {'id': '44779428', 'title': 'Writing a good design document', 'url': 'https://grantslatton.com/how-to-design-document', 'rank': 4}, {'id': '44779428', 'title': 'Writing a good design document', 'url': 'https://grantslatton.com/how-to-design-document', 'rank': 4}, {'id': '44777760', 'title': 'Persona vectors: Monitoring and controlling character traits in language models', 'url': 'https://www.anthropic.com/research/persona-vectors', 'rank': 5}, {'id': '44777760', 'title': 'Persona vectors: Monitoring and controlling character traits in language models', 'url': 'https://www.anthropic.com/research/persona-vectors', 'rank': 5}, {'id': '44781523', 'title': 'A parser for TypeScript types, written in TypeScript types', 'url': 'https://github.com/easrng/tsints', 'rank': 6}, {'id': '44781523', 'title': 'A parser for TypeScript types, written in TypeScript types', 'url': 'https://github.com/easrng/tsints', 'rank': 6}, {'id': '44775563', 'title': \"If you're remote, ramble\", 'url': 'https://stephango.com/ramblings', 'rank': 7}, {'id': '44775563', 'title': \"If you're remote, ramble\", 'url': 'https://stephango.com/ramblings', 'rank': 7}, {'id': '44765562', 'title': 'Life, Work, Death and the Peasant: Family Formation', 'url': 'https://acoup.blog/2025/08/01/collections-life-work-death-and-the-peasant-part-iiia-family-formation/', 'rank': 8}, {'id': '44765562', 'title': 'Life, Work, Death and the Peasant: Family Formation', 'url': 'https://acoup.blog/2025/08/01/collections-life-work-death-and-the-peasant-part-iiia-family-formation/', 'rank': 8}, {'id': '44777766', 'title': 'How Python grew from a language to a community', 'url': 'https://thenewstack.io/how-python-grew-from-a-language-to-a-community/', 'rank': 9}, {'id': '44777766', 'title': 'How Python grew from a language to a community', 'url': 'https://thenewstack.io/how-python-grew-from-a-language-to-a-community/', 'rank': 9}, {'id': '44781116', 'title': 'Why doctors hate their computers (2018)', 'url': 'https://www.newyorker.com/magazine/2018/11/12/why-doctors-hate-their-computers', 'rank': 10}, {'id': '44781116', 'title': 'Why doctors hate their computers (2018)', 'url': 'https://www.newyorker.com/magazine/2018/11/12/why-doctors-hate-their-computers', 'rank': 10}, {'id': '44780878', 'title': 'Typed languages are better suited for vibecoding', 'url': 'https://solmaz.io/typed-languages-are-better-suited-for-vibecoding', 'rank': 11}, {'id': '44780878', 'title': 'Typed languages are better suited for vibecoding', 'url': 'https://solmaz.io/typed-languages-are-better-suited-for-vibecoding', 'rank': 11}, {'id': '44782046', 'title': 'Rising Young Worker Despair in the United States', 'url': 'https://www.nber.org/papers/w34071', 'rank': 12}, {'id': '44782046', 'title': 'Rising Young Worker Despair in the United States', 'url': 'https://www.nber.org/papers/w34071', 'rank': 12}, {'id': '44743631', 'title': 'C++: \"model of the hardware\" vs. \"model of the compiler\" (2018)', 'url': 'http://ithare.com/c-model-of-the-hardware-vs-model-of-the-compiler/', 'rank': 13}, {'id': '44743631', 'title': 'C++: \"model of the hardware\" vs. \"model of the compiler\" (2018)', 'url': 'http://ithare.com/c-model-of-the-hardware-vs-model-of-the-compiler/', 'rank': 13}, {'id': '44780540', 'title': 'How to grow almost anything', 'url': 'https://howtogrowalmostanything.notion.site/htgaa25', 'rank': 14}, {'id': '44780540', 'title': 'How to grow almost anything', 'url': 'https://howtogrowalmostanything.notion.site/htgaa25', 'rank': 14}, {'id': '44767508', 'title': 'Efficiently Generating a Number in a Range (2018)', 'url': 'https://www.pcg-random.org/posts/bounded-rands.html', 'rank': 15}, {'id': '44767508', 'title': 'Efficiently Generating a Number in a Range (2018)', 'url': 'https://www.pcg-random.org/posts/bounded-rands.html', 'rank': 15}, {'id': '44745441', 'title': \"2,500-year-old Siberian 'ice mummy' had intricate tattoos, imaging reveals\", 'url': 'https://www.bbc.com/news/articles/c4gzx0zm68vo', 'rank': 16}, {'id': '44745441', 'title': \"2,500-year-old Siberian 'ice mummy' had intricate tattoos, imaging reveals\", 'url': 'https://www.bbc.com/news/articles/c4gzx0zm68vo', 'rank': 16}, {'id': '44765730', 'title': 'Welcome to url.town, population 465', 'url': 'https://url.town/', 'rank': 17}, {'id': '44765730', 'title': 'Welcome to url.town, population 465', 'url': 'https://url.town/', 'rank': 17}, {'id': '44775700', 'title': 'Tokens are getting more expensive', 'url': 'https://ethanding.substack.com/p/ai-subscriptions-get-short-squeezed', 'rank': 18}, {'id': '44775700', 'title': 'Tokens are getting more expensive', 'url': 'https://ethanding.substack.com/p/ai-subscriptions-get-short-squeezed', 'rank': 18}, {'id': '44760583', 'title': 'Survival at High Altitudes: Wheel-Well Passengers (1996)', 'url': 'https://rosap.ntl.bts.gov/view/dot/57536', 'rank': 19}, {'id': '44760583', 'title': 'Survival at High Altitudes: Wheel-Well Passengers (1996)', 'url': 'https://rosap.ntl.bts.gov/view/dot/57536', 'rank': 19}, {'id': '44781189', 'title': 'Poorest US workers hit hardest by slowing wage growth', 'url': 'https://www.ft.com/content/cfb77a53-fef8-4382-b102-c217e0aa4b25', 'rank': 20}, {'id': '44781189', 'title': 'Poorest US workers hit hardest by slowing wage growth', 'url': 'https://www.ft.com/content/cfb77a53-fef8-4382-b102-c217e0aa4b25', 'rank': 20}, {'id': '44774104', 'title': 'Twenty Eighth International Obfuscated C Code Contest', 'url': 'https://www.ioccc.org/2024/index.html', 'rank': 21}, {'id': '44774104', 'title': 'Twenty Eighth International Obfuscated C Code Contest', 'url': 'https://www.ioccc.org/2024/index.html', 'rank': 21}, {'id': '44764696', 'title': 'A dedicated skin-to-brain circuit for cool sensation in mice', 'url': 'https://www.sciencedaily.com/releases/2025/07/250730030354.htm', 'rank': 22}, {'id': '44764696', 'title': 'A dedicated skin-to-brain circuit for cool sensation in mice', 'url': 'https://www.sciencedaily.com/releases/2025/07/250730030354.htm', 'rank': 22}, {'id': '44777055', 'title': 'This Old SGI: notes and memoirs on the Silicon Graphics 4D series (1996)', 'url': 'https://archive.irixnet.org/thisoldsgi/', 'rank': 23}, {'id': '44777055', 'title': 'This Old SGI: notes and memoirs on the Silicon Graphics 4D series (1996)', 'url': 'https://archive.irixnet.org/thisoldsgi/', 'rank': 23}, {'id': '44775830', 'title': 'How to make almost anything (2019)', 'url': 'https://fab.cba.mit.edu/classes/863.19/CBA/people/dsculley/index.html', 'rank': 24}, {'id': '44775830', 'title': 'How to make almost anything (2019)', 'url': 'https://fab.cba.mit.edu/classes/863.19/CBA/people/dsculley/index.html', 'rank': 24}, {'id': '44779839', 'title': 'Everything to know about UniFi OS Server', 'url': 'https://deluisio.com/networking/unifi/2025/08/03/everything-you-need-to-know-about-unifi-os-server-before-you-waste-time-testing-it/', 'rank': 25}, {'id': '44779839', 'title': 'Everything to know about UniFi OS Server', 'url': 'https://deluisio.com/networking/unifi/2025/08/03/everything-you-need-to-know-about-unifi-os-server-before-you-waste-time-testing-it/', 'rank': 25}, {'id': '44762397', 'title': 'Show HN: Schematra – Sinatra-inspired minimal web framework for Chicken Scheme', 'url': 'https://github.com/rolandoam/schematra', 'rank': 26}, {'id': '44762397', 'title': 'Show HN: Schematra – Sinatra-inspired minimal web framework for Chicken Scheme', 'url': 'https://github.com/rolandoam/schematra', 'rank': 26}, {'id': '44754789', 'title': 'The first lunar road trip', 'url': 'https://nautil.us/the-first-lunar-road-trip-1227738/', 'rank': 27}, {'id': '44754789', 'title': 'The first lunar road trip', 'url': 'https://nautil.us/the-first-lunar-road-trip-1227738/', 'rank': 27}, {'id': '44771808', 'title': 'Lina Khan points to Figma IPO as vindication of M&A scrutiny', 'url': 'https://techcrunch.com/2025/08/02/lina-khan-points-to-figma-ipo-as-vindication-for-ma-scrutiny/', 'rank': 28}, {'id': '44771808', 'title': 'Lina Khan points to Figma IPO as vindication of M&A scrutiny', 'url': 'https://techcrunch.com/2025/08/02/lina-khan-points-to-figma-ipo-as-vindication-for-ma-scrutiny/', 'rank': 28}, {'id': '44780552', 'title': 'Learnable Programming (2012)', 'url': 'https://worrydream.com/LearnableProgramming/', 'rank': 29}, {'id': '44780552', 'title': 'Learnable Programming (2012)', 'url': 'https://worrydream.com/LearnableProgramming/', 'rank': 29}, {'id': '44779178', 'title': 'Shrinking freshwater availability increasing land contribution to sea level rise', 'url': 'https://news.asu.edu/20250725-environment-and-sustainability-new-global-study-shows-freshwater-disappearing-alarming', 'rank': 30}, {'id': '44779178', 'title': 'Shrinking freshwater availability increasing land contribution to sea level rise', 'url': 'https://news.asu.edu/20250725-environment-and-sustainability-new-global-study-shows-freshwater-disappearing-alarming', 'rank': 30}]\n\n\n\n\nStore data as .csv\n\nimport csv\n\nfile = 'hacker_news_posts.csv'\nwith open(file, 'w', newline=\"\") as f:\n    writer = csv.DictWriter(f, fieldnames=['id', 'title', 'url', 'rank'])\n    writer.writeheader()\n    for row in formatted_links:\n        writer.writerow(row)\n\n\n\nStore data in PostgreSQL\n\nStep 1: Installing PostgreSQL\nFollow the PostgreSQL download page for downloads and installation\n\n\nStep 2: Creating a Database Table\nFirst you’ll need a table\n\n#start service\nsudo systemctl start postgresql.service\n\n#log in as a superuser\nsudo -i -u postgres\n\nCREATE DATABASE scrape_demo;\n\n\nCREATE TABLE \"hn_links\" (\n    \"id\" INTEGER NOT NULL,\n    \"title\" VARCHAR NOT NULL,\n    \"url\" VARCHAR NOT NULL,\n    \"rank\" INTEGER NOT NULL\n);\n\n\nStep 3: Install Psycopg2 to Connect to PostgreSQL\npip install psycopg2\nEstablish connection to the database\nEnsure you set password for postgres user, which logs without a password by default\n\nsudo -u postgres psql\n\npostgres=# ALTER USER postgres PASSWORD 'myPassword';\nALTER ROLE\n\n\nimport psycopg2\nimport os\nimport dotenv\ndotenv.load_dotenv()\n\np = os.getenv(\"pass\")\ntable_name = \"hn_links\"\ncsv_path = \"hacker_news_posts.csv\"\n\n\ncon = psycopg2.connect(host=\"127.0.0.1\", port=\"5432\", user=\"postgres\", password = p,database=\"scrape_demo\")\n\n# Get a database cursor\ncur = con.cursor()\n\nr = requests.get('https://news.ycombinator.com')\nsoup = BeautifulSoup(r.text, 'html.parser')\nlinks = soup.findAll('tr', class_='athing')\n\nfor link in links:\n    cur.execute(\"\"\"\n        INSERT INTO hn_links (id, title, url, rank)\n        VALUES (%s, %s, %s, %s)\n        \"\"\",\n        (\n            link['id'],\n            link.find_all('td')[2].a.text,\n            link.find_all('td')[2].a['href'],\n            int(link.find_all('td')[0].span.text.replace('.', ''))\n        )\n    )\n\n# Commit the data\ncon.commit()\n\n# Close our database connections\ncur.close()\ncon.close()\n\n/tmp/ipykernel_21147/1184676145.py:18: DeprecationWarning: Call to deprecated method findAll. (Replaced by find_all) -- Deprecated since version 4.0.0.\n  links = soup.findAll('tr', class_='athing')"
  },
  {
    "objectID": "posts/2025-08-03-Web-scraping-python/index.html#using-scapingbee-python-client",
    "href": "posts/2025-08-03-Web-scraping-python/index.html#using-scapingbee-python-client",
    "title": "Web Scraping with Python",
    "section": "Using ScapingBee Python Client",
    "text": "Using ScapingBee Python Client\nScrapingBee is a subscription API providing a way to bypass any website’s anti-scraping measures.\n\nfrom scrapingbee import ScrapingBeeClient\nimport json\nimport pandas as pd\n\ndotenv.load_dotenv()\nkey = os.getenv(\"spring_bee_api_key\")\n\nsb_client = ScrapingBeeClient(api_key=key)\nurl = \"https://www.aljazeera.com/\"\n\n\n\nclient = ScrapingBeeClient(api_key=key)\n\ndef google_news_headlines_api(country_code='US'):\n\n    extract_rules = {\n        \"news\": {\n        \"selector\": \"article\",\n        \"type\": \"list\",\n            \"output\": {\n                \"title\": \".gPFEn,.JtKRv\",\n                \"source\": \".vr1PYe\",\n                \"time\": \"time@datetime\",\n                \"author\": \".bInasb\",\n                \"link\": \".WwrzSb@href\"\n            }\n        }\n    }\n\n    js_scenario = {\n        \"instructions\":[\n            {\"evaluate\":\"document.querySelectorAll('.WwrzSb').forEach( (e) =&gt; e.href = e.href );\"}\n        ]\n    }\n\n    response =  client.get(\n        f'https://news.google.com/topics/CAAqJggKIiBDQkFTRWdvSUwyMHZNRFZxYUdjU0FtVnVHZ0pWVXlnQVAB?&gl={country_code}',\n        params={ \n            \"custom_google\": \"true\",\n            \"wait_for\": \".bInasb\",\n            \"extract_rules\": extract_rules,\n            \"js_scenario\": js_scenario, \n        },\n        retries=2\n    )\n\n    if response.text.startswith('{\"message\":\"Invalid api key:'):\n        return f\"Oops! It seems you may have missed adding your API KEY or you are using an incorrect key.\\nGet your free API KEY and 1000 free scraping credits by signing up to our platform here: https://app.scrapingbee.com/account/register\"\n    else:\n        def get_info():\n            if len(response.json()['news']) == 0:\n                return \"FAILED TO RETRIEVE NEWS\"\n            else:\n                return \"SUCCESS\"\n\n        return pd.DataFrame({\n            'count': len(response.json()['news']),\n            'news_extracts': response.json()['news'],\n            'info': f\"{response.status_code} {get_info()}\",\n        })\n#country_code: Set the news location; US, IN, etc.\ndf = google_news_headlines_api(country_code='US')\n\nprint(df.iloc[:10])\n\n   count                                      news_extracts         info\n0    263  {'title': 'Texas Democrats Leave State to Stop...  200 SUCCESS\n1    263  {'title': 'Democrats flee Texas to block Repub...  200 SUCCESS\n2    263  {'title': 'A Texas Democratic lawmaker on thei...  200 SUCCESS\n3    263  {'title': 'Texas Democrats flee state to preve...  200 SUCCESS\n4    263  {'title': 'Videos of emaciated hostages condem...  200 SUCCESS\n5    263  {'title': 'Hamas says it will allow aid for ho...  200 SUCCESS\n6    263  {'title': 'Netanyahu asks Red Cross to help ho...  200 SUCCESS\n7    263  {'title': 'Hamas says open to ICRC delivering ...  200 SUCCESS\n8    263  {'title': 'White House advisers defend Trump’s...  200 SUCCESS\n9    263  {'title': 'Trump Fired America’s Economic Data...  200 SUCCESS"
  },
  {
    "objectID": "posts/2025-08-03-Web-scraping-python/index.html#web-scraping-with-scrapy",
    "href": "posts/2025-08-03-Web-scraping-python/index.html#web-scraping-with-scrapy",
    "title": "Web Scraping with Python",
    "section": "Web scraping with Scrapy",
    "text": "Web scraping with Scrapy\nScrapy is a web scraping framework using an event-driven networking infrastracture built around an asynchronous network engine that allows for more efficiency and scalability. It is made of a crawler that handles low-level logic, and a spider that is provider by the user to help the crawler generate request, parse and retrieve data.\nIn this section we use scrapy to scrape product listings available at web-scraping.dev, but first some house-keeping.\nTo install scrapy run pip install scrapy or better still add scrapy to your project’s requirements.txt and run pip install -r requirements.txt. Start a scrapy project by running scrapy startproject &lt;project-name&gt; &lt;project-directory&gt; in terminal. Some scrapy commands below:\n\n!scrapy --help\n\nScrapy 2.13.3 - active project: webscrapingdev\n\nUsage:\n  scrapy &lt;command&gt; [options] [args]\n\nAvailable commands:\n  bench         Run quick benchmark test\n  check         Check spider contracts\n  crawl         Run a spider\n  edit          Edit spider\n  fetch         Fetch a URL using the Scrapy downloader\n  genspider     Generate new spider using pre-defined templates\n  list          List available spiders\n  parse         Parse URL (using its spider) and print the results\n  runspider     Run a self-contained spider (without creating a project)\n  settings      Get settings values\n  shell         Interactive scraping console\n  startproject  Create new project\n  version       Print Scrapy version\n  view          Open URL in browser, as seen by Scrapy\n\nUse \"scrapy &lt;command&gt; -h\" to see more info about a command\n\n\n\nCreating a spider\nrun scrapy genspider &lt;name&gt; &lt;host-to-scrape&gt;\n\n!scrapy genspider products web-scraping.dev\n\nSpider 'products' already exists in module:\n  webscrapingdev.spiders.products\n\n\n\n!scrapy list\n!tree\n\n\nproducts\n\n.\n\n├── article-class.png\n\n├── hacker-element.png\n\n├── hacker_news_posts.csv\n\n├── LICENSE\n\n├── producthunt.json\n\n├── README.md\n\n├── requirements.txt\n\n├── results.json\n\n├── scrapy.cfg\n\n├── web-scrap_files\n\n│   ├── figure-html\n\n│   │   └── cell-2-output-1.png\n\n│   └── libs\n\n│       ├── bootstrap\n\n│       │   ├── bootstrap-b9f025fa521194ab51f5de92fbd134be.min.css\n\n│       │   ├── bootstrap-icons.css\n\n│       │   ├── bootstrap-icons.woff\n\n│       │   └── bootstrap.min.js\n\n│       ├── clipboard\n\n│       │   └── clipboard.min.js\n\n│       └── quarto-html\n\n│           ├── anchor.min.js\n\n│           ├── popper.min.js\n\n│           ├── quarto.js\n\n│           ├── quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css\n\n│           ├── tabsets\n\n│           │   └── tabsets.js\n\n│           ├── tippy.css\n\n│           └── tippy.umd.min.js\n\n├── web-scrap.html\n\n├── webscrapingdev\n\n│   ├── __init__.py\n\n│   ├── items.py\n\n│   ├── middlewares.py\n\n│   ├── pipelines.py\n\n│   ├── __pycache__\n\n│   │   ├── __init__.cpython-312.pyc\n\n│   │   └── settings.cpython-312.pyc\n\n│   ├── settings.py\n\n│   └── spiders\n\n│       ├── __init__.py\n\n│       ├── products.py\n\n│       └── __pycache__\n\n│           ├── __init__.cpython-312.pyc\n\n│           └── products.cpython-312.pyc\n\n├── web-scrap.ipynb\n\n├── web-scrap.py\n\n├── web-scrap.qmd\n\n└── web-scrap.quarto_ipynb\n\n\n\n12 directories, 38 files\n\n\n\n\nif you open the generated spider - products.py, you’ll find the following\nimport scrapy\n\n\nclass ProductsSpider(scrapy.Spider):\n    name = \"products\"\n    allowed_domains = [\"web-scraping.dev\"]\n    start_urls = [\"https://web-scraping.dev\"]\n\n    def parse(self, response):\n        pass\n\n\nname is used as a reference to the spider for scrapy commands like crawl` - this would run the scraper\nallowed_domains is a safety feauture restricting this spider to crawl only particular domains.\nstart_urls indicates the spider starting point while parse() is the first callback to execute above instructions.\n\n\n\nAdding crawling logic\nWe want our start_urls to be some topic directories e.g., https://www.producthunt.com/topics/developer-tools and our parse() callback method to find all product links and schedule them to be scrapped:\n# /spiders/products.py\nimport scrapy\nfrom scrapy.http import Response, Request\n\n\nclass ProductsSpider(scrapy.Spider):\n    name = 'products'\n    allowed_domains = ['web-scraping.dev']\n    start_urls = [\n        'https://web-scraping.dev/products',\n    ]\n\n    def parse(self, response: Response):\n        product_urls = response.xpath(\n            \"//div[@class='row product']/div/h3/a/@href\"\n        ).getall()\n        for url in product_urls:\n            yield Request(url, callback=self.parse_product)\n        # or shortcut in scrapy &gt;2.0\n        # yield from response.follow_all(product_urls, callback=self.parse_product)\n    \n    def parse_product(self, response: Response):\n        print(response)\n\n\n\nAdding Parsing Logic\nPopulate parse_product()\n# /spiders/products.py\n...\n\n    def parse_product(self, response: Response):\n        yield {\n            \"title\": response.xpath(\"//h3[contains(@class, 'product-title')]/text()\").get(),\n            \"price\": response.xpath(\"//span[contains(@class, 'product-price')]/text()\").get(),\n            \"image\": response.xpath(\"//div[contains(@class, 'product-image')]/img/@src\").get(),\n            \"description\": response.xpath(\"//p[contains(@class, 'description')]/text()\").get()\n        }\n\n\n\nBasic Settings\nAdjust recommended settings:\n# settings.py\n# will ignore /robots.txt rules that might prevent scraping\nROBOTSTXT_OBEY = False\n# will cache all request to /httpcache directory which makes running spiders in development much quicker\n# tip: to refresh cache just delete /httpcache directory\nHTTPCACHE_ENABLED = True\n# while developing we want to see debug logs\nLOG_LEVEL = \"DEBUG\" # or \"INFO\" in production\n\n# to avoid basic bot detection we want to set some basic headers\nDEFAULT_REQUEST_HEADERS = {\n    # we should use headers\n    'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\",\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\n\nRunning Spiders\nEither through the scrapy command or explicitly calling scrapy using a Python script.\n\n%%capture\n!scrapy crawl products\n\n\n\nSaving results\n\n%%capture\n\n!scrapy crawl products --output results.json\n\n\n!tree\n\n\n.\n\n├── article-class.png\n\n├── hacker-element.png\n\n├── hacker_news_posts.csv\n\n├── LICENSE\n\n├── producthunt.json\n\n├── README.md\n\n├── requirements.txt\n\n├── results.json\n\n├── scrapy.cfg\n\n├── web-scrap_files\n\n│   ├── figure-html\n\n│   │   └── cell-2-output-1.png\n\n│   └── libs\n\n│       ├── bootstrap\n\n│       │   ├── bootstrap-b9f025fa521194ab51f5de92fbd134be.min.css\n\n│       │   ├── bootstrap-icons.css\n\n│       │   ├── bootstrap-icons.woff\n\n│       │   └── bootstrap.min.js\n\n│       ├── clipboard\n\n│       │   └── clipboard.min.js\n\n│       └── quarto-html\n\n│           ├── anchor.min.js\n\n│           ├── popper.min.js\n\n│           ├── quarto.js\n\n│           ├── quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css\n\n│           ├── tabsets\n\n│           │   └── tabsets.js\n\n│           ├── tippy.css\n\n│           └── tippy.umd.min.js\n\n├── web-scrap.html\n\n├── webscrapingdev\n\n│   ├── __init__.py\n\n│   ├── items.py\n\n│   ├── middlewares.py\n\n│   ├── pipelines.py\n\n│   ├── __pycache__\n\n│   │   ├── __init__.cpython-312.pyc\n\n│   │   └── settings.cpython-312.pyc\n\n│   ├── settings.py\n\n│   └── spiders\n\n│       ├── __init__.py\n\n│       ├── products.py\n\n│       └── __pycache__\n\n│           ├── __init__.cpython-312.pyc\n\n│           └── products.cpython-312.pyc\n\n├── web-scrap.ipynb\n\n├── web-scrap.py\n\n├── web-scrap.qmd\n\n└── web-scrap.quarto_ipynb\n\n\n\n12 directories, 38 files\n\n\n\n\n\nimport json\njson_file = 'results.json'\nwith open(json_file) as f:\n    j_obj = json.load(f)\n\n\njson_fmt = json.dumps(j_obj, indent=2)\nprint(json_fmt)\n\n[\n  {\n    \"title\": \"Blue Energy Potion\",\n    \"price\": \"$4.99\",\n    \"image\": \"https://web-scraping.dev/assets/products/blue-potion.webp\",\n    \"description\": \"Ignite your gaming sessions with our 'Blue Energy Potion', a premium energy drink crafted for dedicated gamers. Inspired by the classic video game potions, this energy drink provides a much-needed boost to keep you focused and energized. It's more than just an energy drink - it's an ode to the gaming culture, packaged in an aesthetically pleasing potion-like bottle that'll make you feel like you're in your favorite game world. Drink up and game on!\"\n  },\n  {\n    \"title\": \"Red Energy Potion\",\n    \"price\": \"$4.99\",\n    \"image\": \"https://web-scraping.dev/assets/products/red-potion.webp\",\n    \"description\": \"Elevate your game with our 'Red Potion', an extraordinary energy drink that's as enticing as it is effective. This fiery red potion delivers an explosive berry flavor and an energy kick that keeps you at the top of your game. Are you ready to level up?\"\n  },\n  {\n    \"title\": \"Teal Energy Potion\",\n    \"price\": \"$4.99\",\n    \"image\": \"https://web-scraping.dev/assets/products/teal-potion.webp\",\n    \"description\": \"Experience a surge of vitality with our 'Teal Potion', an exceptional energy drink designed for the gaming community. With its intriguing teal color and a flavor that keeps you asking for more, this potion is your best companion during those long gaming nights. Every sip is an adventure - let the quest begin!\"\n  },\n  {\n    \"title\": \"Dark Red Energy Potion\",\n    \"price\": \"$4.99\",\n    \"image\": \"https://web-scraping.dev/assets/products/darkred-potion.webp\",\n    \"description\": \"Unleash the power within with our 'Dark Red Potion', an energy drink as intense as the games you play. Its deep red color and bold cherry cola flavor are as inviting as they are invigorating. Bring out the best in your gaming performance, and unlock your full potential.\"\n  },\n  {\n    \"title\": \"Box of Chocolate Candy\",\n    \"price\": \"$9.99 \",\n    \"image\": \"https://web-scraping.dev/assets/products/orange-chocolate-box-small-1.webp\",\n    \"description\": \"Indulge your sweet tooth with our Box of Chocolate Candy. Each box contains an assortment of rich, flavorful chocolates with a smooth, creamy filling. Choose from a variety of flavors including zesty orange and sweet cherry. Whether you're looking for the perfect gift or just want to treat yourself, our Box of Chocolate Candy is sure to satisfy.\"\n  }\n]"
  },
  {
    "objectID": "posts/2025-08-08-data-enginnering/index.html",
    "href": "posts/2025-08-08-data-enginnering/index.html",
    "title": "End-to-end Data Engineering with Python DuckDB, Google Cloud, dbt",
    "section": "",
    "text": "BibTeX citation:@online{okola2025,\n  author = {Okola, Basil},\n  title = {End-to-End {Data} {Engineering} with {Python} {DuckDB,}\n    {Google} {Cloud,} Dbt},\n  date = {2025-08-08},\n  url = {https://bokola.github.io/posts/2025-08-08-data-engineering/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nOkola, Basil. 2025. “End-to-End Data Engineering with Python\nDuckDB, Google Cloud, Dbt.” August 8, 2025. https://bokola.github.io/posts/2025-08-08-data-engineering/.\nThis is a ‘do it along’ following DuckDB material. We will source data from PyPi, a repository of python packages providing logs of how given libraries have been used across platforms, transform it into relevant tables, and feed it into a dashboard using SQL, dtb - a tool for building modular, maintainable data pipelines to power analytics, and DuckDB - an in-process SQL online analytical processing (OLAPs) relational database management system; then use evidence to create a dashboard utilizing SQL and markdown."
  },
  {
    "objectID": "posts/2025-08-08-data-enginnering/index.html#part-1-ingestion-pipeline",
    "href": "posts/2025-08-08-data-enginnering/index.html#part-1-ingestion-pipeline",
    "title": "End-to-end Data Engineering with Python DuckDB, Google Cloud, dbt",
    "section": "Part 1: Ingestion pipeline",
    "text": "Part 1: Ingestion pipeline\n\nSetup & requirements\n\nPython 3.12\nPoetry for dependency management\nMake to run Makefile commands\nA Google Cloud account to fetch the source data. Free tier covers any computing cost.\nA host of libraries: poetry duckdb google-cloud-bigquery google-auth google-cloud-bigquery-storage pyarrow pandas fire loguru pydantic pytest ruff\n\nThe architecture woul look like below\n\nfrom PIL import Image\nfrom IPython.display import display\n\nimg_f = \"./docs/architecture.png\"\nimg = Image.open(img_f)\ndisplay(img)"
  },
  {
    "objectID": "posts/2025-08-08-data-enginnering/index.html#exploring-the-data",
    "href": "posts/2025-08-08-data-enginnering/index.html#exploring-the-data",
    "title": "End-to-end Data Engineering with Python DuckDB, Google Cloud, dbt",
    "section": "Exploring the data",
    "text": "Exploring the data\nHead to your google cloud console, then under BigQuery search “file_downloads” and click SEARCH ALL PROJECTS.\n\nimg_f = \"./docs/big_query_search.png\"\nimg = Image.open(img_f)\ndisplay(img)\n\n\n\n\n\n\n\n\nLimit your query to a given timestamp as the data is very big.\n\nimg_f = \"./docs/query.png\"\nimg = Image.open(img_f)\ndisplay(img)\n\n\n\n\n\n\n\n\nThe ingestion part is bundled in /ingestion directory with the following primary files:\n\n!tree ./ingestion\n\n\n./ingestion\n\n├── bigquery.py\n\n├── duck.py\n\n├── models.py\n\n├── pipeline.py\n\n└── __pycache__\n\n    ├── bigquery.cpython-312.pyc\n\n    └── pipeline.cpython-312.pyc\n\n\n\n2 directories, 6 files\n\n\n\n\n\nbigquery.py has BigQuery helpers to get the client, get query results and query the public dataset\nduck.py contains DuckDB helpers to create a table from a dataframe, and write to a local or MotherDuck database\nmodels.py defines table schema\npipeline runs the ingestion pipeline use pydantic syntax\n\nThe pipeline was successfuly executed and data written locally and to motherduck:\n\nimg_f = \"./docs/motherduck_db.png\"\nimg = Image.open(img_f)\ndisplay(img)"
  },
  {
    "objectID": "posts/2025-08-08-data-engineering/index.html",
    "href": "posts/2025-08-08-data-engineering/index.html",
    "title": "End-to-end Data Engineering with Python DuckDB, Google Cloud, dbt",
    "section": "",
    "text": "BibTeX citation:@online{okola2025,\n  author = {Okola, Basil},\n  title = {End-to-End {Data} {Engineering} with {Python} {DuckDB,}\n    {Google} {Cloud,} Dbt},\n  date = {2025-08-08},\n  url = {https://bokola.github.io/posts/2025-08-08-data-engineering/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nOkola, Basil. 2025. “End-to-End Data Engineering with Python\nDuckDB, Google Cloud, Dbt.” August 8, 2025. https://bokola.github.io/posts/2025-08-08-data-engineering/.\nThis is a ‘do it along’ following DuckDB material. We will source data from PyPi, a repository of python packages providing logs of how given libraries have been used across platforms, transform it into relevant tables, and feed it into a dashboard using SQL, dtb - a tool for building modular, maintainable data pipelines to power analytics, and DuckDB - an in-process SQL online analytical processing (OLAPs) relational database management system; then use evidence to create a dashboard utilizing SQL and markdown."
  },
  {
    "objectID": "posts/2025-08-08-data-engineering/index.html#part-1-ingestion-pipeline",
    "href": "posts/2025-08-08-data-engineering/index.html#part-1-ingestion-pipeline",
    "title": "End-to-end Data Engineering with Python DuckDB, Google Cloud, dbt",
    "section": "Part 1: Ingestion pipeline",
    "text": "Part 1: Ingestion pipeline\n\nSetup & requirements\n\nPython 3.12\nPoetry for dependency management\nMake to run Makefile commands\nA Google Cloud account to fetch the source data. Free tier covers any computing cost.\nA host of libraries: poetry duckdb google-cloud-bigquery google-auth google-cloud-bigquery-storage pyarrow pandas fire loguru pydantic pytest ruff\n\nThe architecture woul look like below\n\nfrom PIL import Image\nfrom IPython.display import display\n\nimg_f = \"./docs/architecture.png\"\nimg = Image.open(img_f)\ndisplay(img)"
  },
  {
    "objectID": "posts/2025-08-08-data-engineering/index.html#exploring-the-data",
    "href": "posts/2025-08-08-data-engineering/index.html#exploring-the-data",
    "title": "End-to-end Data Engineering with Python DuckDB, Google Cloud, dbt",
    "section": "Exploring the data",
    "text": "Exploring the data\nHead to your google cloud console, then under BigQuery search “file_downloads” and click SEARCH ALL PROJECTS.\n\nimg_f = \"./docs/big_query_search.png\"\nimg = Image.open(img_f)\ndisplay(img)\n\n\n\n\n\n\n\n\nLimit your query to a given timestamp as the data is very big.\n\nimg_f = \"./docs/query.png\"\nimg = Image.open(img_f)\ndisplay(img)\n\n\n\n\n\n\n\n\nThe ingestion part is bundled in /ingestion directory with the following primary files:\n\n!tree ./ingestion\n\n\n./ingestion\n\n├── bigquery.py\n\n├── duck.py\n\n├── models.py\n\n├── pipeline.py\n\n└── __pycache__\n\n    ├── bigquery.cpython-312.pyc\n\n    └── pipeline.cpython-312.pyc\n\n\n\n2 directories, 6 files\n\n\n\n\n\nbigquery.py has BigQuery helpers to get the client, get query results and query the public dataset\nduck.py contains DuckDB helpers to create a table from a dataframe, and write to a local or MotherDuck database\nmodels.py defines table schema\npipeline runs the ingestion pipeline use pydantic syntax\n\nThe pipeline was successfuly executed and data written locally and to motherduck:\n\nimg_f = \"./docs/motherduck_db.png\"\nimg = Image.open(img_f)\ndisplay(img)"
  },
  {
    "objectID": "posts/2025-10-29-data-pub-ready-viz/index.html",
    "href": "posts/2025-10-29-data-pub-ready-viz/index.html",
    "title": "Publication ready visualization in R",
    "section": "",
    "text": "CitationBibTeX citation:@online{okola2025,\n  author = {Okola, Basil},\n  title = {Publication Ready Visualization in {R}},\n  date = {2025-10-29},\n  url = {https://bokola.github.io/posts/2025-10-29-pub-ready-viz/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nOkola, Basil. 2025. “Publication Ready Visualization in R.”\nOctober 29, 2025. https://bokola.github.io/posts/2025-10-29-pub-ready-viz/."
  },
  {
    "objectID": "posts/2025-10-29-pub-ready-viz/index.html",
    "href": "posts/2025-10-29-pub-ready-viz/index.html",
    "title": "Publication ready visualization in R",
    "section": "",
    "text": "CitationBibTeX citation:@online{okola2025,\n  author = {Okola, Basil},\n  title = {Publication Ready Visualization in {R}},\n  date = {2025-10-29},\n  url = {https://bokola.github.io/posts/2025-10-29-pub-ready-viz/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nOkola, Basil. 2025. “Publication Ready Visualization in R.”\nOctober 29, 2025. https://bokola.github.io/posts/2025-10-29-pub-ready-viz/."
  },
  {
    "objectID": "posts/2025-10-30-interactive-maps/index.html",
    "href": "posts/2025-10-30-interactive-maps/index.html",
    "title": "Interactive maps (R)",
    "section": "",
    "text": "This document explores making interactive maps using R. There are a number of R packages that can be used to generate such visualizations. We consider just a small number (not exhaustively)."
  },
  {
    "objectID": "posts/2025-10-30-interactive-maps/index.html#data",
    "href": "posts/2025-10-30-interactive-maps/index.html#data",
    "title": "Interactive maps (R)",
    "section": "Data",
    "text": "Data\nWe use gapminder dataset from {gapminder} package. It is contains data from 187 countries covering the periods 1952 - 2007 with the following columns:\n\ncountry: Country name\ncontinent: Continental territory of a country\nyear: 1952 - 2007\nlifeExp: Life expectancy in years\npop: population size\ngdpPercap: GDP per capita in infaltion-adjusted dollars\n\nThe second dataset is the meteorite landings from Tidytuesday project from the Meteoritical Society of NASA. It comes with the following variables:\n\nname: Meteorite name\nmass: Mass in grams\nlat: latitude\nlong: longitude\nfall: fall or found meteorite\n\n\n\nCode\npacman::p_load(\n    leaflet\n    ,gapminder\n    ,echarts4r\n    ,tidyverse\n    ,ggiraph\n    ,widgetframe\n    ,ggthemes\n    ,plotly\n    ,viridis\n    ,DT\n)\n\n\nFirst we organize the datasets:\n\n\nCode\n# 1. gapminder\n\n# country codes\ncodes &lt;- gapminder::country_codes\n# countries with info unfiltered version\ngapminder &lt;- gapminder::gapminder_unfiltered\n# join\ngapminder &lt;- gapminder %&gt;% left_join(codes) %&gt;%\n  mutate(code = iso_alpha)\n\n\nJoining with `by = join_by(country)`\n\n\nCode\n# a map of the world - Antarctica removed\nworld &lt;- map_data(\"world\") %&gt;%\n  filter(!grepl(\"antarctica\", region, ignore.case = T))\ngapminder_df &lt;- gapminder %&gt;%\n  inner_join(maps::iso3166 %&gt;% \n  select(a3, mapname), by = c(code = \"a3\")\n  ) %&gt;%\n    mutate(\n      mapname = str_remove(mapname, \"\\\\(.*\")\n    )\n\n\nWarning in inner_join(., maps::iso3166 %&gt;% select(a3, mapname), by = c(code = \"a3\")): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 605 of `x` matches multiple rows in `y`.\nℹ Row 2 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\nGapminder dataset:\n\n\nCode\ndatatable(gapminder_df)\n\n\n\n\n\n\n\n\nCode\nmeteorites &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-06-11/meteorites.csv\")\n\n\nRows: 45716 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): name, name_type, class, fall, geolocation\ndbl (5): id, mass, year, lat, long\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nMeteorite dataset:\n\n\nCode\ndatatable(meteorites)\n\n\nWarning in instance$preRenderHook(instance): It seems your data is too big for\nclient-side DataTables. You may consider server-side processing:\nhttps://rstudio.github.io/DT/server.html"
  },
  {
    "objectID": "posts/2025-10-30-interactive-maps/index.html#interactive-choropleth-map-with-ggiraph",
    "href": "posts/2025-10-30-interactive-maps/index.html#interactive-choropleth-map-with-ggiraph",
    "title": "Interactive maps (R)",
    "section": "Interactive choropleth map with {ggiraph}",
    "text": "Interactive choropleth map with {ggiraph}\nTo turn a static choropleth map by invoking a tooltip when we hover the pointer over a country, we can use {ggiraph} - geom_polygon_interactive() & girafe and {widgetframe} - framewidget() packages.\nIt is useful creating a reusable theming function:\n\n\nCode\ntheme_helper &lt;- function(){\n   theme(\n    axis.line = element_blank(),\n    axis.text = element_blank(),\n    axis.title = element_blank(),\n    axis.ticks = element_blank(),\n    plot.background = element_rect(fill = \"snow\", color = NA),\n    panel.background = element_rect(fill= \"snow\", color = NA),\n    plot.title = element_text(size = 16, hjust = 0.5),\n    plot.subtitle = element_text(size = 12, hjust = 0.5),\n    plot.caption = element_text(size = 8, hjust = 1),\n    legend.title = element_text(color = \"grey40\", size = 8),\n    legend.text = element_text(color = \"grey40\", size = 7, hjust = 0),\n    legend.position = c(0.05, 0.25),\n    plot.margin = unit(c(0.5,2,0.5,1), \"cm\")) \n}\n\n\n\n\nCode\nlife_exp_map &lt;- gapminder_df %&gt;%\n  filter(year == 2007) %&gt;%\n  right_join(world, by = c(mapname = \"region\")) %&gt;%\n  ggplot() +\n  geom_polygon_interactive(\n    color = \"white\", size = 0.01, \n    aes(long, lat, group = group, fill = lifeExp,\n    tooltip = sprintf(\"%s&lt;br/&gt;%s\", country, lifeExp))\n  ) +\n    theme_void() + \n    scale_fill_viridis(option = \"B\") +\n    labs(\n      title = \"Life Expectancy\",\n      subtitle = \"Year: 2007\",\n      caption = \"Source: gapminder.org\",\n      fill = \"Years\"\n    ) \n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\nℹ The deprecated feature was likely used in the ggiraph package.\n  Please report the issue at &lt;https://github.com/davidgohel/ggiraph/issues&gt;.\n\n\nCode\nlife_exp_map &lt;- life_exp_map + theme_helper() + coord_fixed(ratio = 1.3)\n\n\nPrint it interactively\n\nCode\nwidgetframe::frameWidget(girafe(code = print(life_exp_map)))"
  },
  {
    "objectID": "posts/2025-10-30-interactive-maps/index.html#interactive-choropleth-maps-with-plotly",
    "href": "posts/2025-10-30-interactive-maps/index.html#interactive-choropleth-maps-with-plotly",
    "title": "Interactive maps (R)",
    "section": "Interactive choropleth maps with {plotly}",
    "text": "Interactive choropleth maps with {plotly}\n{plotly} on top of tooltips also allows zooming, lasso/box selections or downloading the map as .png.\n\nCode\nlife_exp07 &lt;- gapminder_df %&gt;%\n  filter(year == 2007) %&gt;% \n  select(mapname, code, lifeExp)\n\np_07 &lt;- plot_geo(life_exp07)\n\np_07 &lt;- p_07 %&gt;% add_trace(\n  z = ~lifeExp, color = ~ lifeExp, colors = 'Oranges',\n  text = ~mapname, locations = ~ code\n) %&gt;% colorbar(title = \"Years\")\n\np_07 &lt;- p_07 %&gt;%\n  layout(\n    title = 'Life Expectancy in 2007 &lt;br&gt;Source:&lt;a href= \"https://www.gapminder.org\"&gt; gapminder.org&lt;/a&gt;', geo = p_07\n  )\n\np_07"
  },
  {
    "objectID": "posts/2025-10-30-interactive-maps/index.html#interactive-points-using-plotly",
    "href": "posts/2025-10-30-interactive-maps/index.html#interactive-points-using-plotly",
    "title": "Interactive maps (R)",
    "section": "Interactive points using {plotly}",
    "text": "Interactive points using {plotly}\n\n\nCode\nmeteorites_fell &lt;- meteorites %&gt;%\n  filter(fall == \"Fell\")\n\nmeteorites_map &lt;- list(\n  #scope = 'usa',\n  projection = list(type = 'Mercator'),\n  showland = TRUE,\n  landcolor = toRGB(\"grey80\")\n)\n\n\nmeteo_map &lt;- plot_geo(meteorites_fell, lat = ~lat, lon = ~long)\nmeteo_map &lt;- meteo_map %&gt;% add_markers(\n  text = ~paste(paste(\"Name:\", name), \n                paste(\"Year:\", year), \n                paste(\"Mass:\", mass), sep = \"&lt;br /&gt;\"),\n  color = ~mass, symbol = I(\"circle-dot\"), size = I(8), hoverinfo = \"text\"\n)\nmeteo_map &lt;- meteo_map %&gt;% colorbar(title = \"Mass\")\n\n\nWarning: Ignoring 10 observations\n\n\nCode\nmeteo_map &lt;- meteo_map %&gt;% layout(\n  title = 'Meteorite Landings&lt;br /&gt;(Meteorite falls)', geo = meteorites_map\n)\n\nmeteo_map"
  },
  {
    "objectID": "posts/2025-10-30-interactive-maps/index.html#interactive-maps-with-echarts4r",
    "href": "posts/2025-10-30-interactive-maps/index.html#interactive-maps-with-echarts4r",
    "title": "Interactive maps (R)",
    "section": "interactive maps with {echarts4r}",
    "text": "interactive maps with {echarts4r}\n\n\nCode\ndf &lt;- gapminder %&gt;% \n    mutate(Name = recode_factor(country,\n                              `Congo, Dem. Rep.`= \"Dem. Rep. Congo\",\n                              `Congo, Rep.`= \"Congo\",\n                              `Cote d'Ivoire`= \"Côte d'Ivoire\",\n                              `Central African Republic`= \"Central African Rep.\",\n                              `Yemen, Rep.`= \"Yemen\",\n                              `Korea, Rep.`= \"Korea\",\n                              `Korea, Dem. Rep.`= \"Dem. Rep. Korea\",\n                              `Czech Republic`= \"Czech Rep.\",\n                              `Slovak Republic`= \"Slovakia\",\n                              `Dominican Republic`= \"Dominican Rep.\",\n                              `Equatorial Guinea`= \"Eq. Guinea\"))\n\ndf %&gt;% group_by(year) %&gt;%\n  e_chart(Name, timeline = TRUE) %&gt;%\n  e_map(lifeExp) %&gt;%\n  e_visual_map(\n    min = 30, max = 90, type = \"piecewise\"\n  ) %&gt;%\n    e_title(\"Life expectancy by country and year\", left = \"center\") %&gt;%\n    e_tooltip(\n      trigger = \"item\", formatter = e_tooltip_choro_formatter()\n    )"
  },
  {
    "objectID": "posts/2025-10-30-interactive-maps/interactive-maps.html",
    "href": "posts/2025-10-30-interactive-maps/interactive-maps.html",
    "title": "Interactive maps (R)",
    "section": "",
    "text": "This document explores making interactive maps using R. There are a number of R packages that can be used to generate such visualizations. We consider just a small number (not exhaustively)."
  },
  {
    "objectID": "posts/2025-10-30-interactive-maps/interactive-maps.html#data",
    "href": "posts/2025-10-30-interactive-maps/interactive-maps.html#data",
    "title": "Interactive maps (R)",
    "section": "Data",
    "text": "Data\nWe use gapminder dataset from {gapminder} package. It is contains data from 187 countries covering the periods 1952 - 2007 with the following columns:\n\ncountry: Country name\ncontinent: Continental territory of a country\nyear: 1952 - 2007\nlifeExp: Life expectancy in years\npop: population size\ngdpPercap: GDP per capita in infaltion-adjusted dollars\n\nThe second dataset is the meteorite landings from Tidytuesday project from the Meteoritical Society of NASA. It comes with the following variables:\n\nname: Meteorite name\nmass: Mass in grams\nlat: latitude\nlong: longitude\nfall: fall or found meteorite\n\n\n\nCode\npacman::p_load(\n    leaflet\n    ,gapminder\n    ,echarts4r\n    ,tidyverse\n    ,ggiraph\n    ,widgetframe\n    ,ggthemes\n    ,plotly\n    ,viridis\n    ,DT\n)\n\n\nFirst we organize the datasets:\n\n\nCode\n# 1. gapminder\n\n# country codes\ncodes &lt;- gapminder::country_codes\n# countries with info unfiltered version\ngapminder &lt;- gapminder::gapminder_unfiltered\n# join\ngapminder &lt;- gapminder %&gt;% left_join(codes) %&gt;%\n  mutate(code = iso_alpha)\n# a map of the world - Antarctica removed\nworld &lt;- map_data(\"world\") %&gt;%\n  filter(!grepl(\"antarctica\", region, ignore.case = T))\ngapminder_df &lt;- gapminder %&gt;%\n  inner_join(maps::iso3166 %&gt;% \n  select(a3, mapname), by = c(code = \"a3\")\n  ) %&gt;%\n    mutate(\n      mapname = str_remove(mapname, \"\\\\(.*\")\n    )\n\n\nGapminder dataset:\n\n\nCode\ndatatable(gapminder_df)\n\n\n\n\n\n\n\n\nCode\nmeteorites &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-06-11/meteorites.csv\")\n\n\nMeteorite dataset:\n\n\nCode\ndatatable(meteorites)"
  },
  {
    "objectID": "posts/2025-10-30-interactive-maps/interactive-maps.html#interactive-choropleth-map-with-ggiraph",
    "href": "posts/2025-10-30-interactive-maps/interactive-maps.html#interactive-choropleth-map-with-ggiraph",
    "title": "Interactive maps (R)",
    "section": "Interactive choropleth map with {ggiraph}",
    "text": "Interactive choropleth map with {ggiraph}\nTo turn a static choropleth map by invoking a tooltip when we hover the pointer over a country, we can use {ggiraph} - geom_polygon_interactive() & girafe and {widgetframe} - framewidget() packages.\nIt is useful creating a reusable theming function:\n\n\nCode\ntheme_helper &lt;- function(){\n   theme(\n    axis.line = element_blank(),\n    axis.text = element_blank(),\n    axis.title = element_blank(),\n    axis.ticks = element_blank(),\n    plot.background = element_rect(fill = \"snow\", color = NA),\n    panel.background = element_rect(fill= \"snow\", color = NA),\n    plot.title = element_text(size = 16, hjust = 0.5),\n    plot.subtitle = element_text(size = 12, hjust = 0.5),\n    plot.caption = element_text(size = 8, hjust = 1),\n    legend.title = element_text(color = \"grey40\", size = 8),\n    legend.text = element_text(color = \"grey40\", size = 7, hjust = 0),\n    legend.position = c(0.05, 0.25),\n    plot.margin = unit(c(0.5,2,0.5,1), \"cm\")) \n}\n\n\n\n\nCode\nlife_exp_map &lt;- gapminder_df %&gt;%\n  filter(year == 2007) %&gt;%\n  right_join(world, by = c(mapname = \"region\")) %&gt;%\n  ggplot() +\n  geom_polygon_interactive(\n    color = \"white\", size = 0.01, \n    aes(long, lat, group = group, fill = lifeExp,\n    tooltip = sprintf(\"%s&lt;br/&gt;%s\", country, lifeExp))\n  ) +\n    theme_void() + \n    scale_fill_viridis(option = \"B\") +\n    labs(\n      title = \"Life Expectancy\",\n      subtitle = \"Year: 2007\",\n      caption = \"Source: gapminder.org\",\n      fill = \"Years\"\n    ) \n\nlife_exp_map &lt;- life_exp_map + theme_helper() + coord_fixed(ratio = 1.3)\n\n\nPrint it interactively\n\nCode\nwidgetframe::frameWidget(girafe(code = print(life_exp_map)))"
  },
  {
    "objectID": "posts/2025-10-30-interactive-maps/interactive-maps.html#interactive-choropleth-maps-with-plotly",
    "href": "posts/2025-10-30-interactive-maps/interactive-maps.html#interactive-choropleth-maps-with-plotly",
    "title": "Interactive maps (R)",
    "section": "Interactive choropleth maps with {plotly}",
    "text": "Interactive choropleth maps with {plotly}\n{plotly} on top of tooltips also allows zooming, lasso/box selections or downloading the map as .png.\n\nCode\nlife_exp07 &lt;- gapminder_df %&gt;%\n  filter(year == 2007) %&gt;% \n  select(mapname, code, lifeExp)\n\np_07 &lt;- plot_geo(life_exp07)\n\np_07 &lt;- p_07 %&gt;% add_trace(\n  z = ~lifeExp, color = ~ lifeExp, colors = 'Oranges',\n  text = ~mapname, locations = ~ code\n) %&gt;% colorbar(title = \"Years\")\n\np_07 &lt;- p_07 %&gt;%\n  layout(\n    title = 'Life Expectancy in 2007 &lt;br&gt;Source:&lt;a href= \"https://www.gapminder.org\"&gt; gapminder.org&lt;/a&gt;', geo = p_07\n  )\n\np_07"
  },
  {
    "objectID": "posts/2025-10-30-interactive-maps/interactive-maps.html#interactive-points-using-plotly",
    "href": "posts/2025-10-30-interactive-maps/interactive-maps.html#interactive-points-using-plotly",
    "title": "Interactive maps (R)",
    "section": "Interactive points using {plotly}",
    "text": "Interactive points using {plotly}\n\n\nCode\nmeteorites_fell &lt;- meteorites %&gt;%\n  filter(fall == \"Fell\")\n\nmeteorites_map &lt;- list(\n  #scope = 'usa',\n  projection = list(type = 'Mercator'),\n  showland = TRUE,\n  landcolor = toRGB(\"grey80\")\n)\n\n\nmeteo_map &lt;- plot_geo(meteorites_fell, lat = ~lat, lon = ~long)\nmeteo_map &lt;- meteo_map %&gt;% add_markers(\n  text = ~paste(paste(\"Name:\", name), \n                paste(\"Year:\", year), \n                paste(\"Mass:\", mass), sep = \"&lt;br /&gt;\"),\n  color = ~mass, symbol = I(\"circle-dot\"), size = I(8), hoverinfo = \"text\"\n)\nmeteo_map &lt;- meteo_map %&gt;% colorbar(title = \"Mass\")\nmeteo_map &lt;- meteo_map %&gt;% layout(\n  title = 'Meteorite Landings&lt;br /&gt;(Meteorite falls)', geo = meteorites_map\n)\n\nmeteo_map"
  },
  {
    "objectID": "posts/2025-10-30-interactive-maps/interactive-maps.html#interactive-maps-with-echarts4r",
    "href": "posts/2025-10-30-interactive-maps/interactive-maps.html#interactive-maps-with-echarts4r",
    "title": "Interactive maps (R)",
    "section": "interactive maps with {echarts4r}",
    "text": "interactive maps with {echarts4r}\n\n\nCode\ndf &lt;- gapminder %&gt;% \n    mutate(Name = recode_factor(country,\n                              `Congo, Dem. Rep.`= \"Dem. Rep. Congo\",\n                              `Congo, Rep.`= \"Congo\",\n                              `Cote d'Ivoire`= \"Côte d'Ivoire\",\n                              `Central African Republic`= \"Central African Rep.\",\n                              `Yemen, Rep.`= \"Yemen\",\n                              `Korea, Rep.`= \"Korea\",\n                              `Korea, Dem. Rep.`= \"Dem. Rep. Korea\",\n                              `Czech Republic`= \"Czech Rep.\",\n                              `Slovak Republic`= \"Slovakia\",\n                              `Dominican Republic`= \"Dominican Rep.\",\n                              `Equatorial Guinea`= \"Eq. Guinea\"))\n\ndf %&gt;% group_by(year) %&gt;%\n  e_chart(Name, timeline = TRUE) %&gt;%\n  e_map(lifeExp) %&gt;%\n  e_visual_map(\n    min = 30, max = 90, type = \"piecewise\"\n  ) %&gt;%\n    e_title(\"Life expectancy by country and year\", left = \"center\") %&gt;%\n    e_tooltip(\n      trigger = \"item\", formatter = e_tooltip_choro_formatter()\n    )"
  },
  {
    "objectID": "posts/2026-01-12-Adaptive-designs/index.html",
    "href": "posts/2026-01-12-Adaptive-designs/index.html",
    "title": "Adaptive Methods for Clinical Trials",
    "section": "",
    "text": "Adaptive trials allow for modifications along the course of the trials. Common modifications include sample size adjustment, dropping an arm (if futile), or even adjusting allocation probabilities. Group sequential design is one of the most utilized forms of adaptive designs allowing for multiple planned interim analyses with pre-specified rules for early stopping. Here, we take a simulation approach to understand adaptive designs.\n\n\nCode\npacman::p_load(flextable, dplyr, future.apply, progressr)\n\n\n\n\nFor simplicity, we use 1:1 treatment allocation ratio using block randomisation of size 4. Uniform random numbers are used to to order treatment allocations thereby producing a sequential list of treatment allocations for consecutively recruited participants.\n\n\nCode\n#' Randomize participants to treatment arms\n#'\n#' @param n sample size\n#' @param blocks randomisation blocks\n#'\n#' @returns a dataframe\n#' @export\n#'\n#' @examples\nsimRandomisation &lt;- function(n=584, blocks = 4){\n  # blocking sequnce\n  block &lt;- rep(seq(1:n), each = blocks, length.out = n)\n  # treatment sequence\n  trt &lt;- rep(0:1, length.out = n)\n  # a random number on a unit interval\n  set.seed(as.numeric(Sys.Date()))\n  random &lt;- runif(n)\n  df &lt;- data.frame(block, trt, random)\n  df &lt;- df[order(df$block, df$random),]\n  df$obs &lt;- 1:n\n  df &lt;- df[, c(\"obs\", \"trt\")]\n  return(df)\n}\n\n\nIf we wanted to adjust allocation ratios:\n\n\nCode\n#' Randomize participants to treatment arms\n#'\n#' @param n sample size\n#' @param blocks randomisation blocks\n#' @param ratio randomization ratio\n#' \n#' @returns a dataframe\n#' @export\n#'\n#' @examples\nsimRandomisation &lt;- function(n = 584, blocks = 4, ratio = c(1, 1)) {\n\n  # 1. Define the possible treatment groups (0, 1, 2...) based on ratio length\n  trts &lt;- 0:(length(ratio) - 1)\n  \n  # 2. Create the sequence by sampling within each block\n  # We use 'replicate' to generate each block independently\n  num_blocks &lt;- ceiling(n / blocks)\n  \n  result &lt;- replicate(num_blocks, {\n    sample(trts, size = blocks, replace = TRUE, prob = ratio)\n  })\n  \n  # 3. Flatten the matrix into a vector and trim to length n\n  trt_vector &lt;- as.vector(result)[1:n]\n  \n  # 4. Return as a clean data frame\n  return(data.frame(obs = 1:n, trt = trt_vector))\n}\n\n\n\n\n\nWe assume participant accrual is constant over time and would take 928 days.\n\n\nCode\n#' Simulate participant accrual\n#'\n#' @param n sample size\n#' @param recruit_period accrual period in days\n#'\n#' @returns a vector\n#' @export\n#'\n#' @examples\nsimAccrual &lt;- function(n = 584, recruit_period = 928){\n  # simulate trial times that patient enters the trial\n  # adding 0.5 to ensure recruitment times &gt; day 1 when rounded\n  accrual_time &lt;- round(runif(n) * recruit_period + 0.5)\n  accrual_time &lt;- sort(accrual_time)\n  return(accrual_time)\n}\n\n\nIf patient accrual was not constant over time, we’d consider a beta distribution\n\n\nCode\n#' Simulate participant accrual\n#'\n#' @param n sample size\n#' @param recruit_period accrual period (days)\n#' @param alpha alpha shape parameter\n#' @param beta beta shape parameter\n#'\n#' @returnsa vector\n#' @export\n#'\n#' @examples\nsimAccrual &lt;- function(n = 584, recruit_period = 928, alpha = 1, beta = 1) {\n  \n  # 1. Generate random numbers using a Beta distribution\n  # rbeta(n, shape1, beta) returns values between 0 and 1\n  # alpha= beta = 1 is equivalent to runif (constant accrual)\n  # alpha= 2, beta = 1 creates a \"late peak\" (accelerating accrual)\n  # alpha= 1, beta = 2 creates an \"early peak\" (decelerating accrual)\n  # alpha= 2, beta = 2 creates a \"bell curve\" (peak in middle)\n  \n  accrual_proportions &lt;- rbeta(n, alpha, beta)\n  \n  # 2. Scale the 0-1 values to the actual recruit_period\n  accrual_time &lt;- round(accrual_proportions * recruit_period + 0.5)\n  \n  # 3. Sort to represent chronological entry\n  accrual_time &lt;- sort(accrual_time)\n  \n  return(accrual_time)\n}\n\n\n\n\n\nChoice of probability distribution depends on the outcome variable. You could use the binomial distribution for a binary outcome, normal distribution for a continuous outcome, or Weibull/exponential for time-to-event outcome.\n\n\nCode\n#' Simulate trial data with outcome variable\n#'\n#' @param n sample size\n#' @param recruit_period recruitment period (days)\n#' @param p vactor of event probabilities\n#'\n#' @returns a dataframe\n#' @export\n#'\n#' @examples\nsimTrialData &lt;- function(n = 584, recruit_period = 928, p){\n  # simulate random allocations\n  data &lt;- simRandomisation()\n  # simulate accrual\n  data$accrual &lt;- simAccrual()\n  # simulate events from binomial dist\n  data$event &lt;- rbinom(n, 1, p[data$trt+1])\n  \n  return(data)\n}\n\n\n\n\n\nWe assume outcome to be available immediately after recruitment and the time of interim analysis to be when a pre-determined number of events to be 20. As the data is ordered by accrual time, we can calculate cumulative number of events cum_events with cum_events=20 indicating the planned time of the interim analysis. Since we assume no time lag between recruitment and outcome assessment, the outcome at the interim (event_iterim: 0,1) would be the same as the outcome at final time point (event) for participants included in the interim analysis (i.e., for observations where obs &lt;= interim_ind)\n\n\nCode\n#' Identify interim data  \n#'\n#' @param data a dataframe\n#' @param events_at_interim number of events at interim\n#'\n#' @returns a dataframe\n#' @export\n#'\n#' @examples\nsimInterimData &lt;- function(data, events_at_interim = 20){\n  # cumulative events\n  data$cum_events &lt;- cumsum(data$event)\n  # when does interim occur\n  data$interim_ind &lt;- data$obs[min(which(data$cum_events == events_at_interim))]\n  # events at interim\n  data$event_interim &lt;- with(data, ifelse(obs &lt;= interim_ind, event, NA))\n  \n  return(data)\n}\n\n\n\n\n\nDuring interim analysis we seek to find out:\n\nif trial would have stopped before maximum recruitment\npoint estimates and their CIs for treatment effect\nsample size at the point of stopping the trial\nif the study would have found evidence of clinically relevant treatment effect or if it was a futile trial\n\n\n\nCode\n#' Analyze trial data\n#'\n#' @param data trial dataframe\n#' @param alpha_interim alpha spend at interim analysis\n#' @param alpha_final alpha spend at final analysis\n#'\n#' @returns a dataframe\n#' @export\n#'\n#' @examples\nanalyseData &lt;- function(data, alpha_interim, alpha_final){\n  nevents0 &lt;- sum(data$event[data$trt == 0])\n  nevents1 &lt;- sum(data$event[data$trt == 1])\n  pevents0 &lt;- nevents0/sum(data$trt ==0)\n  pevents1 &lt;- nevents1/sum(data$trt ==1)\n  \n  # logistic reg at interim\n  data$event_interim &lt;- factor(data$event_interim)\n  logit_interim &lt;- glm(event_interim ~ trt, data = data, family = \"binomial\")\n  conf_int &lt;- confint(logit_interim)\n  \n  # results at interim\n  \n  interim_or &lt;- exp(coef(logit_interim)[\"trt\"])\n  interim_lci &lt;- exp(conf_int[\"trt\", \"2.5 %\"])\n  interim_uci &lt;- exp(conf_int[\"trt\", \"97.5 %\"])\n  interim_p &lt;- coef(summary(logit_interim))[\"trt\", \"Pr(&gt;|z|)\"]\n  \n  # stop for trial success at interim\n  interim_stop &lt;- ifelse(interim_p &lt; alpha_interim, 1, 0)\n  \n  # the proportion of events if the trial stopped at interim\n  \n  if(interim_stop == 1){\n    nevents0 &lt;- sum(data$event[data$trt == 0 & !is.na(data$event_interim)])\n    nevents1 &lt;- sum(data$event[data$trt == 1 & !is.na(data$event_interim)])\n    pevents0 &lt;- nevents0/sum(data$trt == 0 & !is.na(data$event_interim))\n    pevents1 &lt;- nevents1/sum(data$trt == 1 & !is.na(data$event_interim))\n  }\n  # final analysis\n  data$event &lt;- factor(data$event)\n  logit &lt;- glm(event ~ trt, data = data, family = \"binomial\")\n  conf &lt;- confint(logit)\n  \n  # results at final analysis\n  final_or &lt;- exp(coef(logit)[\"trt\"])\n  final_lci &lt;- exp(conf[\"trt\", \"2.5 %\"])\n  final_uci &lt;- exp(conf[\"trt\", \"97.5 %\"])\n  final_p &lt;- coef(summary(logit))[\"trt\", \"Pr(&gt;|z|)\"]\n  \n  final_stop &lt;- ifelse(final_p &lt; alpha_final, 1, 0)\n  \n  # is the trial conclusive?\n  stop &lt;- ifelse(interim_stop == 1, interim_stop, final_stop)\n  \n  # sample size (used to determine average sample size)\n  \n  if(interim_stop == 1){\n    sample_size &lt;- unique(data$interim_ind)\n  } else{\n    sample_size &lt;- nrow(data)\n  }\n  \n  # determine trial flip-flop probability\n  flipflop &lt;- ifelse(interim_stop == 1 & final_stop == 0, 1, 0)\n  \n  results &lt;- data.frame(\n    nevents0, nevents1, pevents0, pevents1, sample_size,\n    interim_time = unique(data$interim_ind),\n    interim_or, interim_lci, interim_uci, interim_p, interim_stop,\n    final_or, final_lci, final_uci, final_p, final_stop, stop, flipflop\n  )\n  return(results)\n}\n\n\nAvoid in model.matrix.default(mt, mf, contrasts) : variable 1 has no levels errors\n\n\nCode\n#' Analyze trial data\n#'\n#' @param data trial dataframe\n#' @param alpha_interim alpha spend at interim analysis\n#' @param alpha_final alpha spend at final analysis\n#'\n#' @returns a dataframe\n#' @export\n#'\n#' @examples\n#' \n\n\nanalyseData &lt;- function(data, alpha_interim, alpha_final){\n  \n  # Helper to check if GLM can run (needs variation in both Y and X)\n  is_estimable &lt;- function(df, outcome_col) {\n    if (nrow(df) == 0) return(FALSE)\n    has_outcome_var &lt;- length(unique(df[[outcome_col]])) &gt; 1\n    has_trt_var     &lt;- length(unique(df$trt)) &gt; 1\n    return(has_outcome_var && has_trt_var)\n  }\n\n  # --- Initial Calculations (Total Trial potential) ---\n  nevents0 &lt;- sum(data$event[data$trt == 0], na.rm = TRUE)\n  nevents1 &lt;- sum(data$event[data$trt == 1], na.rm = TRUE)\n  pevents0 &lt;- nevents0 / sum(data$trt == 0)\n  pevents1 &lt;- nevents1 / sum(data$trt == 1)\n  \n  # --- Interim Analysis ---\n  # Filter data to only those available at interim for the model\n  interim_data &lt;- data[!is.na(data$event_interim), ]\n  interim_estimable &lt;- is_estimable(interim_data, \"event_interim\")\n  \n  if(interim_estimable){\n    logit_interim &lt;- glm(factor(event_interim) ~ trt, data = interim_data, family = \"binomial\")\n    interim_p     &lt;- coef(summary(logit_interim))[\"trt\", \"Pr(&gt;|z|)\"]\n    conf_int      &lt;- confint.default(logit_interim) \n    interim_or    &lt;- exp(coef(logit_interim)[\"trt\"])\n    interim_lci   &lt;- exp(conf_int[\"trt\", 1])\n    interim_uci   &lt;- exp(conf_int[\"trt\", 2])\n  } else {\n    interim_p &lt;- interim_or &lt;- interim_lci &lt;- interim_uci &lt;- NA\n  }\n  \n  # Stop for trial success at interim\n  interim_stop &lt;- ifelse(!is.na(interim_p) && interim_p &lt; alpha_interim, 1, 0)\n  \n  # Update event stats if the trial stopped at interim\n  if(interim_stop == 1){\n    nevents0 &lt;- sum(interim_data$event[interim_data$trt == 0])\n    nevents1 &lt;- sum(interim_data$event[interim_data$trt == 1])\n    pevents0 &lt;- nevents0 / sum(interim_data$trt == 0)\n    pevents1 &lt;- nevents1 / sum(interim_data$trt == 1)\n    sample_size &lt;- nrow(interim_data)\n  } else {\n    sample_size &lt;- nrow(data)\n  }\n  \n  # --- Final Analysis ---\n  final_estimable &lt;- is_estimable(data, \"event\")\n  \n  if(final_estimable){\n    logit    &lt;- glm(factor(event) ~ trt, data = data, family = \"binomial\")\n    final_p  &lt;- coef(summary(logit))[\"trt\", \"Pr(&gt;|z|)\"]\n    conf     &lt;- confint.default(logit)\n    final_or  &lt;- exp(coef(logit)[\"trt\"])\n    final_lci &lt;- exp(conf[\"trt\", 1])\n    final_uci &lt;- exp(conf[\"trt\", 2])\n  } else {\n    final_p &lt;- final_or &lt;- final_lci &lt;- final_uci &lt;- NA\n  }\n  \n  final_stop &lt;- ifelse(!is.na(final_p) && final_p &lt; alpha_final, 1, 0)\n  \n  # --- Logic Summary ---\n  stop     &lt;- ifelse(interim_stop == 1, 1, final_stop)\n  flipflop &lt;- ifelse(interim_stop == 1 & final_stop == 0, 1, 0)\n  \n  # Build the final data frame\n  results &lt;- data.frame(\n    nevents0, \n    nevents1, \n    pevents0, \n    pevents1, \n    sample_size,\n    interim_time = max(data$interim_ind, na.rm = TRUE), # Assuming this is a time-point or ID\n    interim_or, \n    interim_lci, \n    interim_uci, \n    interim_p, \n    interim_stop,\n    final_or, \n    final_lci, \n    final_uci, \n    final_p, \n    final_stop, \n    stop, \n    flipflop\n  )\n  \n  return(results)\n}\n\n\n\n\n\n\n\nCode\n#' Run a single experiment\n#'\n#' @param n sample size\n#' @param recruit_period accrual period\n#' @param p vector of treatment success probability\n#' @param events_at_interim number of events at interim\n#' @param alpha_interim interim alpha spend\n#' @param alpha_final final alpha spend\n#'\n#' @returns a dataframe\n#' @export\n#'\n#' @examples\nrunTrial &lt;- function(n, recruit_period, p, events_at_interim, alpha_interim, alpha_final){\n  #1. simulate trial data\n  df &lt;- simTrialData(n, recruit_period, p)\n  #2. create interim data\n  df &lt;- simInterimData(df, events_at_interim)\n  #3. Analyze data\n  results &lt;- analyseData(df, alpha_interim, alpha_final)\n  return(list(data = df, results = results))\n}\n\n\n\n\nCode\n# inputs\ninputs &lt;- data.frame(\n  seed = as.numeric(Sys.Date()),\n  # recruitment period = Days in 2.5 years   \n  #584/690 is the target/total possible = efficiency ratio\n  recruit_period = 365.25 * 3 * 584/ 690,\n  n = 584,\n  events_at_interim = 20,\n  p0 = 0.10,\n  p1 = 0.04,\n  # p = c(p0, p1)\n  alpha_final = 0.045,\n  alpha_interim = 0.005\n  \n)\np &lt;- c(inputs$p0, inputs$p1)\n\ndict &lt;- data.frame(\n  variable = c(\n    'nevents0','nevents1', 'pevents0', 'pevents1', 'sample_size', 'interim_time',\n    \"interim_or\", \"interim_lci\", \"interim_uci\", \"interim_p\", \"interim_stop\",\n    \"final_or\", \"final_lci\", \"final_uci\", \"final_p\", \"final_stop\", \"stop\",\n    \"flipflop\"\n  ),\n  label = c(\n    \"Number of events in control group\",\n    \"Number of events in experimental group\",\n    \"Proportion of events in control group\",\n    \"Proportion of events in experimental group\",\n    \"Sample size\",\n    \"Time (days) at interim\",\n    \"Odds ratio at interim\",\n    \"Lower CI for OR at interim\",\n    \"Upper CI for OR at interim\",\n    \"P-value of any difference in treatments at interim\",\n    \"Whether the trial would have stopped at interim\",\n    \"Odds ratio at final analysis\",\n    \"Lower CI for odds ratio\",\n    \"Upper CI for odds ratio\",\n    \"P-value for treatment difference at final analysis\",\n    \"Whether the trial is conclusive at final analysis\",\n    \"Whether the trial was conclusive (at final or interim)\",\n    \"The probability of trial flip-flopping\"\n    \n    \n  )\n)\n\n\n\n\nCode\nset.seed(inputs$seed)\n\nresults_singleTrial &lt;- runTrial(\n  inputs$n, inputs$recruit_period, p, inputs$events_at_interim,\n  inputs$alpha_interim, inputs$alpha_final\n)\nresults &lt;- results_singleTrial$results|&gt; round(3)  |&gt; t() |&gt; as.data.frame() \nnames(results)[1] &lt;- \"value\"\nresults$variable &lt;- rownames(results)\nrownames(results) &lt;- NULL\nresults &lt;- left_join(results, dict)\n\n\nJoining with `by = join_by(variable)`\n\n\nCode\nresults &lt;- results[, c(\"variable\", \"label\", \"value\")]\n\n\n\n\nCode\ntab_singleT &lt;- flextable(results) |&gt; theme_vanilla() \ntab_singleT\n\n\n\n\nTable 1: Results from a single simulated trial\n\n\n\nvariablelabelvaluenevents0Number of events in control group16.000nevents1Number of events in experimental group4.000pevents0Proportion of events in control group0.140pevents1Proportion of events in experimental group0.030sample_sizeSample size249.000interim_timeTime (days) at interim249.000interim_orOdds ratio at interim0.187interim_lciLower CI for OR at interim0.061interim_uciUpper CI for OR at interim0.577interim_pP-value of any difference in treatments at interim0.004interim_stopWhether the trial would have stopped at interim1.000final_orOdds ratio at final analysis0.305final_lciLower CI for odds ratio0.141final_uciUpper CI for odds ratio0.661final_pP-value for treatment difference at final analysis0.003final_stopWhether the trial is conclusive at final analysis1.000stopWhether the trial was conclusive (at final or interim)1.000flipflopThe probability of trial flip-flopping0.000\n\n\n\n\n\n\n\n\n\n\nCode\nrunMultipleTrials &lt;- function(nsims, seed, n, recruit_period, p, \n                              events_at_interim, alpha_interim, alpha_final){\n  # random seed\n  seeds &lt;- seed + seq(1: nsims)\n  # simulate\n  multiple_trials &lt;- lapply(1:nsims, function(x){\n    set.seed(seeds[x])\n    y &lt;- runTrial(n, recruit_period, p, events_at_interim, alpha_interim,\n                  alpha_final)\n    return(y)\n  })\n  \n  # summarise\n  results &lt;- lapply(multiple_trials, function(x) return(x$results))\n  results_all &lt;- do.call(rbind, results)\n  # saveRDS(multiple_trials, file = \"results_multTrials.rds\")\n  \n  # remove any trials with non-estimable CIs and calculate summary\n  x &lt;- which(apply(results_all, 1, function(x) any(is.na(x))))\n  if(length(x) &gt; 0){\n    result_summary &lt;- apply(results_all[-x,], 2, summary)\n  } else{\n    result_summary &lt;- apply(results_all, 2, summary)\n  }\n  \n  return(\n    list(\n      results_all = results_all,\n      results_summary = results_summary,\n      seeds = seeds\n    )\n  )\n}\n\n\nParallelize computations using future.apply}\n\n\nCode\nlibrary(future.apply)\nlibrary(progressr)\n\nrunMultipleTrials_Parallel &lt;- function(nsims, seed, n, recruit_period, p_param, \n                                       events_at_interim, alpha_interim, alpha_final) {\n  \n  # 1. Setup Parallel Backend\n  # Using 'multisession' is the most stable across Windows/Mac/Linux\n  plan(multisession, workers = parallelly::availableCores() - 1)\n  \n  # Ensure cleanup of workers when function finishes or crashes\n  on.exit(plan(sequential))\n  \n  seeds &lt;- seed + seq_len(nsims)\n  \n  # 2. Access the progressor from the calling environment\n  p &lt;- progressor(steps = nsims)\n  \n  # 3. Run Simulations\n  multiple_trials &lt;- future_lapply(1:nsims, function(i) {\n    # Increment progress bar\n    p(message = sprintf(\"Simulating trial %g\", i))\n    \n    set.seed(seeds[i])\n    \n    # Run the individual trial\n    # Note: I renamed your 'p' to 'p_param' to avoid conflict with progressor 'p'\n    y &lt;- runTrial(n, recruit_period, p_param, events_at_interim, alpha_interim, alpha_final)\n    \n    return(y)\n  }, future.seed = TRUE) \n  \n  # 4. Summarize Results\n  results &lt;- lapply(multiple_trials, function(x) x$results)\n  results_all &lt;- do.call(rbind, results)\n  \n  # Clean up non-estimable rows (NAs from glm failures)\n  results_clean &lt;- results_all[complete.cases(results_all), ]\n  result_summary &lt;- apply(results_clean, 2, summary)\n  \n  return(list(\n    results_all = results_all,\n    results_summary = result_summary,\n    seeds = seeds\n  ))\n}\n\n\nPrint results\n\n\nCode\ntab_mulT &lt;- results_mulT$results_summary |&gt; round(3) |&gt; t() |&gt; as.data.frame()\ntab_mulT$variable &lt;- rownames(tab_mulT)\nrownames(tab_mulT) &lt;- NULL\nnames(tab_mulT) &lt;- c(\"Minimum\", \"Q1\", \"Median\", \"Mean\", \"Q3\", \"Maximum\", \"variable\")\ntab_mulT &lt;- left_join(tab_mulT, dict) \n\n\nJoining with `by = join_by(variable)`\n\n\nCode\ntab_mulT &lt;- tab_mulT |&gt; select(variable, label, everything())\nnames(tab_mulT) &lt;- stringr::str_to_sentence(names(tab_mulT))\n\n\n\n\nCode\nt_mulT &lt;- flextable(tab_mulT) |&gt; theme_vanilla() \nt_mulT\n\n\n\n\nTable 2: Results from a multiple simulated trial\n\n\n\nVariableLabelMinimumQ1MedianMeanQ3Maximumnevents0Number of events in control group12.00024.00028.00027.58232.00047.000nevents1Number of events in experimental group1.0009.00011.00011.03714.00025.000pevents0Proportion of events in control group0.0430.0880.0990.1010.1120.254pevents1Proportion of events in experimental group0.0050.0300.0390.0390.0470.082sample_sizeSample size138.000584.000584.000554.664584.000584.000interim_timeTime (days) at interim116.000243.000281.000286.926327.000565.000interim_orOdds ratio at interim0.0000.2520.3680.4080.5181.626interim_lciLower CI for OR at interim0.0000.0830.1350.1520.2010.650interim_uciUpper CI for OR at interim0.3020.7601.009Inf1.340Infinterim_pP-value of any difference in treatments at interim0.0000.0140.0520.1410.1751.000interim_stopWhether the trial would have stopped at interim0.0000.0000.0000.0980.0001.000final_orOdds ratio at final analysis0.0590.2860.3710.3910.4731.304final_lciLower CI for odds ratio0.0140.1350.1840.1940.2400.651final_uciUpper CI for odds ratio0.2410.6060.7500.7930.9372.653final_pP-value for treatment difference at final analysis0.0000.0010.0060.0420.0320.994final_stopWhether the trial is conclusive at final analysis0.0001.0001.0000.8041.0001.000stopWhether the trial was conclusive (at final or interim)0.0001.0001.0000.8051.0001.000flipflopThe probability of trial flip-flopping0.0000.0000.0000.0010.0001.000"
  },
  {
    "objectID": "posts/2026-01-12-Adaptive-designs/index.html#randomisation",
    "href": "posts/2026-01-12-Adaptive-designs/index.html#randomisation",
    "title": "Adaptive Methods for Clinical Trials",
    "section": "",
    "text": "For simplicity, we use 1:1 treatment allocation ratio using block randomisation of size 4. Uniform random numbers are used to to order treatment allocations thereby producing a sequential list of treatment allocations for consecutively recruited participants.\n\n\nCode\n#' Randomize participants to treatment arms\n#'\n#' @param n sample size\n#' @param blocks randomisation blocks\n#'\n#' @returns a dataframe\n#' @export\n#'\n#' @examples\nsimRandomisation &lt;- function(n=584, blocks = 4){\n  # blocking sequnce\n  block &lt;- rep(seq(1:n), each = blocks, length.out = n)\n  # treatment sequence\n  trt &lt;- rep(0:1, length.out = n)\n  # a random number on a unit interval\n  set.seed(as.numeric(Sys.Date()))\n  random &lt;- runif(n)\n  df &lt;- data.frame(block, trt, random)\n  df &lt;- df[order(df$block, df$random),]\n  df$obs &lt;- 1:n\n  df &lt;- df[, c(\"obs\", \"trt\")]\n  return(df)\n}\n\n\nIf we wanted to adjust allocation ratios:\n\n\nCode\n#' Randomize participants to treatment arms\n#'\n#' @param n sample size\n#' @param blocks randomisation blocks\n#' @param ratio randomization ratio\n#' \n#' @returns a dataframe\n#' @export\n#'\n#' @examples\nsimRandomisation &lt;- function(n = 584, blocks = 4, ratio = c(1, 1)) {\n\n  # 1. Define the possible treatment groups (0, 1, 2...) based on ratio length\n  trts &lt;- 0:(length(ratio) - 1)\n  \n  # 2. Create the sequence by sampling within each block\n  # We use 'replicate' to generate each block independently\n  num_blocks &lt;- ceiling(n / blocks)\n  \n  result &lt;- replicate(num_blocks, {\n    sample(trts, size = blocks, replace = TRUE, prob = ratio)\n  })\n  \n  # 3. Flatten the matrix into a vector and trim to length n\n  trt_vector &lt;- as.vector(result)[1:n]\n  \n  # 4. Return as a clean data frame\n  return(data.frame(obs = 1:n, trt = trt_vector))\n}"
  },
  {
    "objectID": "posts/2026-01-12-Adaptive-designs/index.html#participant-accrual",
    "href": "posts/2026-01-12-Adaptive-designs/index.html#participant-accrual",
    "title": "Adaptive Methods for Clinical Trials",
    "section": "",
    "text": "We assume participant accrual is constant over time and would take 928 days.\n\n\nCode\n#' Simulate participant accrual\n#'\n#' @param n sample size\n#' @param recruit_period accrual period in days\n#'\n#' @returns a vector\n#' @export\n#'\n#' @examples\nsimAccrual &lt;- function(n = 584, recruit_period = 928){\n  # simulate trial times that patient enters the trial\n  # adding 0.5 to ensure recruitment times &gt; day 1 when rounded\n  accrual_time &lt;- round(runif(n) * recruit_period + 0.5)\n  accrual_time &lt;- sort(accrual_time)\n  return(accrual_time)\n}\n\n\nIf patient accrual was not constant over time, we’d consider a beta distribution\n\n\nCode\n#' Simulate participant accrual\n#'\n#' @param n sample size\n#' @param recruit_period accrual period (days)\n#' @param alpha alpha shape parameter\n#' @param beta beta shape parameter\n#'\n#' @returnsa vector\n#' @export\n#'\n#' @examples\nsimAccrual &lt;- function(n = 584, recruit_period = 928, alpha = 1, beta = 1) {\n  \n  # 1. Generate random numbers using a Beta distribution\n  # rbeta(n, shape1, beta) returns values between 0 and 1\n  # alpha= beta = 1 is equivalent to runif (constant accrual)\n  # alpha= 2, beta = 1 creates a \"late peak\" (accelerating accrual)\n  # alpha= 1, beta = 2 creates an \"early peak\" (decelerating accrual)\n  # alpha= 2, beta = 2 creates a \"bell curve\" (peak in middle)\n  \n  accrual_proportions &lt;- rbeta(n, alpha, beta)\n  \n  # 2. Scale the 0-1 values to the actual recruit_period\n  accrual_time &lt;- round(accrual_proportions * recruit_period + 0.5)\n  \n  # 3. Sort to represent chronological entry\n  accrual_time &lt;- sort(accrual_time)\n  \n  return(accrual_time)\n}"
  },
  {
    "objectID": "posts/2026-01-12-Adaptive-designs/index.html#generate-participant-outcomes",
    "href": "posts/2026-01-12-Adaptive-designs/index.html#generate-participant-outcomes",
    "title": "Adaptive Methods for Clinical Trials",
    "section": "",
    "text": "Choice of probability distribution depends on the outcome variable. You could use the binomial distribution for a binary outcome, normal distribution for a continuous outcome, or Weibull/exponential for time-to-event outcome.\n\n\nCode\n#' Simulate trial data with outcome variable\n#'\n#' @param n sample size\n#' @param recruit_period recruitment period (days)\n#' @param p vactor of event probabilities\n#'\n#' @returns a dataframe\n#' @export\n#'\n#' @examples\nsimTrialData &lt;- function(n = 584, recruit_period = 928, p){\n  # simulate random allocations\n  data &lt;- simRandomisation()\n  # simulate accrual\n  data$accrual &lt;- simAccrual()\n  # simulate events from binomial dist\n  data$event &lt;- rbinom(n, 1, p[data$trt+1])\n  \n  return(data)\n}"
  },
  {
    "objectID": "posts/2026-01-12-Adaptive-designs/index.html#identify-data-available-at-interim-analysis",
    "href": "posts/2026-01-12-Adaptive-designs/index.html#identify-data-available-at-interim-analysis",
    "title": "Adaptive Methods for Clinical Trials",
    "section": "",
    "text": "We assume outcome to be available immediately after recruitment and the time of interim analysis to be when a pre-determined number of events to be 20. As the data is ordered by accrual time, we can calculate cumulative number of events cum_events with cum_events=20 indicating the planned time of the interim analysis. Since we assume no time lag between recruitment and outcome assessment, the outcome at the interim (event_iterim: 0,1) would be the same as the outcome at final time point (event) for participants included in the interim analysis (i.e., for observations where obs &lt;= interim_ind)\n\n\nCode\n#' Identify interim data  \n#'\n#' @param data a dataframe\n#' @param events_at_interim number of events at interim\n#'\n#' @returns a dataframe\n#' @export\n#'\n#' @examples\nsimInterimData &lt;- function(data, events_at_interim = 20){\n  # cumulative events\n  data$cum_events &lt;- cumsum(data$event)\n  # when does interim occur\n  data$interim_ind &lt;- data$obs[min(which(data$cum_events == events_at_interim))]\n  # events at interim\n  data$event_interim &lt;- with(data, ifelse(obs &lt;= interim_ind, event, NA))\n  \n  return(data)\n}"
  },
  {
    "objectID": "posts/2026-01-12-Adaptive-designs/index.html#analyze-trial-data",
    "href": "posts/2026-01-12-Adaptive-designs/index.html#analyze-trial-data",
    "title": "Adaptive Methods for Clinical Trials",
    "section": "",
    "text": "During interim analysis we seek to find out:\n\nif trial would have stopped before maximum recruitment\npoint estimates and their CIs for treatment effect\nsample size at the point of stopping the trial\nif the study would have found evidence of clinically relevant treatment effect or if it was a futile trial\n\n\n\nCode\n#' Analyze trial data\n#'\n#' @param data trial dataframe\n#' @param alpha_interim alpha spend at interim analysis\n#' @param alpha_final alpha spend at final analysis\n#'\n#' @returns a dataframe\n#' @export\n#'\n#' @examples\nanalyseData &lt;- function(data, alpha_interim, alpha_final){\n  nevents0 &lt;- sum(data$event[data$trt == 0])\n  nevents1 &lt;- sum(data$event[data$trt == 1])\n  pevents0 &lt;- nevents0/sum(data$trt ==0)\n  pevents1 &lt;- nevents1/sum(data$trt ==1)\n  \n  # logistic reg at interim\n  data$event_interim &lt;- factor(data$event_interim)\n  logit_interim &lt;- glm(event_interim ~ trt, data = data, family = \"binomial\")\n  conf_int &lt;- confint(logit_interim)\n  \n  # results at interim\n  \n  interim_or &lt;- exp(coef(logit_interim)[\"trt\"])\n  interim_lci &lt;- exp(conf_int[\"trt\", \"2.5 %\"])\n  interim_uci &lt;- exp(conf_int[\"trt\", \"97.5 %\"])\n  interim_p &lt;- coef(summary(logit_interim))[\"trt\", \"Pr(&gt;|z|)\"]\n  \n  # stop for trial success at interim\n  interim_stop &lt;- ifelse(interim_p &lt; alpha_interim, 1, 0)\n  \n  # the proportion of events if the trial stopped at interim\n  \n  if(interim_stop == 1){\n    nevents0 &lt;- sum(data$event[data$trt == 0 & !is.na(data$event_interim)])\n    nevents1 &lt;- sum(data$event[data$trt == 1 & !is.na(data$event_interim)])\n    pevents0 &lt;- nevents0/sum(data$trt == 0 & !is.na(data$event_interim))\n    pevents1 &lt;- nevents1/sum(data$trt == 1 & !is.na(data$event_interim))\n  }\n  # final analysis\n  data$event &lt;- factor(data$event)\n  logit &lt;- glm(event ~ trt, data = data, family = \"binomial\")\n  conf &lt;- confint(logit)\n  \n  # results at final analysis\n  final_or &lt;- exp(coef(logit)[\"trt\"])\n  final_lci &lt;- exp(conf[\"trt\", \"2.5 %\"])\n  final_uci &lt;- exp(conf[\"trt\", \"97.5 %\"])\n  final_p &lt;- coef(summary(logit))[\"trt\", \"Pr(&gt;|z|)\"]\n  \n  final_stop &lt;- ifelse(final_p &lt; alpha_final, 1, 0)\n  \n  # is the trial conclusive?\n  stop &lt;- ifelse(interim_stop == 1, interim_stop, final_stop)\n  \n  # sample size (used to determine average sample size)\n  \n  if(interim_stop == 1){\n    sample_size &lt;- unique(data$interim_ind)\n  } else{\n    sample_size &lt;- nrow(data)\n  }\n  \n  # determine trial flip-flop probability\n  flipflop &lt;- ifelse(interim_stop == 1 & final_stop == 0, 1, 0)\n  \n  results &lt;- data.frame(\n    nevents0, nevents1, pevents0, pevents1, sample_size,\n    interim_time = unique(data$interim_ind),\n    interim_or, interim_lci, interim_uci, interim_p, interim_stop,\n    final_or, final_lci, final_uci, final_p, final_stop, stop, flipflop\n  )\n  return(results)\n}\n\n\nAvoid in model.matrix.default(mt, mf, contrasts) : variable 1 has no levels errors\n\n\nCode\n#' Analyze trial data\n#'\n#' @param data trial dataframe\n#' @param alpha_interim alpha spend at interim analysis\n#' @param alpha_final alpha spend at final analysis\n#'\n#' @returns a dataframe\n#' @export\n#'\n#' @examples\n#' \n\n\nanalyseData &lt;- function(data, alpha_interim, alpha_final){\n  \n  # Helper to check if GLM can run (needs variation in both Y and X)\n  is_estimable &lt;- function(df, outcome_col) {\n    if (nrow(df) == 0) return(FALSE)\n    has_outcome_var &lt;- length(unique(df[[outcome_col]])) &gt; 1\n    has_trt_var     &lt;- length(unique(df$trt)) &gt; 1\n    return(has_outcome_var && has_trt_var)\n  }\n\n  # --- Initial Calculations (Total Trial potential) ---\n  nevents0 &lt;- sum(data$event[data$trt == 0], na.rm = TRUE)\n  nevents1 &lt;- sum(data$event[data$trt == 1], na.rm = TRUE)\n  pevents0 &lt;- nevents0 / sum(data$trt == 0)\n  pevents1 &lt;- nevents1 / sum(data$trt == 1)\n  \n  # --- Interim Analysis ---\n  # Filter data to only those available at interim for the model\n  interim_data &lt;- data[!is.na(data$event_interim), ]\n  interim_estimable &lt;- is_estimable(interim_data, \"event_interim\")\n  \n  if(interim_estimable){\n    logit_interim &lt;- glm(factor(event_interim) ~ trt, data = interim_data, family = \"binomial\")\n    interim_p     &lt;- coef(summary(logit_interim))[\"trt\", \"Pr(&gt;|z|)\"]\n    conf_int      &lt;- confint.default(logit_interim) \n    interim_or    &lt;- exp(coef(logit_interim)[\"trt\"])\n    interim_lci   &lt;- exp(conf_int[\"trt\", 1])\n    interim_uci   &lt;- exp(conf_int[\"trt\", 2])\n  } else {\n    interim_p &lt;- interim_or &lt;- interim_lci &lt;- interim_uci &lt;- NA\n  }\n  \n  # Stop for trial success at interim\n  interim_stop &lt;- ifelse(!is.na(interim_p) && interim_p &lt; alpha_interim, 1, 0)\n  \n  # Update event stats if the trial stopped at interim\n  if(interim_stop == 1){\n    nevents0 &lt;- sum(interim_data$event[interim_data$trt == 0])\n    nevents1 &lt;- sum(interim_data$event[interim_data$trt == 1])\n    pevents0 &lt;- nevents0 / sum(interim_data$trt == 0)\n    pevents1 &lt;- nevents1 / sum(interim_data$trt == 1)\n    sample_size &lt;- nrow(interim_data)\n  } else {\n    sample_size &lt;- nrow(data)\n  }\n  \n  # --- Final Analysis ---\n  final_estimable &lt;- is_estimable(data, \"event\")\n  \n  if(final_estimable){\n    logit    &lt;- glm(factor(event) ~ trt, data = data, family = \"binomial\")\n    final_p  &lt;- coef(summary(logit))[\"trt\", \"Pr(&gt;|z|)\"]\n    conf     &lt;- confint.default(logit)\n    final_or  &lt;- exp(coef(logit)[\"trt\"])\n    final_lci &lt;- exp(conf[\"trt\", 1])\n    final_uci &lt;- exp(conf[\"trt\", 2])\n  } else {\n    final_p &lt;- final_or &lt;- final_lci &lt;- final_uci &lt;- NA\n  }\n  \n  final_stop &lt;- ifelse(!is.na(final_p) && final_p &lt; alpha_final, 1, 0)\n  \n  # --- Logic Summary ---\n  stop     &lt;- ifelse(interim_stop == 1, 1, final_stop)\n  flipflop &lt;- ifelse(interim_stop == 1 & final_stop == 0, 1, 0)\n  \n  # Build the final data frame\n  results &lt;- data.frame(\n    nevents0, \n    nevents1, \n    pevents0, \n    pevents1, \n    sample_size,\n    interim_time = max(data$interim_ind, na.rm = TRUE), # Assuming this is a time-point or ID\n    interim_or, \n    interim_lci, \n    interim_uci, \n    interim_p, \n    interim_stop,\n    final_or, \n    final_lci, \n    final_uci, \n    final_p, \n    final_stop, \n    stop, \n    flipflop\n  )\n  \n  return(results)\n}"
  },
  {
    "objectID": "posts/2026-01-12-Adaptive-designs/index.html#simulate-a-single-trial",
    "href": "posts/2026-01-12-Adaptive-designs/index.html#simulate-a-single-trial",
    "title": "Adaptive Methods for Clinical Trials",
    "section": "",
    "text": "Code\n#' Run a single experiment\n#'\n#' @param n sample size\n#' @param recruit_period accrual period\n#' @param p vector of treatment success probability\n#' @param events_at_interim number of events at interim\n#' @param alpha_interim interim alpha spend\n#' @param alpha_final final alpha spend\n#'\n#' @returns a dataframe\n#' @export\n#'\n#' @examples\nrunTrial &lt;- function(n, recruit_period, p, events_at_interim, alpha_interim, alpha_final){\n  #1. simulate trial data\n  df &lt;- simTrialData(n, recruit_period, p)\n  #2. create interim data\n  df &lt;- simInterimData(df, events_at_interim)\n  #3. Analyze data\n  results &lt;- analyseData(df, alpha_interim, alpha_final)\n  return(list(data = df, results = results))\n}\n\n\n\n\nCode\n# inputs\ninputs &lt;- data.frame(\n  seed = as.numeric(Sys.Date()),\n  # recruitment period = Days in 2.5 years   \n  #584/690 is the target/total possible = efficiency ratio\n  recruit_period = 365.25 * 3 * 584/ 690,\n  n = 584,\n  events_at_interim = 20,\n  p0 = 0.10,\n  p1 = 0.04,\n  # p = c(p0, p1)\n  alpha_final = 0.045,\n  alpha_interim = 0.005\n  \n)\np &lt;- c(inputs$p0, inputs$p1)\n\ndict &lt;- data.frame(\n  variable = c(\n    'nevents0','nevents1', 'pevents0', 'pevents1', 'sample_size', 'interim_time',\n    \"interim_or\", \"interim_lci\", \"interim_uci\", \"interim_p\", \"interim_stop\",\n    \"final_or\", \"final_lci\", \"final_uci\", \"final_p\", \"final_stop\", \"stop\",\n    \"flipflop\"\n  ),\n  label = c(\n    \"Number of events in control group\",\n    \"Number of events in experimental group\",\n    \"Proportion of events in control group\",\n    \"Proportion of events in experimental group\",\n    \"Sample size\",\n    \"Time (days) at interim\",\n    \"Odds ratio at interim\",\n    \"Lower CI for OR at interim\",\n    \"Upper CI for OR at interim\",\n    \"P-value of any difference in treatments at interim\",\n    \"Whether the trial would have stopped at interim\",\n    \"Odds ratio at final analysis\",\n    \"Lower CI for odds ratio\",\n    \"Upper CI for odds ratio\",\n    \"P-value for treatment difference at final analysis\",\n    \"Whether the trial is conclusive at final analysis\",\n    \"Whether the trial was conclusive (at final or interim)\",\n    \"The probability of trial flip-flopping\"\n    \n    \n  )\n)\n\n\n\n\nCode\nset.seed(inputs$seed)\n\nresults_singleTrial &lt;- runTrial(\n  inputs$n, inputs$recruit_period, p, inputs$events_at_interim,\n  inputs$alpha_interim, inputs$alpha_final\n)\nresults &lt;- results_singleTrial$results|&gt; round(3)  |&gt; t() |&gt; as.data.frame() \nnames(results)[1] &lt;- \"value\"\nresults$variable &lt;- rownames(results)\nrownames(results) &lt;- NULL\nresults &lt;- left_join(results, dict)\n\n\nJoining with `by = join_by(variable)`\n\n\nCode\nresults &lt;- results[, c(\"variable\", \"label\", \"value\")]\n\n\n\n\nCode\ntab_singleT &lt;- flextable(results) |&gt; theme_vanilla() \ntab_singleT\n\n\n\n\nTable 1: Results from a single simulated trial\n\n\n\nvariablelabelvaluenevents0Number of events in control group16.000nevents1Number of events in experimental group4.000pevents0Proportion of events in control group0.140pevents1Proportion of events in experimental group0.030sample_sizeSample size249.000interim_timeTime (days) at interim249.000interim_orOdds ratio at interim0.187interim_lciLower CI for OR at interim0.061interim_uciUpper CI for OR at interim0.577interim_pP-value of any difference in treatments at interim0.004interim_stopWhether the trial would have stopped at interim1.000final_orOdds ratio at final analysis0.305final_lciLower CI for odds ratio0.141final_uciUpper CI for odds ratio0.661final_pP-value for treatment difference at final analysis0.003final_stopWhether the trial is conclusive at final analysis1.000stopWhether the trial was conclusive (at final or interim)1.000flipflopThe probability of trial flip-flopping0.000"
  },
  {
    "objectID": "posts/2026-01-12-Adaptive-designs/index.html#simulate-multiple-trials",
    "href": "posts/2026-01-12-Adaptive-designs/index.html#simulate-multiple-trials",
    "title": "Adaptive Methods for Clinical Trials",
    "section": "",
    "text": "Code\nrunMultipleTrials &lt;- function(nsims, seed, n, recruit_period, p, \n                              events_at_interim, alpha_interim, alpha_final){\n  # random seed\n  seeds &lt;- seed + seq(1: nsims)\n  # simulate\n  multiple_trials &lt;- lapply(1:nsims, function(x){\n    set.seed(seeds[x])\n    y &lt;- runTrial(n, recruit_period, p, events_at_interim, alpha_interim,\n                  alpha_final)\n    return(y)\n  })\n  \n  # summarise\n  results &lt;- lapply(multiple_trials, function(x) return(x$results))\n  results_all &lt;- do.call(rbind, results)\n  # saveRDS(multiple_trials, file = \"results_multTrials.rds\")\n  \n  # remove any trials with non-estimable CIs and calculate summary\n  x &lt;- which(apply(results_all, 1, function(x) any(is.na(x))))\n  if(length(x) &gt; 0){\n    result_summary &lt;- apply(results_all[-x,], 2, summary)\n  } else{\n    result_summary &lt;- apply(results_all, 2, summary)\n  }\n  \n  return(\n    list(\n      results_all = results_all,\n      results_summary = results_summary,\n      seeds = seeds\n    )\n  )\n}\n\n\nParallelize computations using future.apply}\n\n\nCode\nlibrary(future.apply)\nlibrary(progressr)\n\nrunMultipleTrials_Parallel &lt;- function(nsims, seed, n, recruit_period, p_param, \n                                       events_at_interim, alpha_interim, alpha_final) {\n  \n  # 1. Setup Parallel Backend\n  # Using 'multisession' is the most stable across Windows/Mac/Linux\n  plan(multisession, workers = parallelly::availableCores() - 1)\n  \n  # Ensure cleanup of workers when function finishes or crashes\n  on.exit(plan(sequential))\n  \n  seeds &lt;- seed + seq_len(nsims)\n  \n  # 2. Access the progressor from the calling environment\n  p &lt;- progressor(steps = nsims)\n  \n  # 3. Run Simulations\n  multiple_trials &lt;- future_lapply(1:nsims, function(i) {\n    # Increment progress bar\n    p(message = sprintf(\"Simulating trial %g\", i))\n    \n    set.seed(seeds[i])\n    \n    # Run the individual trial\n    # Note: I renamed your 'p' to 'p_param' to avoid conflict with progressor 'p'\n    y &lt;- runTrial(n, recruit_period, p_param, events_at_interim, alpha_interim, alpha_final)\n    \n    return(y)\n  }, future.seed = TRUE) \n  \n  # 4. Summarize Results\n  results &lt;- lapply(multiple_trials, function(x) x$results)\n  results_all &lt;- do.call(rbind, results)\n  \n  # Clean up non-estimable rows (NAs from glm failures)\n  results_clean &lt;- results_all[complete.cases(results_all), ]\n  result_summary &lt;- apply(results_clean, 2, summary)\n  \n  return(list(\n    results_all = results_all,\n    results_summary = result_summary,\n    seeds = seeds\n  ))\n}\n\n\nPrint results\n\n\nCode\ntab_mulT &lt;- results_mulT$results_summary |&gt; round(3) |&gt; t() |&gt; as.data.frame()\ntab_mulT$variable &lt;- rownames(tab_mulT)\nrownames(tab_mulT) &lt;- NULL\nnames(tab_mulT) &lt;- c(\"Minimum\", \"Q1\", \"Median\", \"Mean\", \"Q3\", \"Maximum\", \"variable\")\ntab_mulT &lt;- left_join(tab_mulT, dict) \n\n\nJoining with `by = join_by(variable)`\n\n\nCode\ntab_mulT &lt;- tab_mulT |&gt; select(variable, label, everything())\nnames(tab_mulT) &lt;- stringr::str_to_sentence(names(tab_mulT))\n\n\n\n\nCode\nt_mulT &lt;- flextable(tab_mulT) |&gt; theme_vanilla() \nt_mulT\n\n\n\n\nTable 2: Results from a multiple simulated trial\n\n\n\nVariableLabelMinimumQ1MedianMeanQ3Maximumnevents0Number of events in control group12.00024.00028.00027.58232.00047.000nevents1Number of events in experimental group1.0009.00011.00011.03714.00025.000pevents0Proportion of events in control group0.0430.0880.0990.1010.1120.254pevents1Proportion of events in experimental group0.0050.0300.0390.0390.0470.082sample_sizeSample size138.000584.000584.000554.664584.000584.000interim_timeTime (days) at interim116.000243.000281.000286.926327.000565.000interim_orOdds ratio at interim0.0000.2520.3680.4080.5181.626interim_lciLower CI for OR at interim0.0000.0830.1350.1520.2010.650interim_uciUpper CI for OR at interim0.3020.7601.009Inf1.340Infinterim_pP-value of any difference in treatments at interim0.0000.0140.0520.1410.1751.000interim_stopWhether the trial would have stopped at interim0.0000.0000.0000.0980.0001.000final_orOdds ratio at final analysis0.0590.2860.3710.3910.4731.304final_lciLower CI for odds ratio0.0140.1350.1840.1940.2400.651final_uciUpper CI for odds ratio0.2410.6060.7500.7930.9372.653final_pP-value for treatment difference at final analysis0.0000.0010.0060.0420.0320.994final_stopWhether the trial is conclusive at final analysis0.0001.0001.0000.8041.0001.000stopWhether the trial was conclusive (at final or interim)0.0001.0001.0000.8051.0001.000flipflopThe probability of trial flip-flopping0.0000.0000.0000.0010.0001.000"
  },
  {
    "objectID": "posts/2026-01-15-survey-design/index.html",
    "href": "posts/2026-01-15-survey-design/index.html",
    "title": "Surveys: Design and Analysis",
    "section": "",
    "text": "Provided by {srvyrexploR} library. The first is America’s elections data.\n\ndata('anes_2020', package = \"srvyrexploR\")\nanes_2020 %&gt;% select(-matches(\"^V\\\\d\")) %&gt;%# starts with V followed by a digit\nglimpse()\n\nRows: 7,453\nColumns: 21\n$ CaseID                  &lt;dbl&gt; 200015, 200022, 200039, 200046, 200053, 200060…\n$ InterviewMode           &lt;fct&gt; Web, Web, Web, Web, Web, Web, Web, Web, Web, W…\n$ Weight                  &lt;dbl&gt; 1.0057375, 1.1634731, 0.7686811, 0.5210195, 0.…\n$ VarUnit                 &lt;fct&gt; 2, 2, 1, 2, 1, 2, 1, 2, 2, 2, 1, 1, 2, 2, 2, 2…\n$ Stratum                 &lt;fct&gt; 9, 26, 41, 29, 23, 37, 7, 37, 32, 41, 22, 7, 3…\n$ CampaignInterest        &lt;fct&gt; Somewhat interested, Not much interested, Some…\n$ EarlyVote2020           &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, Yes, NA, NA, N…\n$ VotedPres2016           &lt;fct&gt; Yes, Yes, Yes, Yes, Yes, No, Yes, No, Yes, Yes…\n$ VotedPres2016_selection &lt;fct&gt; Trump, Other, Clinton, Clinton, Trump, NA, Oth…\n$ PartyID                 &lt;fct&gt; Strong republican, Independent, Independent-de…\n$ TrustGovernment         &lt;fct&gt; Never, Never, Some of the time, About half the…\n$ TrustPeople             &lt;fct&gt; About half the time, Some of the time, Some of…\n$ Age                     &lt;dbl&gt; 46, 37, 40, 41, 72, 71, 37, 45, 70, 43, 37, 55…\n$ AgeGroup                &lt;fct&gt; 40-49, 30-39, 40-49, 40-49, 70 or older, 70 or…\n$ Education               &lt;fct&gt; Bachelor's, Post HS, High school, Post HS, Gra…\n$ RaceEth                 &lt;fct&gt; \"Hispanic\", \"Asian, NH/PI\", \"White\", \"Asian, N…\n$ Gender                  &lt;fct&gt; Male, Female, Female, Male, Male, Female, Fema…\n$ Income                  &lt;fct&gt; \"$175,000-249,999\", \"$70,000-74,999\", \"$100,00…\n$ Income7                 &lt;fct&gt; $125k or more, $60k to &lt; 80k, $100k to &lt; 125k,…\n$ VotedPres2020           &lt;fct&gt; NA, Yes, Yes, Yes, Yes, Yes, Yes, NA, Yes, Yes…\n$ VotedPres2020_selection &lt;fct&gt; NA, Other, Biden, Biden, Trump, Biden, Trump, …\n\n\n\nanes_2020 %&gt;% select(-matches(\"^V\\\\d\")) %&gt;% skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n7453\n\n\nNumber of columns\n21\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n18\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nInterviewMode\n0\n1.00\nFALSE\n3\nWeb: 7064, Vid: 274, Tel: 115\n\n\nVarUnit\n0\n1.00\nFALSE\n3\n2: 3750, 1: 3689, 3: 14\n\n\nStratum\n0\n1.00\nFALSE\n50\n12: 179, 6: 172, 27: 172, 21: 170\n\n\nCampaignInterest\n1\n1.00\nFALSE\n3\nVer: 3940, Som: 2569, Not: 943\n\n\nEarlyVote2020\n6963\n0.07\nFALSE\n2\nYes: 375, No: 115\n\n\nVotedPres2016\n21\n1.00\nFALSE\n2\nYes: 5810, No: 1622\n\n\nVotedPres2016_selection\n1686\n0.77\nFALSE\n3\nCli: 2911, Tru: 2466, Oth: 390\n\n\nPartyID\n25\n1.00\nFALSE\n7\nStr: 1796, Str: 1545, Ind: 881, Ind: 876\n\n\nTrustGovernment\n29\n1.00\nFALSE\n5\nSom: 3313, Abo: 2313, Mos: 1016, Nev: 702\n\n\nTrustPeople\n13\n1.00\nFALSE\n5\nMos: 3511, Abo: 2020, Som: 1597, Nev: 264\n\n\nAgeGroup\n294\n0.96\nFALSE\n6\n60-: 1436, 70 : 1330, 30-: 1241, 50-: 1200\n\n\nEducation\n116\n0.98\nFALSE\n5\nPos: 2514, Bac: 1877, Gra: 1474, Hig: 1160\n\n\nRaceEth\n81\n0.99\nFALSE\n6\nWhi: 5420, His: 662, Bla: 650, Asi: 248\n\n\nGender\n51\n0.99\nFALSE\n2\nFem: 4027, Mal: 3375\n\n\nIncome\n517\n0.93\nFALSE\n22\nUnd: 647, $50: 485, $10: 451, $25: 405\n\n\nIncome7\n517\n0.93\nFALSE\n7\n$12: 1468, Und: 1076, $20: 1051, $40: 984\n\n\nVotedPres2020\n1053\n0.86\nFALSE\n2\nYes: 6313, No: 87\n\n\nVotedPres2020_selection\n1219\n0.84\nFALSE\n3\nBid: 3509, Tru: 2567, Oth: 158\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nCaseID\n0\n1.00\n336416.23\n103653.12\n200015.00\n225427.00\n335416.00\n427865.00\n535469.00\n▇▃▃▆▂\n\n\nWeight\n0\n1.00\n1.00\n1.02\n0.01\n0.39\n0.69\n1.21\n6.65\n▇▂▁▁▁\n\n\nAge\n294\n0.96\n51.83\n17.14\n18.00\n37.00\n53.00\n66.00\n80.00\n▅▇▇▇▇\n\n\n\n\n\nResidential energy consumption survey\n\ndata(\"recs_2020\", package = \"srvyrexploR\")\nrecs_2020 %&gt;% \n  select(-matches(\"^NWEIGHT\")) %&gt;% skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n18496\n\n\nNumber of columns\n39\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nfactor\n10\n\n\nlogical\n2\n\n\nnumeric\n25\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nREGIONC\n0\n1\n4\n9\n0\n4\n0\n\n\nSTATE_FIPS\n0\n1\n2\n2\n0\n51\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nClimateRegion_BA\n0\n1.00\nFALSE\n8\nCol: 7116, Mix: 5579, Hot: 2545, Hot: 1577\n\n\nUrbanicity\n0\n1.00\nFALSE\n3\nUrb: 12395, Rur: 4081, Urb: 2020\n\n\nRegion\n0\n1.00\nFALSE\n4\nSou: 6426, Wes: 4581, Mid: 3832, Nor: 3657\n\n\nDivision\n0\n1.00\nFALSE\n10\nSou: 3256, Pac: 2497, Eas: 2014, Mid: 1977\n\n\nstate_postal\n0\n1.00\nFALSE\n51\nCA: 1152, TX: 1016, NY: 904, FL: 655\n\n\nstate_name\n0\n1.00\nFALSE\n51\nCal: 1152, Tex: 1016, New: 904, Flo: 655\n\n\nHousingUnitType\n0\n1.00\nFALSE\n5\nSin: 12319, Apa: 2439, Sin: 1751, Apa: 1013\n\n\nYearMade\n0\n1.00\nTRUE\n9\n197: 2817, 200: 2748, Bef: 2721, 199: 2451\n\n\nHeatingBehavior\n751\n0.96\nFALSE\n6\nSet: 7806, Man: 4654, Pro: 3310, Tur: 1491\n\n\nACBehavior\n2325\n0.87\nFALSE\n6\nSet: 6738, Man: 3637, Tur: 2746, Pro: 2638\n\n\n\nVariable type: logical\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\n\n\nSpaceHeatingUsed\n0\n1\n0.96\nTRU: 17745, FAL: 751\n\n\nACUsed\n0\n1\n0.87\nTRU: 16171, FAL: 2325\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nDOEID\n0\n1.00\n109248.50\n5339.48\n100001.00\n104624.75\n109248.50\n113872.25\n118496.00\n▇▇▇▇▇\n\n\nHDD65\n0\n1.00\n4271.81\n2329.46\n0.00\n2433.75\n4396.50\n5810.25\n17383.00\n▆▇▂▁▁\n\n\nCDD65\n0\n1.00\n1525.52\n1143.24\n0.00\n814.00\n1179.00\n1805.00\n5534.00\n▇▆▂▁▁\n\n\nHDD30YR\n0\n1.00\n4679.41\n2338.52\n0.00\n2897.75\n4825.00\n6290.00\n16071.00\n▅▇▃▁▁\n\n\nCDD30YR\n0\n1.00\n1309.88\n988.47\n0.00\n601.00\n1020.00\n1703.00\n4905.00\n▇▅▂▁▁\n\n\nTOTSQFT_EN\n0\n1.00\n1959.97\n1164.97\n200.00\n1100.00\n1700.00\n2510.00\n15000.00\n▇▁▁▁▁\n\n\nTOTHSQFT\n0\n1.00\n1744.46\n1106.33\n0.00\n1000.00\n1520.00\n2300.00\n15000.00\n▇▁▁▁▁\n\n\nTOTCSQFT\n0\n1.00\n1393.57\n1168.31\n0.00\n460.00\n1200.00\n2000.00\n14600.00\n▇▁▁▁▁\n\n\nWinterTempDay\n751\n0.96\n69.77\n3.62\n50.00\n68.00\n70.00\n72.00\n90.00\n▁▁▇▁▁\n\n\nWinterTempAway\n751\n0.96\n67.45\n5.02\n50.00\n65.00\n68.00\n70.00\n90.00\n▁▅▇▁▁\n\n\nWinterTempNight\n751\n0.96\n68.01\n4.66\n50.00\n65.00\n68.00\n70.00\n90.00\n▁▃▇▁▁\n\n\nSummerTempDay\n2325\n0.87\n72.01\n4.74\n50.00\n70.00\n72.00\n75.00\n90.00\n▁▁▇▃▁\n\n\nSummerTempAway\n2325\n0.87\n73.45\n5.68\n50.00\n70.00\n74.00\n78.00\n90.00\n▁▁▇▇▁\n\n\nSummerTempNight\n2325\n0.87\n71.22\n4.80\n50.00\n68.00\n72.00\n74.00\n90.00\n▁▂▇▃▁\n\n\nBTUEL\n0\n1.00\n37016.17\n24265.34\n143.32\n20205.76\n31890.03\n48297.98\n628155.47\n▇▁▁▁▁\n\n\nDOLLAREL\n0\n1.00\n1424.81\n862.08\n-889.48\n836.49\n1257.86\n1818.95\n15680.18\n▇▁▁▁▁\n\n\nBTUNG\n0\n1.00\n36960.50\n46538.32\n0.00\n0.00\n22011.97\n62714.20\n1134708.69\n▇▁▁▁▁\n\n\nDOLLARNG\n0\n1.00\n396.05\n483.41\n0.00\n0.00\n313.86\n644.93\n8154.98\n▇▁▁▁▁\n\n\nBTULP\n0\n1.00\n3916.95\n16741.93\n0.00\n0.00\n0.00\n0.00\n364215.39\n▇▁▁▁▁\n\n\nDOLLARLP\n0\n1.00\n80.89\n333.79\n0.00\n0.00\n0.00\n0.00\n6621.44\n▇▁▁▁▁\n\n\nBTUFO\n0\n1.00\n5108.60\n22698.98\n0.00\n0.00\n0.00\n0.00\n426268.50\n▇▁▁▁▁\n\n\nDOLLARFO\n0\n1.00\n88.43\n395.97\n0.00\n0.00\n0.00\n0.00\n7003.69\n▇▁▁▁▁\n\n\nBTUWOOD\n0\n1.00\n3596.49\n17766.37\n0.00\n0.00\n0.00\n0.00\n500000.00\n▇▁▁▁▁\n\n\nTOTALBTU\n0\n1.00\n83002.29\n53206.00\n1182.22\n45565.16\n74180.11\n108535.02\n1367548.13\n▇▁▁▁▁\n\n\nTOTALDOL\n0\n1.00\n1990.17\n1108.99\n-150.51\n1258.32\n1793.20\n2472.00\n20043.41\n▇▁▁▁▁\n\n\n\n\n\nNumeric vars\n\nrecs_2020 %&gt;% select(where(is.numeric)) %&gt;% colnames()\n\n [1] \"DOEID\"           \"HDD65\"           \"CDD65\"           \"HDD30YR\"        \n [5] \"CDD30YR\"         \"TOTSQFT_EN\"      \"TOTHSQFT\"        \"TOTCSQFT\"       \n [9] \"WinterTempDay\"   \"WinterTempAway\"  \"WinterTempNight\" \"SummerTempDay\"  \n[13] \"SummerTempAway\"  \"SummerTempNight\" \"NWEIGHT\"         \"NWEIGHT1\"       \n[17] \"NWEIGHT2\"        \"NWEIGHT3\"        \"NWEIGHT4\"        \"NWEIGHT5\"       \n[21] \"NWEIGHT6\"        \"NWEIGHT7\"        \"NWEIGHT8\"        \"NWEIGHT9\"       \n[25] \"NWEIGHT10\"       \"NWEIGHT11\"       \"NWEIGHT12\"       \"NWEIGHT13\"      \n[29] \"NWEIGHT14\"       \"NWEIGHT15\"       \"NWEIGHT16\"       \"NWEIGHT17\"      \n[33] \"NWEIGHT18\"       \"NWEIGHT19\"       \"NWEIGHT20\"       \"NWEIGHT21\"      \n[37] \"NWEIGHT22\"       \"NWEIGHT23\"       \"NWEIGHT24\"       \"NWEIGHT25\"      \n[41] \"NWEIGHT26\"       \"NWEIGHT27\"       \"NWEIGHT28\"       \"NWEIGHT29\"      \n[45] \"NWEIGHT30\"       \"NWEIGHT31\"       \"NWEIGHT32\"       \"NWEIGHT33\"      \n[49] \"NWEIGHT34\"       \"NWEIGHT35\"       \"NWEIGHT36\"       \"NWEIGHT37\"      \n[53] \"NWEIGHT38\"       \"NWEIGHT39\"       \"NWEIGHT40\"       \"NWEIGHT41\"      \n[57] \"NWEIGHT42\"       \"NWEIGHT43\"       \"NWEIGHT44\"       \"NWEIGHT45\"      \n[61] \"NWEIGHT46\"       \"NWEIGHT47\"       \"NWEIGHT48\"       \"NWEIGHT49\"      \n[65] \"NWEIGHT50\"       \"NWEIGHT51\"       \"NWEIGHT52\"       \"NWEIGHT53\"      \n[69] \"NWEIGHT54\"       \"NWEIGHT55\"       \"NWEIGHT56\"       \"NWEIGHT57\"      \n[73] \"NWEIGHT58\"       \"NWEIGHT59\"       \"NWEIGHT60\"       \"BTUEL\"          \n[77] \"DOLLAREL\"        \"BTUNG\"           \"DOLLARNG\"        \"BTULP\"          \n[81] \"DOLLARLP\"        \"BTUFO\"           \"DOLLARFO\"        \"BTUWOOD\"        \n[85] \"TOTALBTU\"        \"TOTALDOL\"       \n\n\nAmerican national election studies design object\n\ncps_state_in &lt;- getCensus(\n  name = \"cps/basic/mar\",\n  vintage = 2020,\n  region = \"state\",\n  vars = c(\n    \"HRMONTH\", \"HRYEAR4\",\n    \"PRTAGE\", \"PRCITSHP\", \"PWSSWGT\"\n  ),\n  key = Sys.getenv(\"CENSUS_KEY\")\n)\n\ncps_state &lt;- cps_state_in %&gt;%\n  as_tibble() %&gt;%\n  mutate(\n    across(\n      .cols = everything(),\n      .fns = as.numeric\n    )\n  )\n\n\ncps_narrow_resp &lt;- cps_state %&gt;%\n  filter(\n    PRTAGE &gt;= 18,\n    PRCITSHP %in% c(1:4)\n  )\n\nCalculate use population from the narrow data. Weights should add to total pop.\n\ntargetpop &lt;- cps_narrow_resp %&gt;% \n  pull(PWSSWGT) %&gt;%\n  sum()\nscales::comma(targetpop)\n\n[1] \"231,034,125\"\n\n\nWe can the weight the us election study appropriately\n\nanes_adjwgt &lt;- anes_2020 %&gt;%\n  mutate(\n    weight = V200010b / sum(V200010b) * targetpop\n  )\n\nWe then make it conform to a survey design\n\nanes_des &lt;- anes_adjwgt %&gt;%\n  as_survey_design(\n    weights = weight,\n    strata = V200010d,\n    ids = V200010c,\n    nest = TRUE\n  )\nanes_des\n\nStratified 1 - level Cluster Sampling design (with replacement)\nWith (101) clusters.\nCalled via srvyr\nSampling variables:\n  - ids: V200010c \n  - strata: V200010d \n  - weights: weight \nData variables: \n  - V200001 (dbl), CaseID (dbl), V200002 (hvn_lbll), InterviewMode (fct),\n    V200010b (dbl), Weight (dbl), V200010c (dbl), VarUnit (fct), V200010d\n    (dbl), Stratum (fct), V201006 (hvn_lbll), CampaignInterest (fct), V201023\n    (hvn_lbll), EarlyVote2020 (fct), V201024 (hvn_lbll), V201025x (hvn_lbll),\n    V201028 (hvn_lbll), V201029 (hvn_lbll), V201101 (hvn_lbll), V201102\n    (hvn_lbll), VotedPres2016 (fct), V201103 (hvn_lbll),\n    VotedPres2016_selection (fct), V201228 (hvn_lbll), V201229 (hvn_lbll),\n    V201230 (hvn_lbll), V201231x (hvn_lbll), PartyID (fct), V201233 (hvn_lbll),\n    TrustGovernment (fct), V201237 (hvn_lbll), TrustPeople (fct), V201507x\n    (hvn_lbll), Age (dbl), AgeGroup (fct), V201510 (hvn_lbll), Education (fct),\n    V201546 (hvn_lbll), V201547a (hvn_lbll), V201547b (hvn_lbll), V201547c\n    (hvn_lbll), V201547d (hvn_lbll), V201547e (hvn_lbll), V201547z (hvn_lbll),\n    V201549x (hvn_lbll), RaceEth (fct), V201600 (hvn_lbll), Gender (fct),\n    V201607 (hvn_lbll), V201610 (hvn_lbll), V201611 (hvn_lbll), V201613\n    (hvn_lbll), V201615 (hvn_lbll), V201616 (hvn_lbll), V201617x (hvn_lbll),\n    Income (fct), Income7 (fct), V202051 (hvn_lbll), V202066 (hvn_lbll),\n    V202072 (hvn_lbll), VotedPres2020 (fct), V202073 (hvn_lbll), V202109x\n    (hvn_lbll), V202110x (hvn_lbll), VotedPres2020_selection (fct), weight\n    (dbl)\n\n\nUsing replicated weights\n\nrecs_des &lt;- recs_2020 %&gt;%\n  as_survey_rep(\n    weights = NWEIGHT,\n    repweights = NWEIGHT1:NWEIGHT60,\n    type = \"JK1\",\n    scale = 59 / 60,\n    mse = TRUE\n  )\n\nrecs_des\n\nCall: Called via srvyr\nUnstratified cluster jacknife (JK1) with 60 replicates and MSE variances.\nSampling variables:\n  - repweights: `NWEIGHT1 + NWEIGHT2 + NWEIGHT3 + NWEIGHT4 + NWEIGHT5 +\n    NWEIGHT6 + NWEIGHT7 + NWEIGHT8 + NWEIGHT9 + NWEIGHT10 + NWEIGHT11 +\n    NWEIGHT12 + NWEIGHT13 + NWEIGHT14 + NWEIGHT15 + NWEIGHT16 + NWEIGHT17 +\n    NWEIGHT18 + NWEIGHT19 + NWEIGHT20 + NWEIGHT21 + NWEIGHT22 + NWEIGHT23 +\n    NWEIGHT24 + NWEIGHT25 + NWEIGHT26 + NWEIGHT27 + NWEIGHT28 + NWEIGHT29 +\n    NWEIGHT30 + NWEIGHT31 + NWEIGHT32 + NWEIGHT33 + NWEIGHT34 + NWEIGHT35 +\n    NWEIGHT36 + NWEIGHT37 + NWEIGHT38 + NWEIGHT39 + NWEIGHT40 + NWEIGHT41 +\n    NWEIGHT42 + NWEIGHT43 + NWEIGHT44 + NWEIGHT45 + NWEIGHT46 + NWEIGHT47 +\n    NWEIGHT48 + NWEIGHT49 + NWEIGHT50 + NWEIGHT51 + NWEIGHT52 + NWEIGHT53 +\n    NWEIGHT54 + NWEIGHT55 + NWEIGHT56 + NWEIGHT57 + NWEIGHT58 + NWEIGHT59 +\n    NWEIGHT60` \n  - weights: NWEIGHT \nData variables: \n  - DOEID (dbl), ClimateRegion_BA (fct), Urbanicity (fct), Region (fct),\n    REGIONC (chr), Division (fct), STATE_FIPS (chr), state_postal (fct),\n    state_name (fct), HDD65 (dbl), CDD65 (dbl), HDD30YR (dbl), CDD30YR (dbl),\n    HousingUnitType (fct), YearMade (ord), TOTSQFT_EN (dbl), TOTHSQFT (dbl),\n    TOTCSQFT (dbl), SpaceHeatingUsed (lgl), ACUsed (lgl), HeatingBehavior\n    (fct), WinterTempDay (dbl), WinterTempAway (dbl), WinterTempNight (dbl),\n    ACBehavior (fct), SummerTempDay (dbl), SummerTempAway (dbl),\n    SummerTempNight (dbl), NWEIGHT (dbl), NWEIGHT1 (dbl), NWEIGHT2 (dbl),\n    NWEIGHT3 (dbl), NWEIGHT4 (dbl), NWEIGHT5 (dbl), NWEIGHT6 (dbl), NWEIGHT7\n    (dbl), NWEIGHT8 (dbl), NWEIGHT9 (dbl), NWEIGHT10 (dbl), NWEIGHT11 (dbl),\n    NWEIGHT12 (dbl), NWEIGHT13 (dbl), NWEIGHT14 (dbl), NWEIGHT15 (dbl),\n    NWEIGHT16 (dbl), NWEIGHT17 (dbl), NWEIGHT18 (dbl), NWEIGHT19 (dbl),\n    NWEIGHT20 (dbl), NWEIGHT21 (dbl), NWEIGHT22 (dbl), NWEIGHT23 (dbl),\n    NWEIGHT24 (dbl), NWEIGHT25 (dbl), NWEIGHT26 (dbl), NWEIGHT27 (dbl),\n    NWEIGHT28 (dbl), NWEIGHT29 (dbl), NWEIGHT30 (dbl), NWEIGHT31 (dbl),\n    NWEIGHT32 (dbl), NWEIGHT33 (dbl), NWEIGHT34 (dbl), NWEIGHT35 (dbl),\n    NWEIGHT36 (dbl), NWEIGHT37 (dbl), NWEIGHT38 (dbl), NWEIGHT39 (dbl),\n    NWEIGHT40 (dbl), NWEIGHT41 (dbl), NWEIGHT42 (dbl), NWEIGHT43 (dbl),\n    NWEIGHT44 (dbl), NWEIGHT45 (dbl), NWEIGHT46 (dbl), NWEIGHT47 (dbl),\n    NWEIGHT48 (dbl), NWEIGHT49 (dbl), NWEIGHT50 (dbl), NWEIGHT51 (dbl),\n    NWEIGHT52 (dbl), NWEIGHT53 (dbl), NWEIGHT54 (dbl), NWEIGHT55 (dbl),\n    NWEIGHT56 (dbl), NWEIGHT57 (dbl), NWEIGHT58 (dbl), NWEIGHT59 (dbl),\n    NWEIGHT60 (dbl), BTUEL (dbl), DOLLAREL (dbl), BTUNG (dbl), DOLLARNG (dbl),\n    BTULP (dbl), DOLLARLP (dbl), BTUFO (dbl), DOLLARFO (dbl), BTUWOOD (dbl),\n    TOTALBTU (dbl), TOTALDOL (dbl)\n\n\n\n\n\n\nCount observations with survey_count() or survey_tally()\nSum variables with survey_total()\nMeans and proportions: survey_mean(), survey_prop()\nQuantiles and medians: survey_quantile(), survey_median()\nCorrelations: survey_cor()\nRatios: survey_ratio()\nVariances and standard deviation: survey_var(), survey_sd()\n\n\n\n\nrecs_des %&gt;% survey_tally()\n\n# A tibble: 1 × 2\n           n  n_se\n       &lt;dbl&gt; &lt;dbl&gt;\n1 123529025. 0.148\n\n\nEstimated counts by subgroups\n\nrecs_des %&gt;% survey_count(Region, Division, name = \"N\")\n\n# A tibble: 10 × 4\n   Region    Division                   N         N_se\n   &lt;fct&gt;     &lt;fct&gt;                  &lt;dbl&gt;        &lt;dbl&gt;\n 1 Northeast New England         5876166  0.0000000137\n 2 Northeast Middle Atlantic    16043503  0.0000000487\n 3 Midwest   East North Central 18546912  0.000000437 \n 4 Midwest   West North Central  8495815  0.0000000177\n 5 South     South Atlantic     24843261  0.0000000418\n 6 South     East South Central  7380717. 0.114       \n 7 South     West South Central 14619094  0.000488    \n 8 West      Mountain North      4615844  0.119       \n 9 West      Mountain South      4602070  0.0000000492\n10 West      Pacific            18505643. 0.00000295  \n\n\nTo achieve same result by survey_tally(), you first group\n\nrecs_des %&gt;% group_by(Region, Division) %&gt;% survey_tally(name = \"N\")\n\n# A tibble: 10 × 4\n# Groups:   Region [4]\n   Region    Division                   N         N_se\n   &lt;fct&gt;     &lt;fct&gt;                  &lt;dbl&gt;        &lt;dbl&gt;\n 1 Northeast New England         5876166  0.0000000137\n 2 Northeast Middle Atlantic    16043503  0.0000000487\n 3 Midwest   East North Central 18546912  0.000000437 \n 4 Midwest   West North Central  8495815  0.0000000177\n 5 South     South Atlantic     24843261  0.0000000418\n 6 South     East South Central  7380717. 0.114       \n 7 South     West South Central 14619094  0.000488    \n 8 West      Mountain North      4615844  0.119       \n 9 West      Mountain South      4602070  0.0000000492\n10 West      Pacific            18505643. 0.00000295  \n\n\n\n\n\nTo get population count estimate, we leave argument x, empty.\n\nrecs_des %&gt;% \n  summarize(\n    tot = survey_total()\n  )\n\n# A tibble: 1 × 2\n         tot tot_se\n       &lt;dbl&gt;  &lt;dbl&gt;\n1 123529025.  0.148\n\n\nOverall summation of a continuous variable\n\nrecs_des %&gt;%\n  summarize(elec_bill = survey_total(DOLLAREL))\n\n# A tibble: 1 × 2\n      elec_bill elec_bill_se\n          &lt;dbl&gt;        &lt;dbl&gt;\n1 170473527909.   664893504.\n\n\nSummation by groups\n\nrecs_des %&gt;% group_by(Region) %&gt;%\n  summarize(\n  elec_bill = survey_total(DOLLAREL, vartype = \"ci\")\n  \n)\n\n# A tibble: 4 × 4\n  Region       elec_bill elec_bill_low elec_bill_upp\n  &lt;fct&gt;            &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n1 Northeast 29430369947.  28788987554.  30071752341.\n2 Midwest   34972544751.  34339576041.  35605513460.\n3 South     72496840204.  71534780902.  73458899506.\n4 West      33573773008.  32909111702.  34238434313.\n\n\nYou can supply .by argument inside summarize\n\nrecs_des  %&gt;%\n  summarize(\n  elec_bill = survey_total(DOLLAREL, vartype = \"ci\"),\n  .by = Region\n)\n\n# A tibble: 4 × 4\n  Region       elec_bill elec_bill_low elec_bill_upp\n  &lt;fct&gt;            &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n1 Northeast 29430369947.  28788987554.  30071752341.\n2 Midwest   34972544751.  34339576041.  35605513460.\n3 South     72496840204.  71534780902.  73458899506.\n4 West      33573773008.  32909111702.  34238434313.\n\n\n\n\n\n\nrecs_des %&gt;% group_by(Region, ACUsed) %&gt;%\n  summarise(\n  p = survey_prop()\n)\n\nWhen `proportion` is unspecified, `survey_prop()` now defaults to `proportion = TRUE`.\nℹ This should improve confidence interval coverage.\nThis message is displayed once per session.\n\n\n# A tibble: 8 × 4\n# Groups:   Region [4]\n  Region    ACUsed      p    p_se\n  &lt;fct&gt;     &lt;lgl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 Northeast FALSE  0.110  0.00590\n2 Northeast TRUE   0.890  0.00590\n3 Midwest   FALSE  0.0666 0.00508\n4 Midwest   TRUE   0.933  0.00508\n5 South     FALSE  0.0581 0.00278\n6 South     TRUE   0.942  0.00278\n7 West      FALSE  0.255  0.00759\n8 West      TRUE   0.745  0.00759\n\n\nJoint proportions using interact\n\nrecs_des %&gt;% \n  group_by(\n    interact(Region, ACUsed)\n  ) %&gt;% \n  summarize(\n    p = survey_prop() \n  ) %&gt;% mutate(\n    p = scales::percent(p)\n  )\n\n# A tibble: 8 × 4\n  Region    ACUsed p         p_se\n  &lt;fct&gt;     &lt;lgl&gt;  &lt;chr&gt;    &lt;dbl&gt;\n1 Northeast FALSE  1.96%  0.00105\n2 Northeast TRUE   15.79% 0.00105\n3 Midwest   FALSE  1.46%  0.00111\n4 Midwest   TRUE   20.43% 0.00111\n5 South     FALSE  2.20%  0.00106\n6 South     TRUE   35.72% 0.00106\n7 West      FALSE  5.73%  0.00170\n8 West      TRUE   16.72% 0.00170\n\n\nOverall mean\n\nrecs_des %&gt;% summarize(\n  elec_bill = survey_mean(DOLLAREL, vartype = c(\"se\", \"ci\"))\n)\n\n# A tibble: 1 × 4\n  elec_bill elec_bill_se elec_bill_low elec_bill_upp\n      &lt;dbl&gt;        &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n1     1380.         5.38         1369.         1391.\n\n\n\n\n\nOverall quantiles\n\nrecs_des %&gt;%\n  summarize(\n    elec_bill = survey_quantile(DOLLAREL, quantiles = c(.25, .5, .75))\n  )\n\n# A tibble: 1 × 6\n  elec_bill_q25 elec_bill_q50 elec_bill_q75 elec_bill_q25_se elec_bill_q50_se\n          &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n1          795.         1215.         1770.             5.69             6.33\n# ℹ 1 more variable: elec_bill_q75_se &lt;dbl&gt;\n\n\nOverall median\n\nrecs_des %&gt;%\n  summarize(\n    elec_bill = survey_median(DOLLAREL)\n  )\n\n# A tibble: 1 × 2\n  elec_bill elec_bill_se\n      &lt;dbl&gt;        &lt;dbl&gt;\n1     1215.         6.33\n\n\nCorrelations\n\nrecs_des %&gt;%\n  summarize(SQFT_Elec_Corr = survey_corr(TOTSQFT_EN, BTUEL))\n\nWarning: There was 1 warning in `dplyr::summarise()`.\nℹ In argument: `SQFT_Elec_Corr = survey_corr(TOTSQFT_EN, BTUEL)`.\nCaused by warning in `sweep()`:\n! length(STATS) or dim(STATS) do not match dim(x)[MARGIN]\n\n\n# A tibble: 1 × 2\n  SQFT_Elec_Corr SQFT_Elec_Corr_se\n           &lt;dbl&gt;             &lt;dbl&gt;\n1          0.417           0.00689\n\n\n\n\nDesign effect measures the precision of an estimate under a particular sampling design relative to a simple random sampling design. If &lt;1, the design is statistically more efficient than SRS. It is used in the calculation of effective sample size - the sample size needed if we were to employ SRS\n\\[n_{eff} = \\frac{n}{D_{eff}}\\]\n\nrecs_des %&gt;%\n  summarize(\n    across(\n          c(BTUEL, BTUNG, BTULP, BTUFO, BTUWOOD),\n          ~ survey_mean(.x, deff = TRUE, vartype = NULL)\n    )\n  ) %&gt;% select(ends_with(\"deff\"))\n\n# A tibble: 1 × 5\n  BTUEL_deff BTUNG_deff BTULP_deff BTUFO_deff BTUWOOD_deff\n       &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;\n1      0.597      0.938       1.21      0.720         1.10\n\n\n\n\n\n\nrecs_des %&gt;%\n  cascade(DOLLAREL_mn = survey_mean(DOLLAREL))\n\n# A tibble: 1 × 2\n  DOLLAREL_mn DOLLAREL_mn_se\n        &lt;dbl&gt;          &lt;dbl&gt;\n1       1380.           5.38\n\n\nGroup\n\nrecs_des %&gt;%\n  group_by(Region) %&gt;%\n  cascade(DOLLAREL_mn = survey_mean(DOLLAREL), .fill = \"National\")\n\n# A tibble: 5 × 3\n  Region    DOLLAREL_mn DOLLAREL_mn_se\n  &lt;fct&gt;           &lt;dbl&gt;          &lt;dbl&gt;\n1 Northeast       1343.          14.6 \n2 Midwest         1293.          11.7 \n3 South           1548.          10.3 \n4 West            1211.          12.0 \n5 National        1380.           5.38\n\n\nOr through a long dplyr pipeline\n\nrecs_des %&gt;% \n  summarize(\n    cons_mean = survey_mean(DOLLAREL),\n    .by = Region\n  ) %&gt;% bind_rows(\n    ., recs_des %&gt;%\n      summarize(\n        cons_mean = survey_mean(DOLLAREL)\n      ) %&gt;% mutate(\n        ., Region = \"National\"\n      ) %&gt;% select(Region, everything())\n  )\n\n# A tibble: 5 × 3\n  Region    cons_mean cons_mean_se\n  &lt;chr&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1 Northeast     1343.        14.6 \n2 Midwest       1293.        11.7 \n3 South         1548.        10.3 \n4 West          1211.        12.0 \n5 National      1380.         5.38\n\n\n\n\n\n\nrecs_des %&gt;% \n  summarize(\n    across(\n      starts_with(\"BTU\"),\n      list(\n        Total = ~ survey_total(.x, vartype = \"cv\"),\n        Mean = ~ survey_mean(.x, vartype = \"cv\")\n      ),\n      .unpack = \"{outer}.{inner}\"\n    )\n  )\n\n# A tibble: 1 × 20\n  BTUEL_Total.coef BTUEL_Total._cv BTUEL_Mean.coef BTUEL_Mean._cv\n             &lt;dbl&gt;           &lt;dbl&gt;           &lt;dbl&gt;          &lt;dbl&gt;\n1    4453284510065         0.00377          36051.        0.00377\n# ℹ 16 more variables: BTUNG_Total.coef &lt;dbl&gt;, BTUNG_Total._cv &lt;dbl&gt;,\n#   BTUNG_Mean.coef &lt;dbl&gt;, BTUNG_Mean._cv &lt;dbl&gt;, BTULP_Total.coef &lt;dbl&gt;,\n#   BTULP_Total._cv &lt;dbl&gt;, BTULP_Mean.coef &lt;dbl&gt;, BTULP_Mean._cv &lt;dbl&gt;,\n#   BTUFO_Total.coef &lt;dbl&gt;, BTUFO_Total._cv &lt;dbl&gt;, BTUFO_Mean.coef &lt;dbl&gt;,\n#   BTUFO_Mean._cv &lt;dbl&gt;, BTUWOOD_Total.coef &lt;dbl&gt;, BTUWOOD_Total._cv &lt;dbl&gt;,\n#   BTUWOOD_Mean.coef &lt;dbl&gt;, BTUWOOD_Mean._cv &lt;dbl&gt;\n\n\n\n\n\n\ncalc &lt;- function(df, var) {\n  df %&gt;%\n    drop_na(!!sym(var)) %&gt;%\n    group_by(!!sym(var)) %&gt;%\n    summarize(p = survey_prop()) %&gt;%\n    mutate(Variable = var) %&gt;%\n    rename(Answer := !!sym(var)) %&gt;%\n    mutate(p = scales::percent(p)) %&gt;%\n    select(Variable, everything())\n}\n\nv &lt;- c(\"TrustGovernment\", \"TrustPeople\")\nmap2(\n  .x = list(anes_des),\n  .y = v,\n  .f = ~calc(df = .x, var = .y)\n) %&gt;% bind_rows()\n\n# A tibble: 10 × 4\n   Variable        Answer              p        p_se\n   &lt;chr&gt;           &lt;fct&gt;               &lt;chr&gt;   &lt;dbl&gt;\n 1 TrustGovernment Always              1.6%  0.00204\n 2 TrustGovernment Most of the time    13.2% 0.00553\n 3 TrustGovernment About half the time 30.9% 0.00829\n 4 TrustGovernment Some of the time    43.4% 0.00855\n 5 TrustGovernment Never               11.0% 0.00566\n 6 TrustPeople     Always              0.8%  0.00164\n 7 TrustPeople     Most of the time    41.4% 0.00857\n 8 TrustPeople     About half the time 28.2% 0.00776\n 9 TrustPeople     Some of the time    24.5% 0.00670\n10 TrustPeople     Never               5.0%  0.00422\n\n\n\n\n\n\n\n\nComparison of means and proportions: svyttest()\ngoodness-of-fit test: svygofchisq()\nTest of independence: svychisq()\nTest of homogeneity: svychisq()\n\n\n\n\none-sample t-test: compare to 0 - var ~ 0 or to a different value var - val = 0\ntwo-sample t-test:\n\nunpaired: two level grouping variable - var ~ groupvar or three level grouping variable - var ~ groupvar == level\npaired: var_1 - var_2 = 0\n\n\nExample: one-sample t-test for mean\n\nrecs_des %&gt;%\n  svyttest(\n    formula = SummerTempNight - 68 ~ 0,\n    design =.,\n    na.rm = TRUE\n  )\n\n\n    Design-based one-sample t-test\n\ndata:  SummerTempNight - 68 ~ 0\nt = 84.788, df = 58, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 3.287816 3.446810\nsample estimates:\n    mean \n3.367313 \n\n\nget mean summer temps at night\n\nrecs_des %&gt;% \n  summarize(m = survey_mean(SummerTempNight, na.rm = TRUE))\n\n# A tibble: 1 × 2\n      m   m_se\n  &lt;dbl&gt;  &lt;dbl&gt;\n1  71.4 0.0397\n\n\nExample: one-sample t-test for proportion\n\nrecs_des %&gt;% summarize(\n  p = survey_prop(),\n  .by = ACUsed\n)\n\n# A tibble: 2 × 3\n  ACUsed     p    p_se\n  &lt;lgl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 FALSE  0.113 0.00306\n2 TRUE   0.887 0.00306\n\n\n\nrecs_des %&gt;%\n  svyttest(\n    formula = (ACUsed == TRUE) - 0.90 ~ 0,\n    design = .,\n    na.rm = TRUE\n  ) %&gt;% tidy() %&gt;%\n  mutate(p.value = pretty_p_value(p.value)) %&gt;%\n  gt() %&gt;%\n  fmt_number()\n\n\n\nTable 1: One-sample t-test for estimate of US housholds use of A/C in their homes differing from 90%\n\n\n\n\n\n\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n\n\n−0.01\n−4.40\n&lt;0.0001\n58.00\n−0.02\n−0.01\nDesign-based one-sample t-test\ntwo.sided\n\n\n\n\n\n\n\n\n\n\nExample: paired two-sample t-test\n\nrecs_des %&gt;%\n  svyttest(\n    formula = SummerTempNight - WinterTempNight ~ 0,\n    design = .,\n    na.rm = TRUE\n  ) %&gt;% tidy() %&gt;%\n  mutate(\n    p.value = pretty_p_value(p.value)\n  ) %&gt;%\n  gt() %&gt;%\n  fmt_number()\n\n\n\nTable 2: Paired two-sample t-test\n\n\n\n\n\n\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n\n\n2.85\n50.83\n&lt;0.0001\n58.00\n2.74\n2.96\nDesign-based one-sample t-test\ntwo.sided\n\n\n\n\n\n\n\n\n\n\n\n\n\nSee https://tidy-survey-r.github.io/tidy-survey-book/c06-statistical-testing.html\nExample: goodness of fit test\n\nanes_des_educ &lt;- anes_des %&gt;%\n  mutate(\n    Education2 = fct_collapse(Education,\n      \"Bachelor or Higher\" = c(\"Bachelor's\", \"Graduate\")\n    )\n  )\n\nanes_des_educ %&gt;%\n  drop_na(Education2) %&gt;%\n  group_by(Education2) %&gt;% \n  summarize(p = survey_mean())\n\n# A tibble: 4 × 3\n  Education2              p    p_se\n  &lt;fct&gt;               &lt;dbl&gt;   &lt;dbl&gt;\n1 Less than HS       0.0805 0.00568\n2 High school        0.277  0.0102 \n3 Post HS            0.290  0.00713\n4 Bachelor or Higher 0.352  0.00732\n\n\n\nanes_des_educ %&gt;%\n  svygofchisq(\n    formula = ~Education2,\n    design = .,\n    p = c(0.11, 0.27, 0.28, 0.35),\n    na.rm = TRUE\n  ) %&gt;% tidy()\n\nMultiple parameters; naming those columns scale and df.\n\n\n# A tibble: 1 × 5\n    scale    df statistic  p.value method                                       \n    &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;                                        \n1 113213.  2.26  1922280. 0.000292 Design-based chi-squared test for given prob…\n\n\n\nex1_table &lt;- anes_des_educ %&gt;%\n  drop_na(Education2) %&gt;%\n  group_by(Education2) %&gt;%\n  summarize(Observed = survey_mean(vartype = \"ci\")) %&gt;%\n  rename(Education = Education2) %&gt;%\n  mutate(Expected = c(0.11, 0.27, 0.29, 0.33)) %&gt;%\n  select(Education, Expected, everything())\n\nex1_table\n\n# A tibble: 4 × 5\n  Education          Expected Observed Observed_low Observed_upp\n  &lt;fct&gt;                 &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1 Less than HS           0.11   0.0805       0.0691       0.0919\n2 High school            0.27   0.277        0.257        0.298 \n3 Post HS                0.29   0.290        0.276        0.305 \n4 Bachelor or Higher     0.33   0.352        0.337        0.367 \n\n\n\nex1_table %&gt;%\n  pivot_longer(\n    cols = c(\"Expected\", \"Observed\"),\n    names_to = \"Names\",\n    values_to = \"Proportion\"\n  ) %&gt;%\n  mutate(\n    Observed_low = if_else(Names == \"Observed\", Observed_low, NA_real_),\n    Observed_upp = if_else(Names == \"Observed\", Observed_upp, NA_real_),\n    Names = if_else(Names == \"Observed\",\n      \"ANES (observed)\", \"ACS (expected)\"\n    )\n  ) %&gt;%\n  ggplot(aes(x = Education, y = Proportion, color = Names)) +\n  geom_point(alpha = 0.75, size = 2) +\n  geom_errorbar(aes(ymin = Observed_low, ymax = Observed_upp),\n    width = 0.25\n  ) +\n  theme_bw() +\n  # scale_color_manual(name = \"Type\", values = book_colors[c(4, 1)]) +\n  theme(legend.position = \"bottom\", legend.title = element_blank())"
  },
  {
    "objectID": "posts/2026-01-15-survey-design/index.html#data",
    "href": "posts/2026-01-15-survey-design/index.html#data",
    "title": "Surveys: Design and Analysis",
    "section": "",
    "text": "Provided by {srvyrexploR} library. The first is America’s elections data.\n\ndata('anes_2020', package = \"srvyrexploR\")\nanes_2020 %&gt;% select(-matches(\"^V\\\\d\")) %&gt;%# starts with V followed by a digit\nglimpse()\n\nRows: 7,453\nColumns: 21\n$ CaseID                  &lt;dbl&gt; 200015, 200022, 200039, 200046, 200053, 200060…\n$ InterviewMode           &lt;fct&gt; Web, Web, Web, Web, Web, Web, Web, Web, Web, W…\n$ Weight                  &lt;dbl&gt; 1.0057375, 1.1634731, 0.7686811, 0.5210195, 0.…\n$ VarUnit                 &lt;fct&gt; 2, 2, 1, 2, 1, 2, 1, 2, 2, 2, 1, 1, 2, 2, 2, 2…\n$ Stratum                 &lt;fct&gt; 9, 26, 41, 29, 23, 37, 7, 37, 32, 41, 22, 7, 3…\n$ CampaignInterest        &lt;fct&gt; Somewhat interested, Not much interested, Some…\n$ EarlyVote2020           &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, Yes, NA, NA, N…\n$ VotedPres2016           &lt;fct&gt; Yes, Yes, Yes, Yes, Yes, No, Yes, No, Yes, Yes…\n$ VotedPres2016_selection &lt;fct&gt; Trump, Other, Clinton, Clinton, Trump, NA, Oth…\n$ PartyID                 &lt;fct&gt; Strong republican, Independent, Independent-de…\n$ TrustGovernment         &lt;fct&gt; Never, Never, Some of the time, About half the…\n$ TrustPeople             &lt;fct&gt; About half the time, Some of the time, Some of…\n$ Age                     &lt;dbl&gt; 46, 37, 40, 41, 72, 71, 37, 45, 70, 43, 37, 55…\n$ AgeGroup                &lt;fct&gt; 40-49, 30-39, 40-49, 40-49, 70 or older, 70 or…\n$ Education               &lt;fct&gt; Bachelor's, Post HS, High school, Post HS, Gra…\n$ RaceEth                 &lt;fct&gt; \"Hispanic\", \"Asian, NH/PI\", \"White\", \"Asian, N…\n$ Gender                  &lt;fct&gt; Male, Female, Female, Male, Male, Female, Fema…\n$ Income                  &lt;fct&gt; \"$175,000-249,999\", \"$70,000-74,999\", \"$100,00…\n$ Income7                 &lt;fct&gt; $125k or more, $60k to &lt; 80k, $100k to &lt; 125k,…\n$ VotedPres2020           &lt;fct&gt; NA, Yes, Yes, Yes, Yes, Yes, Yes, NA, Yes, Yes…\n$ VotedPres2020_selection &lt;fct&gt; NA, Other, Biden, Biden, Trump, Biden, Trump, …\n\n\n\nanes_2020 %&gt;% select(-matches(\"^V\\\\d\")) %&gt;% skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n7453\n\n\nNumber of columns\n21\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n18\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nInterviewMode\n0\n1.00\nFALSE\n3\nWeb: 7064, Vid: 274, Tel: 115\n\n\nVarUnit\n0\n1.00\nFALSE\n3\n2: 3750, 1: 3689, 3: 14\n\n\nStratum\n0\n1.00\nFALSE\n50\n12: 179, 6: 172, 27: 172, 21: 170\n\n\nCampaignInterest\n1\n1.00\nFALSE\n3\nVer: 3940, Som: 2569, Not: 943\n\n\nEarlyVote2020\n6963\n0.07\nFALSE\n2\nYes: 375, No: 115\n\n\nVotedPres2016\n21\n1.00\nFALSE\n2\nYes: 5810, No: 1622\n\n\nVotedPres2016_selection\n1686\n0.77\nFALSE\n3\nCli: 2911, Tru: 2466, Oth: 390\n\n\nPartyID\n25\n1.00\nFALSE\n7\nStr: 1796, Str: 1545, Ind: 881, Ind: 876\n\n\nTrustGovernment\n29\n1.00\nFALSE\n5\nSom: 3313, Abo: 2313, Mos: 1016, Nev: 702\n\n\nTrustPeople\n13\n1.00\nFALSE\n5\nMos: 3511, Abo: 2020, Som: 1597, Nev: 264\n\n\nAgeGroup\n294\n0.96\nFALSE\n6\n60-: 1436, 70 : 1330, 30-: 1241, 50-: 1200\n\n\nEducation\n116\n0.98\nFALSE\n5\nPos: 2514, Bac: 1877, Gra: 1474, Hig: 1160\n\n\nRaceEth\n81\n0.99\nFALSE\n6\nWhi: 5420, His: 662, Bla: 650, Asi: 248\n\n\nGender\n51\n0.99\nFALSE\n2\nFem: 4027, Mal: 3375\n\n\nIncome\n517\n0.93\nFALSE\n22\nUnd: 647, $50: 485, $10: 451, $25: 405\n\n\nIncome7\n517\n0.93\nFALSE\n7\n$12: 1468, Und: 1076, $20: 1051, $40: 984\n\n\nVotedPres2020\n1053\n0.86\nFALSE\n2\nYes: 6313, No: 87\n\n\nVotedPres2020_selection\n1219\n0.84\nFALSE\n3\nBid: 3509, Tru: 2567, Oth: 158\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nCaseID\n0\n1.00\n336416.23\n103653.12\n200015.00\n225427.00\n335416.00\n427865.00\n535469.00\n▇▃▃▆▂\n\n\nWeight\n0\n1.00\n1.00\n1.02\n0.01\n0.39\n0.69\n1.21\n6.65\n▇▂▁▁▁\n\n\nAge\n294\n0.96\n51.83\n17.14\n18.00\n37.00\n53.00\n66.00\n80.00\n▅▇▇▇▇\n\n\n\n\n\nResidential energy consumption survey\n\ndata(\"recs_2020\", package = \"srvyrexploR\")\nrecs_2020 %&gt;% \n  select(-matches(\"^NWEIGHT\")) %&gt;% skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n18496\n\n\nNumber of columns\n39\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nfactor\n10\n\n\nlogical\n2\n\n\nnumeric\n25\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nREGIONC\n0\n1\n4\n9\n0\n4\n0\n\n\nSTATE_FIPS\n0\n1\n2\n2\n0\n51\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nClimateRegion_BA\n0\n1.00\nFALSE\n8\nCol: 7116, Mix: 5579, Hot: 2545, Hot: 1577\n\n\nUrbanicity\n0\n1.00\nFALSE\n3\nUrb: 12395, Rur: 4081, Urb: 2020\n\n\nRegion\n0\n1.00\nFALSE\n4\nSou: 6426, Wes: 4581, Mid: 3832, Nor: 3657\n\n\nDivision\n0\n1.00\nFALSE\n10\nSou: 3256, Pac: 2497, Eas: 2014, Mid: 1977\n\n\nstate_postal\n0\n1.00\nFALSE\n51\nCA: 1152, TX: 1016, NY: 904, FL: 655\n\n\nstate_name\n0\n1.00\nFALSE\n51\nCal: 1152, Tex: 1016, New: 904, Flo: 655\n\n\nHousingUnitType\n0\n1.00\nFALSE\n5\nSin: 12319, Apa: 2439, Sin: 1751, Apa: 1013\n\n\nYearMade\n0\n1.00\nTRUE\n9\n197: 2817, 200: 2748, Bef: 2721, 199: 2451\n\n\nHeatingBehavior\n751\n0.96\nFALSE\n6\nSet: 7806, Man: 4654, Pro: 3310, Tur: 1491\n\n\nACBehavior\n2325\n0.87\nFALSE\n6\nSet: 6738, Man: 3637, Tur: 2746, Pro: 2638\n\n\n\nVariable type: logical\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\n\n\nSpaceHeatingUsed\n0\n1\n0.96\nTRU: 17745, FAL: 751\n\n\nACUsed\n0\n1\n0.87\nTRU: 16171, FAL: 2325\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nDOEID\n0\n1.00\n109248.50\n5339.48\n100001.00\n104624.75\n109248.50\n113872.25\n118496.00\n▇▇▇▇▇\n\n\nHDD65\n0\n1.00\n4271.81\n2329.46\n0.00\n2433.75\n4396.50\n5810.25\n17383.00\n▆▇▂▁▁\n\n\nCDD65\n0\n1.00\n1525.52\n1143.24\n0.00\n814.00\n1179.00\n1805.00\n5534.00\n▇▆▂▁▁\n\n\nHDD30YR\n0\n1.00\n4679.41\n2338.52\n0.00\n2897.75\n4825.00\n6290.00\n16071.00\n▅▇▃▁▁\n\n\nCDD30YR\n0\n1.00\n1309.88\n988.47\n0.00\n601.00\n1020.00\n1703.00\n4905.00\n▇▅▂▁▁\n\n\nTOTSQFT_EN\n0\n1.00\n1959.97\n1164.97\n200.00\n1100.00\n1700.00\n2510.00\n15000.00\n▇▁▁▁▁\n\n\nTOTHSQFT\n0\n1.00\n1744.46\n1106.33\n0.00\n1000.00\n1520.00\n2300.00\n15000.00\n▇▁▁▁▁\n\n\nTOTCSQFT\n0\n1.00\n1393.57\n1168.31\n0.00\n460.00\n1200.00\n2000.00\n14600.00\n▇▁▁▁▁\n\n\nWinterTempDay\n751\n0.96\n69.77\n3.62\n50.00\n68.00\n70.00\n72.00\n90.00\n▁▁▇▁▁\n\n\nWinterTempAway\n751\n0.96\n67.45\n5.02\n50.00\n65.00\n68.00\n70.00\n90.00\n▁▅▇▁▁\n\n\nWinterTempNight\n751\n0.96\n68.01\n4.66\n50.00\n65.00\n68.00\n70.00\n90.00\n▁▃▇▁▁\n\n\nSummerTempDay\n2325\n0.87\n72.01\n4.74\n50.00\n70.00\n72.00\n75.00\n90.00\n▁▁▇▃▁\n\n\nSummerTempAway\n2325\n0.87\n73.45\n5.68\n50.00\n70.00\n74.00\n78.00\n90.00\n▁▁▇▇▁\n\n\nSummerTempNight\n2325\n0.87\n71.22\n4.80\n50.00\n68.00\n72.00\n74.00\n90.00\n▁▂▇▃▁\n\n\nBTUEL\n0\n1.00\n37016.17\n24265.34\n143.32\n20205.76\n31890.03\n48297.98\n628155.47\n▇▁▁▁▁\n\n\nDOLLAREL\n0\n1.00\n1424.81\n862.08\n-889.48\n836.49\n1257.86\n1818.95\n15680.18\n▇▁▁▁▁\n\n\nBTUNG\n0\n1.00\n36960.50\n46538.32\n0.00\n0.00\n22011.97\n62714.20\n1134708.69\n▇▁▁▁▁\n\n\nDOLLARNG\n0\n1.00\n396.05\n483.41\n0.00\n0.00\n313.86\n644.93\n8154.98\n▇▁▁▁▁\n\n\nBTULP\n0\n1.00\n3916.95\n16741.93\n0.00\n0.00\n0.00\n0.00\n364215.39\n▇▁▁▁▁\n\n\nDOLLARLP\n0\n1.00\n80.89\n333.79\n0.00\n0.00\n0.00\n0.00\n6621.44\n▇▁▁▁▁\n\n\nBTUFO\n0\n1.00\n5108.60\n22698.98\n0.00\n0.00\n0.00\n0.00\n426268.50\n▇▁▁▁▁\n\n\nDOLLARFO\n0\n1.00\n88.43\n395.97\n0.00\n0.00\n0.00\n0.00\n7003.69\n▇▁▁▁▁\n\n\nBTUWOOD\n0\n1.00\n3596.49\n17766.37\n0.00\n0.00\n0.00\n0.00\n500000.00\n▇▁▁▁▁\n\n\nTOTALBTU\n0\n1.00\n83002.29\n53206.00\n1182.22\n45565.16\n74180.11\n108535.02\n1367548.13\n▇▁▁▁▁\n\n\nTOTALDOL\n0\n1.00\n1990.17\n1108.99\n-150.51\n1258.32\n1793.20\n2472.00\n20043.41\n▇▁▁▁▁\n\n\n\n\n\nNumeric vars\n\nrecs_2020 %&gt;% select(where(is.numeric)) %&gt;% colnames()\n\n [1] \"DOEID\"           \"HDD65\"           \"CDD65\"           \"HDD30YR\"        \n [5] \"CDD30YR\"         \"TOTSQFT_EN\"      \"TOTHSQFT\"        \"TOTCSQFT\"       \n [9] \"WinterTempDay\"   \"WinterTempAway\"  \"WinterTempNight\" \"SummerTempDay\"  \n[13] \"SummerTempAway\"  \"SummerTempNight\" \"NWEIGHT\"         \"NWEIGHT1\"       \n[17] \"NWEIGHT2\"        \"NWEIGHT3\"        \"NWEIGHT4\"        \"NWEIGHT5\"       \n[21] \"NWEIGHT6\"        \"NWEIGHT7\"        \"NWEIGHT8\"        \"NWEIGHT9\"       \n[25] \"NWEIGHT10\"       \"NWEIGHT11\"       \"NWEIGHT12\"       \"NWEIGHT13\"      \n[29] \"NWEIGHT14\"       \"NWEIGHT15\"       \"NWEIGHT16\"       \"NWEIGHT17\"      \n[33] \"NWEIGHT18\"       \"NWEIGHT19\"       \"NWEIGHT20\"       \"NWEIGHT21\"      \n[37] \"NWEIGHT22\"       \"NWEIGHT23\"       \"NWEIGHT24\"       \"NWEIGHT25\"      \n[41] \"NWEIGHT26\"       \"NWEIGHT27\"       \"NWEIGHT28\"       \"NWEIGHT29\"      \n[45] \"NWEIGHT30\"       \"NWEIGHT31\"       \"NWEIGHT32\"       \"NWEIGHT33\"      \n[49] \"NWEIGHT34\"       \"NWEIGHT35\"       \"NWEIGHT36\"       \"NWEIGHT37\"      \n[53] \"NWEIGHT38\"       \"NWEIGHT39\"       \"NWEIGHT40\"       \"NWEIGHT41\"      \n[57] \"NWEIGHT42\"       \"NWEIGHT43\"       \"NWEIGHT44\"       \"NWEIGHT45\"      \n[61] \"NWEIGHT46\"       \"NWEIGHT47\"       \"NWEIGHT48\"       \"NWEIGHT49\"      \n[65] \"NWEIGHT50\"       \"NWEIGHT51\"       \"NWEIGHT52\"       \"NWEIGHT53\"      \n[69] \"NWEIGHT54\"       \"NWEIGHT55\"       \"NWEIGHT56\"       \"NWEIGHT57\"      \n[73] \"NWEIGHT58\"       \"NWEIGHT59\"       \"NWEIGHT60\"       \"BTUEL\"          \n[77] \"DOLLAREL\"        \"BTUNG\"           \"DOLLARNG\"        \"BTULP\"          \n[81] \"DOLLARLP\"        \"BTUFO\"           \"DOLLARFO\"        \"BTUWOOD\"        \n[85] \"TOTALBTU\"        \"TOTALDOL\"       \n\n\nAmerican national election studies design object\n\ncps_state_in &lt;- getCensus(\n  name = \"cps/basic/mar\",\n  vintage = 2020,\n  region = \"state\",\n  vars = c(\n    \"HRMONTH\", \"HRYEAR4\",\n    \"PRTAGE\", \"PRCITSHP\", \"PWSSWGT\"\n  ),\n  key = Sys.getenv(\"CENSUS_KEY\")\n)\n\ncps_state &lt;- cps_state_in %&gt;%\n  as_tibble() %&gt;%\n  mutate(\n    across(\n      .cols = everything(),\n      .fns = as.numeric\n    )\n  )\n\n\ncps_narrow_resp &lt;- cps_state %&gt;%\n  filter(\n    PRTAGE &gt;= 18,\n    PRCITSHP %in% c(1:4)\n  )\n\nCalculate use population from the narrow data. Weights should add to total pop.\n\ntargetpop &lt;- cps_narrow_resp %&gt;% \n  pull(PWSSWGT) %&gt;%\n  sum()\nscales::comma(targetpop)\n\n[1] \"231,034,125\"\n\n\nWe can the weight the us election study appropriately\n\nanes_adjwgt &lt;- anes_2020 %&gt;%\n  mutate(\n    weight = V200010b / sum(V200010b) * targetpop\n  )\n\nWe then make it conform to a survey design\n\nanes_des &lt;- anes_adjwgt %&gt;%\n  as_survey_design(\n    weights = weight,\n    strata = V200010d,\n    ids = V200010c,\n    nest = TRUE\n  )\nanes_des\n\nStratified 1 - level Cluster Sampling design (with replacement)\nWith (101) clusters.\nCalled via srvyr\nSampling variables:\n  - ids: V200010c \n  - strata: V200010d \n  - weights: weight \nData variables: \n  - V200001 (dbl), CaseID (dbl), V200002 (hvn_lbll), InterviewMode (fct),\n    V200010b (dbl), Weight (dbl), V200010c (dbl), VarUnit (fct), V200010d\n    (dbl), Stratum (fct), V201006 (hvn_lbll), CampaignInterest (fct), V201023\n    (hvn_lbll), EarlyVote2020 (fct), V201024 (hvn_lbll), V201025x (hvn_lbll),\n    V201028 (hvn_lbll), V201029 (hvn_lbll), V201101 (hvn_lbll), V201102\n    (hvn_lbll), VotedPres2016 (fct), V201103 (hvn_lbll),\n    VotedPres2016_selection (fct), V201228 (hvn_lbll), V201229 (hvn_lbll),\n    V201230 (hvn_lbll), V201231x (hvn_lbll), PartyID (fct), V201233 (hvn_lbll),\n    TrustGovernment (fct), V201237 (hvn_lbll), TrustPeople (fct), V201507x\n    (hvn_lbll), Age (dbl), AgeGroup (fct), V201510 (hvn_lbll), Education (fct),\n    V201546 (hvn_lbll), V201547a (hvn_lbll), V201547b (hvn_lbll), V201547c\n    (hvn_lbll), V201547d (hvn_lbll), V201547e (hvn_lbll), V201547z (hvn_lbll),\n    V201549x (hvn_lbll), RaceEth (fct), V201600 (hvn_lbll), Gender (fct),\n    V201607 (hvn_lbll), V201610 (hvn_lbll), V201611 (hvn_lbll), V201613\n    (hvn_lbll), V201615 (hvn_lbll), V201616 (hvn_lbll), V201617x (hvn_lbll),\n    Income (fct), Income7 (fct), V202051 (hvn_lbll), V202066 (hvn_lbll),\n    V202072 (hvn_lbll), VotedPres2020 (fct), V202073 (hvn_lbll), V202109x\n    (hvn_lbll), V202110x (hvn_lbll), VotedPres2020_selection (fct), weight\n    (dbl)\n\n\nUsing replicated weights\n\nrecs_des &lt;- recs_2020 %&gt;%\n  as_survey_rep(\n    weights = NWEIGHT,\n    repweights = NWEIGHT1:NWEIGHT60,\n    type = \"JK1\",\n    scale = 59 / 60,\n    mse = TRUE\n  )\n\nrecs_des\n\nCall: Called via srvyr\nUnstratified cluster jacknife (JK1) with 60 replicates and MSE variances.\nSampling variables:\n  - repweights: `NWEIGHT1 + NWEIGHT2 + NWEIGHT3 + NWEIGHT4 + NWEIGHT5 +\n    NWEIGHT6 + NWEIGHT7 + NWEIGHT8 + NWEIGHT9 + NWEIGHT10 + NWEIGHT11 +\n    NWEIGHT12 + NWEIGHT13 + NWEIGHT14 + NWEIGHT15 + NWEIGHT16 + NWEIGHT17 +\n    NWEIGHT18 + NWEIGHT19 + NWEIGHT20 + NWEIGHT21 + NWEIGHT22 + NWEIGHT23 +\n    NWEIGHT24 + NWEIGHT25 + NWEIGHT26 + NWEIGHT27 + NWEIGHT28 + NWEIGHT29 +\n    NWEIGHT30 + NWEIGHT31 + NWEIGHT32 + NWEIGHT33 + NWEIGHT34 + NWEIGHT35 +\n    NWEIGHT36 + NWEIGHT37 + NWEIGHT38 + NWEIGHT39 + NWEIGHT40 + NWEIGHT41 +\n    NWEIGHT42 + NWEIGHT43 + NWEIGHT44 + NWEIGHT45 + NWEIGHT46 + NWEIGHT47 +\n    NWEIGHT48 + NWEIGHT49 + NWEIGHT50 + NWEIGHT51 + NWEIGHT52 + NWEIGHT53 +\n    NWEIGHT54 + NWEIGHT55 + NWEIGHT56 + NWEIGHT57 + NWEIGHT58 + NWEIGHT59 +\n    NWEIGHT60` \n  - weights: NWEIGHT \nData variables: \n  - DOEID (dbl), ClimateRegion_BA (fct), Urbanicity (fct), Region (fct),\n    REGIONC (chr), Division (fct), STATE_FIPS (chr), state_postal (fct),\n    state_name (fct), HDD65 (dbl), CDD65 (dbl), HDD30YR (dbl), CDD30YR (dbl),\n    HousingUnitType (fct), YearMade (ord), TOTSQFT_EN (dbl), TOTHSQFT (dbl),\n    TOTCSQFT (dbl), SpaceHeatingUsed (lgl), ACUsed (lgl), HeatingBehavior\n    (fct), WinterTempDay (dbl), WinterTempAway (dbl), WinterTempNight (dbl),\n    ACBehavior (fct), SummerTempDay (dbl), SummerTempAway (dbl),\n    SummerTempNight (dbl), NWEIGHT (dbl), NWEIGHT1 (dbl), NWEIGHT2 (dbl),\n    NWEIGHT3 (dbl), NWEIGHT4 (dbl), NWEIGHT5 (dbl), NWEIGHT6 (dbl), NWEIGHT7\n    (dbl), NWEIGHT8 (dbl), NWEIGHT9 (dbl), NWEIGHT10 (dbl), NWEIGHT11 (dbl),\n    NWEIGHT12 (dbl), NWEIGHT13 (dbl), NWEIGHT14 (dbl), NWEIGHT15 (dbl),\n    NWEIGHT16 (dbl), NWEIGHT17 (dbl), NWEIGHT18 (dbl), NWEIGHT19 (dbl),\n    NWEIGHT20 (dbl), NWEIGHT21 (dbl), NWEIGHT22 (dbl), NWEIGHT23 (dbl),\n    NWEIGHT24 (dbl), NWEIGHT25 (dbl), NWEIGHT26 (dbl), NWEIGHT27 (dbl),\n    NWEIGHT28 (dbl), NWEIGHT29 (dbl), NWEIGHT30 (dbl), NWEIGHT31 (dbl),\n    NWEIGHT32 (dbl), NWEIGHT33 (dbl), NWEIGHT34 (dbl), NWEIGHT35 (dbl),\n    NWEIGHT36 (dbl), NWEIGHT37 (dbl), NWEIGHT38 (dbl), NWEIGHT39 (dbl),\n    NWEIGHT40 (dbl), NWEIGHT41 (dbl), NWEIGHT42 (dbl), NWEIGHT43 (dbl),\n    NWEIGHT44 (dbl), NWEIGHT45 (dbl), NWEIGHT46 (dbl), NWEIGHT47 (dbl),\n    NWEIGHT48 (dbl), NWEIGHT49 (dbl), NWEIGHT50 (dbl), NWEIGHT51 (dbl),\n    NWEIGHT52 (dbl), NWEIGHT53 (dbl), NWEIGHT54 (dbl), NWEIGHT55 (dbl),\n    NWEIGHT56 (dbl), NWEIGHT57 (dbl), NWEIGHT58 (dbl), NWEIGHT59 (dbl),\n    NWEIGHT60 (dbl), BTUEL (dbl), DOLLAREL (dbl), BTUNG (dbl), DOLLARNG (dbl),\n    BTULP (dbl), DOLLARLP (dbl), BTUFO (dbl), DOLLARFO (dbl), BTUWOOD (dbl),\n    TOTALBTU (dbl), TOTALDOL (dbl)"
  },
  {
    "objectID": "posts/2026-01-15-survey-design/index.html#descriptive-statisitcs",
    "href": "posts/2026-01-15-survey-design/index.html#descriptive-statisitcs",
    "title": "Surveys: Design and Analysis",
    "section": "",
    "text": "Count observations with survey_count() or survey_tally()\nSum variables with survey_total()\nMeans and proportions: survey_mean(), survey_prop()\nQuantiles and medians: survey_quantile(), survey_median()\nCorrelations: survey_cor()\nRatios: survey_ratio()\nVariances and standard deviation: survey_var(), survey_sd()\n\n\n\n\nrecs_des %&gt;% survey_tally()\n\n# A tibble: 1 × 2\n           n  n_se\n       &lt;dbl&gt; &lt;dbl&gt;\n1 123529025. 0.148\n\n\nEstimated counts by subgroups\n\nrecs_des %&gt;% survey_count(Region, Division, name = \"N\")\n\n# A tibble: 10 × 4\n   Region    Division                   N         N_se\n   &lt;fct&gt;     &lt;fct&gt;                  &lt;dbl&gt;        &lt;dbl&gt;\n 1 Northeast New England         5876166  0.0000000137\n 2 Northeast Middle Atlantic    16043503  0.0000000487\n 3 Midwest   East North Central 18546912  0.000000437 \n 4 Midwest   West North Central  8495815  0.0000000177\n 5 South     South Atlantic     24843261  0.0000000418\n 6 South     East South Central  7380717. 0.114       \n 7 South     West South Central 14619094  0.000488    \n 8 West      Mountain North      4615844  0.119       \n 9 West      Mountain South      4602070  0.0000000492\n10 West      Pacific            18505643. 0.00000295  \n\n\nTo achieve same result by survey_tally(), you first group\n\nrecs_des %&gt;% group_by(Region, Division) %&gt;% survey_tally(name = \"N\")\n\n# A tibble: 10 × 4\n# Groups:   Region [4]\n   Region    Division                   N         N_se\n   &lt;fct&gt;     &lt;fct&gt;                  &lt;dbl&gt;        &lt;dbl&gt;\n 1 Northeast New England         5876166  0.0000000137\n 2 Northeast Middle Atlantic    16043503  0.0000000487\n 3 Midwest   East North Central 18546912  0.000000437 \n 4 Midwest   West North Central  8495815  0.0000000177\n 5 South     South Atlantic     24843261  0.0000000418\n 6 South     East South Central  7380717. 0.114       \n 7 South     West South Central 14619094  0.000488    \n 8 West      Mountain North      4615844  0.119       \n 9 West      Mountain South      4602070  0.0000000492\n10 West      Pacific            18505643. 0.00000295  \n\n\n\n\n\nTo get population count estimate, we leave argument x, empty.\n\nrecs_des %&gt;% \n  summarize(\n    tot = survey_total()\n  )\n\n# A tibble: 1 × 2\n         tot tot_se\n       &lt;dbl&gt;  &lt;dbl&gt;\n1 123529025.  0.148\n\n\nOverall summation of a continuous variable\n\nrecs_des %&gt;%\n  summarize(elec_bill = survey_total(DOLLAREL))\n\n# A tibble: 1 × 2\n      elec_bill elec_bill_se\n          &lt;dbl&gt;        &lt;dbl&gt;\n1 170473527909.   664893504.\n\n\nSummation by groups\n\nrecs_des %&gt;% group_by(Region) %&gt;%\n  summarize(\n  elec_bill = survey_total(DOLLAREL, vartype = \"ci\")\n  \n)\n\n# A tibble: 4 × 4\n  Region       elec_bill elec_bill_low elec_bill_upp\n  &lt;fct&gt;            &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n1 Northeast 29430369947.  28788987554.  30071752341.\n2 Midwest   34972544751.  34339576041.  35605513460.\n3 South     72496840204.  71534780902.  73458899506.\n4 West      33573773008.  32909111702.  34238434313.\n\n\nYou can supply .by argument inside summarize\n\nrecs_des  %&gt;%\n  summarize(\n  elec_bill = survey_total(DOLLAREL, vartype = \"ci\"),\n  .by = Region\n)\n\n# A tibble: 4 × 4\n  Region       elec_bill elec_bill_low elec_bill_upp\n  &lt;fct&gt;            &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n1 Northeast 29430369947.  28788987554.  30071752341.\n2 Midwest   34972544751.  34339576041.  35605513460.\n3 South     72496840204.  71534780902.  73458899506.\n4 West      33573773008.  32909111702.  34238434313.\n\n\n\n\n\n\nrecs_des %&gt;% group_by(Region, ACUsed) %&gt;%\n  summarise(\n  p = survey_prop()\n)\n\nWhen `proportion` is unspecified, `survey_prop()` now defaults to `proportion = TRUE`.\nℹ This should improve confidence interval coverage.\nThis message is displayed once per session.\n\n\n# A tibble: 8 × 4\n# Groups:   Region [4]\n  Region    ACUsed      p    p_se\n  &lt;fct&gt;     &lt;lgl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 Northeast FALSE  0.110  0.00590\n2 Northeast TRUE   0.890  0.00590\n3 Midwest   FALSE  0.0666 0.00508\n4 Midwest   TRUE   0.933  0.00508\n5 South     FALSE  0.0581 0.00278\n6 South     TRUE   0.942  0.00278\n7 West      FALSE  0.255  0.00759\n8 West      TRUE   0.745  0.00759\n\n\nJoint proportions using interact\n\nrecs_des %&gt;% \n  group_by(\n    interact(Region, ACUsed)\n  ) %&gt;% \n  summarize(\n    p = survey_prop() \n  ) %&gt;% mutate(\n    p = scales::percent(p)\n  )\n\n# A tibble: 8 × 4\n  Region    ACUsed p         p_se\n  &lt;fct&gt;     &lt;lgl&gt;  &lt;chr&gt;    &lt;dbl&gt;\n1 Northeast FALSE  1.96%  0.00105\n2 Northeast TRUE   15.79% 0.00105\n3 Midwest   FALSE  1.46%  0.00111\n4 Midwest   TRUE   20.43% 0.00111\n5 South     FALSE  2.20%  0.00106\n6 South     TRUE   35.72% 0.00106\n7 West      FALSE  5.73%  0.00170\n8 West      TRUE   16.72% 0.00170\n\n\nOverall mean\n\nrecs_des %&gt;% summarize(\n  elec_bill = survey_mean(DOLLAREL, vartype = c(\"se\", \"ci\"))\n)\n\n# A tibble: 1 × 4\n  elec_bill elec_bill_se elec_bill_low elec_bill_upp\n      &lt;dbl&gt;        &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n1     1380.         5.38         1369.         1391.\n\n\n\n\n\nOverall quantiles\n\nrecs_des %&gt;%\n  summarize(\n    elec_bill = survey_quantile(DOLLAREL, quantiles = c(.25, .5, .75))\n  )\n\n# A tibble: 1 × 6\n  elec_bill_q25 elec_bill_q50 elec_bill_q75 elec_bill_q25_se elec_bill_q50_se\n          &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n1          795.         1215.         1770.             5.69             6.33\n# ℹ 1 more variable: elec_bill_q75_se &lt;dbl&gt;\n\n\nOverall median\n\nrecs_des %&gt;%\n  summarize(\n    elec_bill = survey_median(DOLLAREL)\n  )\n\n# A tibble: 1 × 2\n  elec_bill elec_bill_se\n      &lt;dbl&gt;        &lt;dbl&gt;\n1     1215.         6.33\n\n\nCorrelations\n\nrecs_des %&gt;%\n  summarize(SQFT_Elec_Corr = survey_corr(TOTSQFT_EN, BTUEL))\n\nWarning: There was 1 warning in `dplyr::summarise()`.\nℹ In argument: `SQFT_Elec_Corr = survey_corr(TOTSQFT_EN, BTUEL)`.\nCaused by warning in `sweep()`:\n! length(STATS) or dim(STATS) do not match dim(x)[MARGIN]\n\n\n# A tibble: 1 × 2\n  SQFT_Elec_Corr SQFT_Elec_Corr_se\n           &lt;dbl&gt;             &lt;dbl&gt;\n1          0.417           0.00689\n\n\n\n\nDesign effect measures the precision of an estimate under a particular sampling design relative to a simple random sampling design. If &lt;1, the design is statistically more efficient than SRS. It is used in the calculation of effective sample size - the sample size needed if we were to employ SRS\n\\[n_{eff} = \\frac{n}{D_{eff}}\\]\n\nrecs_des %&gt;%\n  summarize(\n    across(\n          c(BTUEL, BTUNG, BTULP, BTUFO, BTUWOOD),\n          ~ survey_mean(.x, deff = TRUE, vartype = NULL)\n    )\n  ) %&gt;% select(ends_with(\"deff\"))\n\n# A tibble: 1 × 5\n  BTUEL_deff BTUNG_deff BTULP_deff BTUFO_deff BTUWOOD_deff\n       &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;\n1      0.597      0.938       1.21      0.720         1.10\n\n\n\n\n\n\nrecs_des %&gt;%\n  cascade(DOLLAREL_mn = survey_mean(DOLLAREL))\n\n# A tibble: 1 × 2\n  DOLLAREL_mn DOLLAREL_mn_se\n        &lt;dbl&gt;          &lt;dbl&gt;\n1       1380.           5.38\n\n\nGroup\n\nrecs_des %&gt;%\n  group_by(Region) %&gt;%\n  cascade(DOLLAREL_mn = survey_mean(DOLLAREL), .fill = \"National\")\n\n# A tibble: 5 × 3\n  Region    DOLLAREL_mn DOLLAREL_mn_se\n  &lt;fct&gt;           &lt;dbl&gt;          &lt;dbl&gt;\n1 Northeast       1343.          14.6 \n2 Midwest         1293.          11.7 \n3 South           1548.          10.3 \n4 West            1211.          12.0 \n5 National        1380.           5.38\n\n\nOr through a long dplyr pipeline\n\nrecs_des %&gt;% \n  summarize(\n    cons_mean = survey_mean(DOLLAREL),\n    .by = Region\n  ) %&gt;% bind_rows(\n    ., recs_des %&gt;%\n      summarize(\n        cons_mean = survey_mean(DOLLAREL)\n      ) %&gt;% mutate(\n        ., Region = \"National\"\n      ) %&gt;% select(Region, everything())\n  )\n\n# A tibble: 5 × 3\n  Region    cons_mean cons_mean_se\n  &lt;chr&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1 Northeast     1343.        14.6 \n2 Midwest       1293.        11.7 \n3 South         1548.        10.3 \n4 West          1211.        12.0 \n5 National      1380.         5.38\n\n\n\n\n\n\nrecs_des %&gt;% \n  summarize(\n    across(\n      starts_with(\"BTU\"),\n      list(\n        Total = ~ survey_total(.x, vartype = \"cv\"),\n        Mean = ~ survey_mean(.x, vartype = \"cv\")\n      ),\n      .unpack = \"{outer}.{inner}\"\n    )\n  )\n\n# A tibble: 1 × 20\n  BTUEL_Total.coef BTUEL_Total._cv BTUEL_Mean.coef BTUEL_Mean._cv\n             &lt;dbl&gt;           &lt;dbl&gt;           &lt;dbl&gt;          &lt;dbl&gt;\n1    4453284510065         0.00377          36051.        0.00377\n# ℹ 16 more variables: BTUNG_Total.coef &lt;dbl&gt;, BTUNG_Total._cv &lt;dbl&gt;,\n#   BTUNG_Mean.coef &lt;dbl&gt;, BTUNG_Mean._cv &lt;dbl&gt;, BTULP_Total.coef &lt;dbl&gt;,\n#   BTULP_Total._cv &lt;dbl&gt;, BTULP_Mean.coef &lt;dbl&gt;, BTULP_Mean._cv &lt;dbl&gt;,\n#   BTUFO_Total.coef &lt;dbl&gt;, BTUFO_Total._cv &lt;dbl&gt;, BTUFO_Mean.coef &lt;dbl&gt;,\n#   BTUFO_Mean._cv &lt;dbl&gt;, BTUWOOD_Total.coef &lt;dbl&gt;, BTUWOOD_Total._cv &lt;dbl&gt;,\n#   BTUWOOD_Mean.coef &lt;dbl&gt;, BTUWOOD_Mean._cv &lt;dbl&gt;\n\n\n\n\n\n\ncalc &lt;- function(df, var) {\n  df %&gt;%\n    drop_na(!!sym(var)) %&gt;%\n    group_by(!!sym(var)) %&gt;%\n    summarize(p = survey_prop()) %&gt;%\n    mutate(Variable = var) %&gt;%\n    rename(Answer := !!sym(var)) %&gt;%\n    mutate(p = scales::percent(p)) %&gt;%\n    select(Variable, everything())\n}\n\nv &lt;- c(\"TrustGovernment\", \"TrustPeople\")\nmap2(\n  .x = list(anes_des),\n  .y = v,\n  .f = ~calc(df = .x, var = .y)\n) %&gt;% bind_rows()\n\n# A tibble: 10 × 4\n   Variable        Answer              p        p_se\n   &lt;chr&gt;           &lt;fct&gt;               &lt;chr&gt;   &lt;dbl&gt;\n 1 TrustGovernment Always              1.6%  0.00204\n 2 TrustGovernment Most of the time    13.2% 0.00553\n 3 TrustGovernment About half the time 30.9% 0.00829\n 4 TrustGovernment Some of the time    43.4% 0.00855\n 5 TrustGovernment Never               11.0% 0.00566\n 6 TrustPeople     Always              0.8%  0.00164\n 7 TrustPeople     Most of the time    41.4% 0.00857\n 8 TrustPeople     About half the time 28.2% 0.00776\n 9 TrustPeople     Some of the time    24.5% 0.00670\n10 TrustPeople     Never               5.0%  0.00422"
  },
  {
    "objectID": "posts/2026-01-15-survey-design/index.html#statistical-testing",
    "href": "posts/2026-01-15-survey-design/index.html#statistical-testing",
    "title": "Surveys: Design and Analysis",
    "section": "",
    "text": "Comparison of means and proportions: svyttest()\ngoodness-of-fit test: svygofchisq()\nTest of independence: svychisq()\nTest of homogeneity: svychisq()\n\n\n\n\none-sample t-test: compare to 0 - var ~ 0 or to a different value var - val = 0\ntwo-sample t-test:\n\nunpaired: two level grouping variable - var ~ groupvar or three level grouping variable - var ~ groupvar == level\npaired: var_1 - var_2 = 0\n\n\nExample: one-sample t-test for mean\n\nrecs_des %&gt;%\n  svyttest(\n    formula = SummerTempNight - 68 ~ 0,\n    design =.,\n    na.rm = TRUE\n  )\n\n\n    Design-based one-sample t-test\n\ndata:  SummerTempNight - 68 ~ 0\nt = 84.788, df = 58, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 3.287816 3.446810\nsample estimates:\n    mean \n3.367313 \n\n\nget mean summer temps at night\n\nrecs_des %&gt;% \n  summarize(m = survey_mean(SummerTempNight, na.rm = TRUE))\n\n# A tibble: 1 × 2\n      m   m_se\n  &lt;dbl&gt;  &lt;dbl&gt;\n1  71.4 0.0397\n\n\nExample: one-sample t-test for proportion\n\nrecs_des %&gt;% summarize(\n  p = survey_prop(),\n  .by = ACUsed\n)\n\n# A tibble: 2 × 3\n  ACUsed     p    p_se\n  &lt;lgl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 FALSE  0.113 0.00306\n2 TRUE   0.887 0.00306\n\n\n\nrecs_des %&gt;%\n  svyttest(\n    formula = (ACUsed == TRUE) - 0.90 ~ 0,\n    design = .,\n    na.rm = TRUE\n  ) %&gt;% tidy() %&gt;%\n  mutate(p.value = pretty_p_value(p.value)) %&gt;%\n  gt() %&gt;%\n  fmt_number()\n\n\n\nTable 1: One-sample t-test for estimate of US housholds use of A/C in their homes differing from 90%\n\n\n\n\n\n\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n\n\n−0.01\n−4.40\n&lt;0.0001\n58.00\n−0.02\n−0.01\nDesign-based one-sample t-test\ntwo.sided\n\n\n\n\n\n\n\n\n\n\nExample: paired two-sample t-test\n\nrecs_des %&gt;%\n  svyttest(\n    formula = SummerTempNight - WinterTempNight ~ 0,\n    design = .,\n    na.rm = TRUE\n  ) %&gt;% tidy() %&gt;%\n  mutate(\n    p.value = pretty_p_value(p.value)\n  ) %&gt;%\n  gt() %&gt;%\n  fmt_number()\n\n\n\nTable 2: Paired two-sample t-test\n\n\n\n\n\n\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n\n\n2.85\n50.83\n&lt;0.0001\n58.00\n2.74\n2.96\nDesign-based one-sample t-test\ntwo.sided\n\n\n\n\n\n\n\n\n\n\n\n\n\nSee https://tidy-survey-r.github.io/tidy-survey-book/c06-statistical-testing.html\nExample: goodness of fit test\n\nanes_des_educ &lt;- anes_des %&gt;%\n  mutate(\n    Education2 = fct_collapse(Education,\n      \"Bachelor or Higher\" = c(\"Bachelor's\", \"Graduate\")\n    )\n  )\n\nanes_des_educ %&gt;%\n  drop_na(Education2) %&gt;%\n  group_by(Education2) %&gt;% \n  summarize(p = survey_mean())\n\n# A tibble: 4 × 3\n  Education2              p    p_se\n  &lt;fct&gt;               &lt;dbl&gt;   &lt;dbl&gt;\n1 Less than HS       0.0805 0.00568\n2 High school        0.277  0.0102 \n3 Post HS            0.290  0.00713\n4 Bachelor or Higher 0.352  0.00732\n\n\n\nanes_des_educ %&gt;%\n  svygofchisq(\n    formula = ~Education2,\n    design = .,\n    p = c(0.11, 0.27, 0.28, 0.35),\n    na.rm = TRUE\n  ) %&gt;% tidy()\n\nMultiple parameters; naming those columns scale and df.\n\n\n# A tibble: 1 × 5\n    scale    df statistic  p.value method                                       \n    &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;                                        \n1 113213.  2.26  1922280. 0.000292 Design-based chi-squared test for given prob…\n\n\n\nex1_table &lt;- anes_des_educ %&gt;%\n  drop_na(Education2) %&gt;%\n  group_by(Education2) %&gt;%\n  summarize(Observed = survey_mean(vartype = \"ci\")) %&gt;%\n  rename(Education = Education2) %&gt;%\n  mutate(Expected = c(0.11, 0.27, 0.29, 0.33)) %&gt;%\n  select(Education, Expected, everything())\n\nex1_table\n\n# A tibble: 4 × 5\n  Education          Expected Observed Observed_low Observed_upp\n  &lt;fct&gt;                 &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1 Less than HS           0.11   0.0805       0.0691       0.0919\n2 High school            0.27   0.277        0.257        0.298 \n3 Post HS                0.29   0.290        0.276        0.305 \n4 Bachelor or Higher     0.33   0.352        0.337        0.367 \n\n\n\nex1_table %&gt;%\n  pivot_longer(\n    cols = c(\"Expected\", \"Observed\"),\n    names_to = \"Names\",\n    values_to = \"Proportion\"\n  ) %&gt;%\n  mutate(\n    Observed_low = if_else(Names == \"Observed\", Observed_low, NA_real_),\n    Observed_upp = if_else(Names == \"Observed\", Observed_upp, NA_real_),\n    Names = if_else(Names == \"Observed\",\n      \"ANES (observed)\", \"ACS (expected)\"\n    )\n  ) %&gt;%\n  ggplot(aes(x = Education, y = Proportion, color = Names)) +\n  geom_point(alpha = 0.75, size = 2) +\n  geom_errorbar(aes(ymin = Observed_low, ymax = Observed_upp),\n    width = 0.25\n  ) +\n  theme_bw() +\n  # scale_color_manual(name = \"Type\", values = book_colors[c(4, 1)]) +\n  theme(legend.position = \"bottom\", legend.title = element_blank())"
  },
  {
    "objectID": "posts/2026-01-15-survey-design/index.html#statified-sampling",
    "href": "posts/2026-01-15-survey-design/index.html#statified-sampling",
    "title": "Surveys: Design and Analysis",
    "section": "Statified Sampling",
    "text": "Statified Sampling\nPopulation can be grouped into homogeneous units (strata). Random samples are then drawn from each of the units.\nIf \\(\\hat{y}_h\\) is the sample mean for stratum \\(h\\), \\(N_h\\) the population size of stratum \\(h\\), and \\(H\\) the total number of strata, then estimates of the mean\n\\[\\hat{y} = \\frac{1}{N}\\sum_{h=1}^H N_h \\hat{y}_h\\] and\n\\[se(\\hat{y}) = \\sqrt{\\frac{1}{N^2}\\sum_{h=1}^H N^2_h \\frac{s^2_h}{n_h}(1 - \\frac{n_h}{N_h})}\\] where\n\\[s^2_h = \\frac{1}{n_h - 1} \\sum_{i=1}^{n_h}(y_{i,h} - \\hat{y_h})^2\\] and proportion\n\\[\\hat{p} = \\frac{1}{N} \\sum_{h=1}^H N_h \\hat{p}_h\\]\n\\[se(\\hat{p}) = \\frac{1}{N}\\sqrt{\\sum_{h=1}^H N^2_h\\frac{\\hat{p_h}(1- \\hat{p_h})}{n_h - 1}(1 - \\frac{n_h}{N_h})}\\]"
  },
  {
    "objectID": "posts/2026-01-15-survey-design/index.html#clustered-sampling",
    "href": "posts/2026-01-15-survey-design/index.html#clustered-sampling",
    "title": "Surveys: Design and Analysis",
    "section": "Clustered sampling",
    "text": "Clustered sampling\nApplies if a population can be divided into mutually exclusive subgroups (clusters or primary sampling units (PSU)).A random selection of PSUs is sampled followed by another level of sampling within the chosen clusters. Suppose \\(a\\) clusters are sampled from a population of A clusters via SRS. Within each sampled cluster \\(i\\) there are \\(B_i\\) units in the population of which \\(b_i\\) units are sampled using SRS. if \\(\\hat{y}_i\\) is the \\(i\\)th cluster mean, then the population mean is estimated by\n\\[\\hat{y} = \\frac{\\sum_{i=1}^aB_i\\hat{y_i}}{\\sum_{i=1}^a B_i}\\]\n\\[se(\\hat{y}) = \\frac{1}{\\hat{N}}\\sqrt{(1 - \\frac{a}{A})\\frac{s^2_a}{a} + \\frac{A}{a}\\sum_{i=1}^a(1 - \\frac{b_i}{B_i}) \\frac{s_i^2}{b_i}}\\] where \\(\\hat{N}\\) is the estimated population size, \\(s^2_a\\) is the between-cluster variance, and \\(s^2_i\\) is the within-cluster variance.\nBetween-cluster variance \\(s^2_a\\) is:\n\\[s_a^2 = \\frac{1}{a - 1} \\sum_{i = 1}^a (\\hat{y_i} - \\frac{\\sum_{i=1}^a \\hat{y_i}}{a})^2\\] where \\(\\frac{\\sum_{i=1}^a \\hat{y_i}}{a} = \\bar{y}\\) is the grand mean.\nThe within-cluster variance (\\(s^2_i\\)) is estimated through\n\\[s^2_i = \\frac{1}{a(b_i - 1)} \\sum_{j = 1}^{b_i}(y_{ij} - \\hat{y_i})\\] where \\(y_{ij}\\) is the outcome for sampled unit \\(j\\) from cluster \\(i\\)"
  }
]