[
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "resources",
    "section": "",
    "text": "Rodrigues’ Reproducible Analytical Pipelines in R\nSamantha’s quarto website tutorial\nSamantha’s quarto blog posts tutorial\nMastering Shiny, by Hadley Wickham\nHappy Git and GitHub for the useR, by Jenny Bryan\nR for Data Science, by Hadley Wickham\nHitchhiker’s Guide to Python, by Kenneth Reitz & Tanya Schlusser\nData wrangling essentials: comparisons in JavaScript, Python, SQL, R, and Excel, by Allison Horst & Paul Buffa\nW3Schools, particularly for their HTML & CSS tutorials\nJayde’s parameterized reports with quarto\nMine’s quarto manuscripts"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Portfolio",
    "section": "",
    "text": "R package development\nTaking on package development inculcates in one best practices in programming: using a secluded development environment made possible by {renv} package and Rstudio’s project initialization functionality, documenting code, shared data and publishing user guides to get people started with the software. Package development workflow included {fussen}, {roxygen2}, and {devtools}\nThe packaged is named pbwrangler, it’s goal to document functions used in reading, processing and writing field experimental data in potato breeding.\n\n\nShiny web app\n{shiny} is undoubtedly a go to tool for building a web app that runs in production or just for presenting a proof of concept. I have been reading about using {golem} in packaging and modularizing shiny applications for better project level management (being a package you able to document, check, and test every function), and other reproducubility benefits (for instance docker images). I was able to modularize parts of a COVID-19 dashboard initially developed by a team from LSHTM, into a golem-framework application called COVID19Dash. It was challenging navigating a large codebase and breaking it into parts, troubleshooting instances of some lines of code not running in an isolated environment, the kind we work with during package package development and docker image building/serving.\nPrior to this, I had also developed a small app as proof that I understand the underlying framework to be able to build an app that can be used in production.\n\n\nReproducible analytical pipelines in R\nThis was a ‘do it yourself too’ as I was reading an online version of Rodrigues’ reproducible piplines text . It reinforced my understanding of package development powered by {fusen}, reproducibility of package versions using {renv}, reproducible pipelines with {targets}, building and sharing docker containers in dockerhub and github, and continuous integration/development (CI/CD) using github actions. A branch with CI/CD running a docker container can be found here.\n\n\nEnd-to-end Machine Learning (MLflow + Docker + Google cloud)\nThis is an account of my learning journey aided by this tutorial to grasp the nitty-gritties of building, logging, saving and serving machine learning models. The code available at this repo and a write-up is here.\n\n\nWeb scraping: getting data from the internet\nI set out to understand how to scrape data using Python. I explored beautifulsoup, requests, scrapingBee API, and scrapy to scrape data from Google news and a product listing page. I wrote a piece about it too. I have also scheduled this to run daily using github actions.\n\n\nProject-based Data Engineering\nThis is an accompanying “do-it-yourself” as I go through data engineering material from DuckDb. It is my initiative to learn how to to build data pipelines with Python, SQL & DuckDB. The first part is about ingestion, involving reading public data from Google Cloud, writing it locally as .csv or to MotherDuck database. Github for project materials and a post\n\n\nELT pipeline with dbt, snowflake, and dagster\nCreated an ELT pipeline that uses a dbt project to read and write tables to a snowflake warehouse database, then orchestrated the workflow with dagster. Github repo."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "Hello!\nYou’ve made it to my landing page! I am a student enrolled in a Master of Science degree in Statistics and data science, specializing in biostatistics. I use R, SAS, and Python for statistical analysis and visualizations. My interest is statistical computing applied to spatial statistics, infectious diseases modelling and Bayesian data analysis.\n\neducation\n\n\nMS in Biostatistics, 2020 - Ongoing|Hasselt University\n\n\n\nBS in Applied Statistics, 2015|Maseno University\n\n\n\n\n\nexperience\n\n\nBiometrician, 2024-present|International Potato Center (CIP)\n\n\nResearch Assistant, 2022-2023|Karolinska Institutet\n\n\n\nData Manager, 2022-2022|Kenya Medical Research Institute\n\n\n\nData Manager, 2018-2020|KEMRI - Wellcome Trust Research Programme"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "posts",
    "section": "",
    "text": "Econometrics: R introduction\n\n\n\neconometrics\n\ninstrumental variables\n\nquasi-experiments\n\nRCTs\n\nR\n\n\n\n\n\n\n\n\n\nJan 21, 2026\n\n\nBasil Okola\n\n\n\n\n\n\n\n\n\n\n\n\nSurveys: Design and Analysis\n\n\n\nbiostatistics\n\nsurvey design\n\n\n\n\n\n\n\n\n\nJan 15, 2026\n\n\nBasil Okola\n\n\n\n\n\n\n\n\n\n\n\n\nAdaptive Methods for Clinical Trials\n\n\n\nbiostatistics\n\nadaptive designs\n\ninterim analysis\n\nsimulations\n\n\n\nA simulation approach to adaptive clinical trial designs\n\n\n\n\n\nJan 12, 2026\n\n\nBasil Okola\n\n\n\n\n\n\n\n\n\n\n\n\nInteractive maps (R)\n\n\n\nggplot2\n\nplotly\n\necharts4r\n\nggiraph\n\nwidgetframe\n\n\n\n\n\n\n\n\n\nOct 30, 2025\n\n\nBasil Okola\n\n\n\n\n\n\n\n\n\n\n\n\nPublication ready visualization in R\n\n\n\nggplot2\n\ncowplot\n\n\n\n\n\n\n\n\n\nOct 29, 2025\n\n\nBasil Okola\n\n\n\n\n\n\n\n\n\n\n\n\nEnd-to-end Data Engineering with Python DuckDB, Google Cloud, dbt\n\n\n\nQuarto\n\nPython\n\nbeautifulsoup\n\nscrapy\n\nscrapingBee\n\n\n\nPart 1: Data Ingestion\n\n\n\n\n\nAug 8, 2025\n\n\nBasil Okola\n\n\n\n\n\n\n\n\n\n\n\n\nWeb Scraping with Python\n\n\n\nQuarto\n\nPython\n\nbeautifulsoup\n\nscrapy\n\nscrapingBee\n\n\n\nScraping data from the web using {beautifulsoup4}, {requests}, {ScrapingBee}, and {Scrapy}\n\n\n\n\n\nAug 3, 2025\n\n\nBasil Okola\n\n\n\n\n\n\n\n\n\n\n\n\nEnd-to-end Machine Learning (MLflow + Docker + Google Cloud)\n\n\n\nQuarto\n\nPython\n\nMLflow\n\n\n\nMachine learning Workflow using MLflow locally and pushing image to google cloud\n\n\n\n\n\nAug 2, 2025\n\n\nBasil Okola\n\n\n\n\n\n\n\n\n\n\n\n\nR package development guide\n\n\n\nQuarto\n\nR\n\npackages\n\n\n\nWriting R packages using devtools, roxygen2 and usethis packages\n\n\n\n\n\nDec 16, 2023\n\n\nBasil Okola\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-12-16-R-package dev/index.html",
    "href": "posts/2023-12-16-R-package dev/index.html",
    "title": "R package development guide",
    "section": "",
    "text": "These are quick step-by-step guides I wrote when going through Andy’s fundamentas of package development notes.\n\nInitialize package dev files with devtools::create_package() which creates mandatory file for the package\nEnable git tracking:\n\nconfigure: usethis::use_git_configure()\ncommit: usethis::use_git()\n\nFill in sections of the DESCRIPTION file. {roxygen2} is used to actualize this.\nEdit.Rprofile() to set options you’ll use in the documentation such as author details. This uses usethis::edit_r_profile()\nCreate a separate R file for each function you’ll be including in the package. Use use_r(\"/path-to-file/R/function-name.R\")\nLoad the functions with devtools::load_all()\nCheck the loaded files with check() which runs R CMD checks to catch errors/warnings/notes that need addressing.\nAdd files you don’t wish to include in the package build-in .Rbuildignore file.\nEnable roxygen2 to be used for package documentation: project options -&gt; Build Tools -&gt; check to generate documentation with roxygen or better devtools::document() which generates NAMESPACE automatically\nAutomate external function imports with usethis::use_import_from(): example usethis::use_import_from(“utils”, “install.packages”)\nDocument functions: put the cursor inside the R function definition and ctrl+shift+alt+R to insert the roxygen skeleton. The workflow here is after documenting -&gt; load_all() -&gt; document() -&gt; check()\nData files go to /data dir. It should be of .rda format\nExternal (non .rda format) data go to /inst/extdata/. Document them in the R file e.g. data.R and store it in /R. load_all() then document()\nCall use_package_doc() to add a dummy .R file that will prompt roxygen to generate basic package-level documentation. I noticed doing this erased imports in {package-name}-package.R file. Add recommended imports (see 10) and check()\nInstall your package with devtools::install()\nAttach your package as with other packages by calling library()\nTesting: Using the edit code -&gt; load_all() -&gt; experiment iteration can be unsustainable if you come back to your code months after development. You should write formal tests supported by {testthat} package.\nSet up formal testing of your package with usethis::use_testthat(). Creates a folder /tests. Don’t edit tests/testhat.R\nCall usethis::use_test() e.g., use_test(\"install_load_packages.R\") to edit tests for functions living in a particular R file in R/.\ntest() or ctr + shift + T runs all tests in your test/ directory. The workflow updates to load_all() -&gt; test() -&gt; document() -&gt; check(). Tests should be small and run quickly.\nDependencies, add imports in DESCRIPTION with use_package().\nAdd README with use_readme_rmd()\nRender readme.rmd with build_readme()\nUse continuous integration with use_github_action() then build_readme() again 25 Build a website for your package with use_pkgdown_github_pages() then document().\n\n\n\n\nCitationBibTeX citation:@online{okola2023,\n  author = {Okola, Basil},\n  title = {R Package Development Guide},\n  date = {2023-12-16},\n  url = {https://bokola.github.io/posts/2023-12-16-R-package dev/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nOkola, Basil. 2023. “R Package Development Guide.” December\n16, 2023. https://bokola.github.io/posts/2023-12-16-R-package\ndev/."
  },
  {
    "objectID": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html",
    "href": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html",
    "title": "End-to-end Machine Learning (MLflow + Docker + Google Cloud)",
    "section": "",
    "text": "BibTeX citation:@online{okola2025,\n  author = {Okola, Basil},\n  title = {End-to-End {Machine} {Learning} {(MLflow} + {Docker} +\n    {Google} {Cloud)}},\n  date = {2025-08-02},\n  url = {https://bokola.github.io/posts/2025-08-02-End-to-end-ML-with-MLflow/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nOkola, Basil. 2025. “End-to-End Machine Learning (MLflow + Docker\n+ Google Cloud).” August 2, 2025. https://bokola.github.io/posts/2025-08-02-End-to-end-ML-with-MLflow/.\nThis is an account of my learning journey aided by this tutorial to grasp the nitty-gritties of buliding, logging, saving and serving machine learning models. First is a description of the development environment used."
  },
  {
    "objectID": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#development-environment",
    "href": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#development-environment",
    "title": "End-to-end Machine Learning (MLflow + Docker + Google Cloud)",
    "section": "Development Environment",
    "text": "Development Environment\nI am running Debian 24.04 LTS, and Pycharm IDE calling Python 3.12 within a .venv virtual environment. Since the model used is a Tensorflow neural network, I had to follow cuda documentation in setting up necessary drives. You also need to start mlflow ui local server by running mlflow ui --port 5000 in the terminal, install dependenices pip install mlflow[extras] hyperopt tensorflow scikit-learn pandas numpy, and set environment variable export MLFLOW_TRACKING_URI=http://localhost:5000."
  },
  {
    "objectID": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-1-data-preparation",
    "href": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-1-data-preparation",
    "title": "End-to-end Machine Learning (MLflow + Docker + Google Cloud)",
    "section": "Step 1 : Data Preparation",
    "text": "Step 1 : Data Preparation\nThe tutorial uses wine quality classification data.\n#prepare data\n\nimport os\nfrom dotenv import load_dotenv\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport tensorflow as tf\nfrom tensorflow import keras\nimport mlflow\nfrom mlflow.models import infer_signature\nfrom hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n\n#test prediction\nimport requests\nimport json\n\n#environment variables\nload_dotenv(\".env\")\n\nMLFLOW_TRACKING_URI=os.getenv(\"MLFLOW_TRACKING_URI\")\nXLA_FLAGS=os.getenv('XLA_FLAGS')\n\n#load data\ndata = pd.read_csv(\n    \"https://raw.githubusercontent.com/mlflow/mlflow/master/tests/datasets/winequality-white.csv\",\n    sep=\";\",\n)\n\ntrain, test = train_test_split(data, test_size=0.2, random_state=12)\ntrain_x = train.drop([\"quality\"], axis=1).values\ntrain_y = train[[\"quality\"]].values.ravel()\ntest_x = test.drop([\"quality\"], axis=1).values\ntest_y = test[[\"quality\"]].values.ravel()\n\n#further split training data for validation\ntrain_x, valid_x, train_y, valid_y = train_test_split(train_x, train_y, test_size=0.2, random_state=12)\n\n#Create model signature for deployment\nsignature = infer_signature(train_x, train_y)"
  },
  {
    "objectID": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#define-model-architecture",
    "href": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#define-model-architecture",
    "title": "End-to-end Machine Learning (MLflow + Docker + Google Cloud)",
    "section": "Define Model architecture",
    "text": "Define Model architecture\ndef create_and_train_model(learning_rate, momentum, epochs=10):\n    \"\"\"\n    Create and train a neural network with specified hyperparameters.\n\n    Returns:\n        dict: Training results including model and metrics\n    \"\"\"\n    # Normalize input features for better training stability\n    mean = np.mean(train_x, axis=0)\n    var = np.var(train_x, axis=0)\n\n    # Define model architecture\n    model = keras.Sequential(\n        [\n            keras.Input([train_x.shape[1]]),\n            keras.layers.Normalization(mean=mean, variance=var),\n            keras.layers.Dense(64, activation=\"relu\"),\n            keras.layers.Dropout(0.2),  # Add regularization\n            keras.layers.Dense(32, activation=\"relu\"),\n            keras.layers.Dense(1),\n        ]\n    )\n\n    # Compile with specified hyperparameters\n    model.compile(\n        optimizer=keras.optimizers.SGD(learning_rate=learning_rate, momentum=momentum),\n        loss=\"mean_squared_error\",\n        metrics=[keras.metrics.RootMeanSquaredError()],\n    )\n\n    # Train with early stopping for efficiency\n    early_stopping = keras.callbacks.EarlyStopping(\n        patience=3, restore_best_weights=True\n    )\n\n    # Train the model\n    history = model.fit(\n        train_x,\n        train_y,\n        validation_data=(valid_x, valid_y),\n        epochs=epochs,\n        batch_size=64,\n        callbacks=[early_stopping],\n        verbose=0,  # Reduce output for cleaner logs\n    )\n\n    # Evaluate on validation set\n    val_loss, val_rmse = model.evaluate(valid_x, valid_y, verbose=0)\n\n    return {\n        \"model\": model,\n        \"val_rmse\": val_rmse,\n        \"val_loss\": val_loss,\n        \"history\": history,\n        \"epochs_trained\": len(history.history[\"loss\"]),\n    }\n    ```\n\n## Step 3: Set up parameter optimization\n\ndef objective(params): ““” Objective function for hyperparameter optimization. This function will be called by Hyperopt for each trial. ““” with mlflow.start_run(nested=True): # Log hyperparameters being tested mlflow.log_params( { “learning_rate”: params[“learning_rate”], “momentum”: params[“momentum”], “optimizer”: “SGD”, “architecture”: “64-32-1”, } )\n    # Train model with current hyperparameters\n    result = create_and_train_model(\n        learning_rate=params[\"learning_rate\"],\n        momentum=params[\"momentum\"],\n        epochs=15,\n    )\n\n    # Log training results\n    mlflow.log_metrics(\n        {\n            \"val_rmse\": result[\"val_rmse\"],\n            \"val_loss\": result[\"val_loss\"],\n            \"epochs_trained\": result[\"epochs_trained\"],\n        }\n    )\n\n    # Log the trained model\n    mlflow.tensorflow.log_model(result[\"model\"], name=\"model\", signature=signature)\n\n    # Log training curves as artifacts\n    import matplotlib.pyplot as plt\n\n    plt.figure(figsize=(12, 4))\n\n    plt.subplot(1, 2, 1)\n    plt.plot(result[\"history\"].history[\"loss\"], label=\"Training Loss\")\n    plt.plot(result[\"history\"].history[\"val_loss\"], label=\"Validation Loss\")\n    plt.title(\"Model Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n\n    plt.subplot(1, 2, 2)\n    plt.plot(\n        result[\"history\"].history[\"root_mean_squared_error\"], label=\"Training RMSE\"\n    )\n    plt.plot(\n        result[\"history\"].history[\"val_root_mean_squared_error\"],\n        label=\"Validation RMSE\",\n    )\n    plt.title(\"Model RMSE\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"RMSE\")\n    plt.legend()\n\n    plt.tight_layout()\n    plt.savefig(\"training_curves.png\")\n    mlflow.log_artifact(\"training_curves.png\")\n    plt.close()\n\n    # Return loss for Hyperopt (it minimizes)\n    return {\"loss\": result[\"val_rmse\"], \"status\": STATUS_OK}"
  },
  {
    "objectID": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-5-analyze-results-in-the-mlflow-ui",
    "href": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-5-analyze-results-in-the-mlflow-ui",
    "title": "End-to-end Machine Learning (MLflow + Docker + Google Cloud)",
    "section": "Step 5: Analyze Results in the MLflow UI",
    "text": "Step 5: Analyze Results in the MLflow UI\n\nNavigate to your experiment → click on “wine-quality-optimization\nAdd key columns: click “columns and add:\n\nMetrics | val_rmse\nParameters | learning_rate\nParameters | momentum\n\nInterprete the visualization: blue lines - better performing runs; red lines - worse performing runs\nAlso take a look at the training curves:\n\n\nfrom PIL import Image\nfrom IPython.display import display\n# Specify the path to your image file\nimage_path = \"val_rmse.png\"\n\n# Read the image\nimg = Image.open(image_path)\n\n# Display the image\ndisplay(img)\n\n\n\n\n\n\n\n\n\nfrom PIL import Image\nfrom IPython.display import display\n\n# Specify the path to your image file\nimage_path = \"training_curves.png\"\n\n# Read the image\nimg = Image.open(image_path)\n\n# Display the image\ndisplay(img)"
  },
  {
    "objectID": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-6-register-your-best-model",
    "href": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-6-register-your-best-model",
    "title": "End-to-end Machine Learning (MLflow + Docker + Google Cloud)",
    "section": "Step 6: Register your best model",
    "text": "Step 6: Register your best model\nTo find the best run: in the table view, click on the run with the lowest val_rmse then navigate to model artifacts and scroll to the “Artifacts” section. then register the model:\n- Go to \"Models\" tab in MLflow UI\n\n- Click on your registered model\n\n- Transition to \"Staging\" stage for testing\n\n- Add tags and descriptions as needed"
  },
  {
    "objectID": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-7-deploy-the-best-model",
    "href": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-7-deploy-the-best-model",
    "title": "End-to-end Machine Learning (MLflow + Docker + Google Cloud)",
    "section": "Step 7: Deploy the best model",
    "text": "Step 7: Deploy the best model\nTest your model with a REST API\n# Serve the model (choose the version number you registered)\nmlflow models serve -m \"models:/wine-quality-predictor/1\" --port 5002\nTest your deployment\n# Test with a sample wine\ncurl -X POST http://localhost:5002/invocations \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"dataframe_split\": {\n      \"columns\": [\n        \"fixed acidity\", \"volatile acidity\", \"citric acid\", \"residual sugar\",\n        \"chlorides\", \"free sulfur dioxide\", \"total sulfur dioxide\", \"density\",\n        \"pH\", \"sulphates\", \"alcohol\"\n      ],\n      \"data\": [[7.0, 0.27, 0.36, 20.7, 0.045, 45, 170, 1.001, 3.0, 0.45, 8.8]]\n    }\n  }'\n\n\nYou could also test with Python\n\nimport requests\nimport json\n\n# Prepare test data\ntest_wine = {\n    \"dataframe_split\": {\n        \"columns\": [\n            \"fixed acidity\",\n            \"volatile acidity\",\n            \"citric acid\",\n            \"residual sugar\",\n            \"chlorides\",\n            \"free sulfur dioxide\",\n            \"total sulfur dioxide\",\n            \"density\",\n            \"pH\",\n            \"sulphates\",\n            \"alcohol\",\n        ],\n        \"data\": [[7.0, 0.27, 0.36, 20.7, 0.045, 45, 170, 1.001, 3.0, 0.45, 8.8]],\n    }\n}\n\n# Make prediction request\nresponse = requests.post(\n    \"http://localhost:5002/invocations\",\n    headers={\"Content-Type\": \"application/json\"},\n    data=json.dumps(test_wine),\n)\n\nprediction = response.json()\nprint(f\"Predicted wine quality: {prediction['predictions'][0]:.2f}\")"
  },
  {
    "objectID": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-8-build-a-production-docker-container",
    "href": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-8-build-a-production-docker-container",
    "title": "End-to-end Machine Learning (MLflow + Docker + Google Cloud)",
    "section": "Step 8: Build a production Docker container",
    "text": "Step 8: Build a production Docker container\n# Build Docker image\nmlflow models build-docker \\\n  --model-uri \"models:/wine-quality-predictor/1\" \\\n  --name \"wine-quality-api\"\nTest your container:\n# Run the container\ndocker run -p 5003:8080 wine-quality-api\n\n# Test in another terminal\ncurl -X POST http://localhost:5003/invocations \\\n-H \"Content-Type: application/json\" \\\n-d '{\n  \"dataframe_split\": {\n    \"columns\": [\"fixed acidity\",\"volatile acidity\",\"citric acid\",\"residual sugar\",\"chlorides\",\"free sulfur dioxide\",\"total sulfur dioxide\",\"density\",\"pH\",\"sulphates\",\"alcohol\"],\n    \"data\": [[7.0, 0.27, 0.36, 20.7, 0.045, 45, 170, 1.001, 3.0, 0.45, 8.8]]\n  }\n}'"
  },
  {
    "objectID": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-9-deploy-to-google-cloud",
    "href": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-9-deploy-to-google-cloud",
    "title": "End-to-end Machine Learning (MLflow + Docker + Google Cloud)",
    "section": "Step 9: Deploy to Google cloud",
    "text": "Step 9: Deploy to Google cloud\n\nAuthentication and project set up\n$ gcloud auth login\nConfigure Docker for gcp $ gcloud auth configure-docker\nset project $ gcloud config set project PROJECT_ID\nIAM roles\n\nArtifacr registry Administrator\nroles/artifactregistry.createOnPushRepoAdmin\nStorage Administrator\n\nExport the credentials export GOOGLE_APPLICATION_CREDENTIALS=\"/path/to/your-service-account-file.json\"\nTag the docker image\n$ docker tag IMAGE_NAME gcr.io/PROJECT_ID/IMAGE_NAME:TAG\nPush the docker image to Google Cloud Container Registry $ docker push gcr.io/PROJECT_ID/IMAGE_NAME:TAG\n\n\nfrom PIL import Image\nfrom IPython.display import display\n# Specify the path to your image file\nimage_path = \"gc-artifact-registry.png\"\n\n# Read the image\nimg = Image.open(image_path)\n\n# Display the image\ndisplay(img)\n\n\n\n\n\n\n\n\nsee"
  },
  {
    "objectID": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-2-define-model-architecture",
    "href": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-2-define-model-architecture",
    "title": "End-to-end Machine Learning (MLflow + Docker + Google Cloud)",
    "section": "Step 2: Define Model architecture",
    "text": "Step 2: Define Model architecture\n\ndef create_and_train_model(learning_rate, momentum, epochs=10):\n    \"\"\"\n    Create and train a neural network with specified hyperparameters.\n\n    Returns:\n        dict: Training results including model and metrics\n    \"\"\"\n    #Normalize input features for better training stability\n    mean = np.mean(train_x, axis=0)\n    var = np.var(train_x, axis=0)\n\n    #Define model architecture\n    model = keras.Sequential(\n        [\n            keras.Input([train_x.shape[1]]),\n            keras.layers.Normalization(mean=mean, variance=var),\n            keras.layers.Dense(64, activation=\"relu\"),\n            keras.layers.Dropout(0.2),  # Add regularization\n            keras.layers.Dense(32, activation=\"relu\"),\n            keras.layers.Dense(1),\n        ]\n    )\n\n    #Compile with specified hyperparameters\n    model.compile(\n        optimizer=keras.optimizers.SGD(learning_rate=learning_rate, momentum=momentum),\n        loss=\"mean_squared_error\",\n        metrics=[keras.metrics.RootMeanSquaredError()],\n    )\n\n    #Train with early stopping for efficiency\n    early_stopping = keras.callbacks.EarlyStopping(\n        patience=3, restore_best_weights=True\n    )\n\n    #Train the model\n    history = model.fit(\n        train_x,\n        train_y,\n        validation_data=(valid_x, valid_y),\n        epochs=epochs,\n        batch_size=64,\n        callbacks=[early_stopping],\n        verbose=0,  # Reduce output for cleaner logs\n    )\n\n    #Evaluate on validation set\n    val_loss, val_rmse = model.evaluate(valid_x, valid_y, verbose=0)\n\n    return {\n        \"model\": model,\n        \"val_rmse\": val_rmse,\n        \"val_loss\": val_loss,\n        \"history\": history,\n        \"epochs_trained\": len(history.history[\"loss\"]),\n    }"
  },
  {
    "objectID": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-3-set-up-parameter-optimization",
    "href": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-3-set-up-parameter-optimization",
    "title": "End-to-end Machine Learning (MLflow + Docker + Google Cloud)",
    "section": "Step 3: Set up parameter optimization",
    "text": "Step 3: Set up parameter optimization\n\ndef objective(params):\n    \"\"\"\n    Objective function for hyperparameter optimization.\n    This function will be called by Hyperopt for each trial.\n    \"\"\"\n    with mlflow.start_run(nested=True):\n        #Log hyperparameters being tested\n        mlflow.log_params(\n            {\n                \"learning_rate\": params[\"learning_rate\"],\n                \"momentum\": params[\"momentum\"],\n                \"optimizer\": \"SGD\",\n                \"architecture\": \"64-32-1\",\n            }\n        )\n\n        #Train model with current hyperparameters\n        result = create_and_train_model(\n            learning_rate=params[\"learning_rate\"],\n            momentum=params[\"momentum\"],\n            epochs=15,\n        )\n\n        #Log training results\n        mlflow.log_metrics(\n            {\n                \"val_rmse\": result[\"val_rmse\"],\n                \"val_loss\": result[\"val_loss\"],\n                \"epochs_trained\": result[\"epochs_trained\"],\n            }\n        )\n\n        #Log the trained model\n        mlflow.tensorflow.log_model(result[\"model\"], name=\"model\", signature=signature)\n\n        #Log training curves as artifacts\n        import matplotlib.pyplot as plt\n\n        plt.figure(figsize=(12, 4))\n\n        plt.subplot(1, 2, 1)\n        plt.plot(result[\"history\"].history[\"loss\"], label=\"Training Loss\")\n        plt.plot(result[\"history\"].history[\"val_loss\"], label=\"Validation Loss\")\n        plt.title(\"Model Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.legend()\n\n        plt.subplot(1, 2, 2)\n        plt.plot(\n            result[\"history\"].history[\"root_mean_squared_error\"], label=\"Training RMSE\"\n        )\n        plt.plot(\n            result[\"history\"].history[\"val_root_mean_squared_error\"],\n            label=\"Validation RMSE\",\n        )\n        plt.title(\"Model RMSE\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"RMSE\")\n        plt.legend()\n\n        plt.tight_layout()\n        plt.savefig(\"training_curves.png\")\n        mlflow.log_artifact(\"training_curves.png\")\n        plt.close()\n\n        #Return loss for Hyperopt (it minimizes)\n        return {\"loss\": result[\"val_rmse\"], \"status\": STATUS_OK}\n\n\n#Define search space for hyperparameters\nsearch_space = {\n    \"learning_rate\": hp.loguniform(\"learning_rate\", np.log(1e-5), np.log(1e-1)),\n    \"momentum\": hp.uniform(\"momentum\", 0.0, 0.9),\n}\n\nprint(\"Search space defined:\")\nprint(\"- Learning rate: 1e-5 to 1e-1 (log-uniform)\")\nprint(\"- Momentum: 0.0 to 0.9 (uniform)\")"
  },
  {
    "objectID": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-4-run-the-hyperparameter-optimization",
    "href": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-4-run-the-hyperparameter-optimization",
    "title": "End-to-end Machine Learning (MLflow + Docker + Google Cloud)",
    "section": "Step 4: Run the hyperparameter optimization",
    "text": "Step 4: Run the hyperparameter optimization\n\n#Create or set experiment\nexperiment_name = \"wine-quality-optimization\"\nmlflow.set_experiment(experiment_name)\n\nprint(f\"Starting hyperparameter optimization experiment: {experiment_name}\")\nprint(\"This will run 15 trials to find optimal hyperparameters...\")\n\nwith mlflow.start_run(run_name=\"hyperparameter-sweep\"):\n    #Log experiment metadata\n    mlflow.log_params(\n        {\n            \"optimization_method\": \"Tree-structured Parzen Estimator (TPE)\",\n            \"max_evaluations\": 15,\n            \"objective_metric\": \"validation_rmse\",\n            \"dataset\": \"wine-quality\",\n            \"model_type\": \"neural_network\",\n        }\n    )\n\n    #Run optimization\n    trials = Trials()\n    best_params = fmin(\n        fn=objective,\n        space=search_space,\n        algo=tpe.suggest,\n        max_evals=15,\n        trials=trials,\n        verbose=True,\n    )\n\n    #Find and log best results\n    best_trial = min(trials.results, key=lambda x: x[\"loss\"])\n    best_rmse = best_trial[\"loss\"]\n\n    #Log optimization results\n    mlflow.log_params(\n        {\n            \"best_learning_rate\": best_params[\"learning_rate\"],\n            \"best_momentum\": best_params[\"momentum\"],\n        }\n    )\n    mlflow.log_metrics(\n        {\n            \"best_val_rmse\": best_rmse,\n            \"total_trials\": len(trials.trials),\n            \"optimization_completed\": 1,\n        }\n    )"
  },
  {
    "objectID": "posts/2025-08-03-Web-scraping-python/index.html",
    "href": "posts/2025-08-03-Web-scraping-python/index.html",
    "title": "Web Scraping with Python",
    "section": "",
    "text": "BibTeX citation:@online{okola2025,\n  author = {Okola, Basil},\n  title = {Web {Scraping} with {Python}},\n  date = {2025-08-03},\n  url = {https://bokola.github.io/posts/2025-08-03-Web-scraping-python/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nOkola, Basil. 2025. “Web Scraping with Python.” August 3,\n2025. https://bokola.github.io/posts/2025-08-03-Web-scraping-python/.\nThis is a web scrapping task to find conflict/war related news articles from the internet. There is been quite a lot of that considering the Russia/Ukraine conflict, The South Sudan conflict, and the Palestine/Israel conflict just to mention the most reported cases."
  },
  {
    "objectID": "posts/2025-08-03-Web-scraping-python/index.html#using-beautifulsoup-requests",
    "href": "posts/2025-08-03-Web-scraping-python/index.html#using-beautifulsoup-requests",
    "title": "Web Scraping with Python",
    "section": "Using Beautifulsoup + requests",
    "text": "Using Beautifulsoup + requests\n\nUnderstanding website’s structure\nPrior to scraping inspect the HTML source code of the web page to identify the elements you want to scrape\n\n\nSet up your develpment environment\nCreate a virtual environment, follow prompts per your IDE. For VScode I pressed Ctrl+Shift+Pthen searched Python: Create Environment A beginner web scraper in Python is advised to start with requests and beautifulsoup4 librarires which is what we will use. \nimport requests\nfrom bs4 import BeautifulSoup\n\nbaseurl = \"https://news.ycombinator.com\"\nuser = \"\"\npassd = \"\"\n\ns = requests.Session()\ndata = {\"goto\": \"news\", \"acct\": user, \"pw\": passd}\nr = s.post(f'{baseurl}', data=data)\n\nsoup = BeautifulSoup(r.text, 'html.parser')\nif soup.find(id='logout') is not None:\n    print(\"Successfully logged in\")\nelse:\n    print(\"Authentication error\")\n\n\nInspect HTML element\nEach post is wrapped in a &lt;tr&gt; tag with the class athing\n\nfrom PIL import Image\nfrom IPython.display import display\n\nimport matplotlib.image as mpimg\nimage_path = \"hacker-element.png\"\n\nimage = Image.open(image_path)\ndisplay(image)\n\n\n\n\n\n\n\n\n\n\nScrape with requests + beautifulsoup4\n\nimport requests\nfrom bs4 import BeautifulSoup\n\nr = requests.get(\"https://news.ycombinator.com/\")\nsoup = BeautifulSoup(r.text, 'html.parser')\nlinks = soup.find_all('tr', class_='athing')\n\nformatted_links = []\nfor link in links:\n    data = {\n        'id': link['id'],\n        'title': link.find_all(\"td\")[2].a.text,\n        'url': link.find_all(\"td\")[2].a['href'],\n        'rank': int(link.find_all(\"td\")[0].span.text.replace('.', ''))\n    }\n    formatted_links.append(data)\n\n\n    formatted_links.append(data)\n    \nprint(formatted_links)\n\n[{'id': '44754697', 'title': 'New quantum state of matter found at interface of exotic materials', 'url': 'https://phys.org/news/2025-07-quantum-state-interface-exotic-materials.html', 'rank': 1}, {'id': '44754697', 'title': 'New quantum state of matter found at interface of exotic materials', 'url': 'https://phys.org/news/2025-07-quantum-state-interface-exotic-materials.html', 'rank': 1}, {'id': '44778936', 'title': 'Modern Node.js Patterns', 'url': 'https://kashw1n.com/blog/nodejs-2025/', 'rank': 2}, {'id': '44778936', 'title': 'Modern Node.js Patterns', 'url': 'https://kashw1n.com/blog/nodejs-2025/', 'rank': 2}, {'id': '44780353', 'title': 'So you want to parse a PDF?', 'url': 'https://eliot-jones.com/2025/8/pdf-parsing-xref', 'rank': 3}, {'id': '44780353', 'title': 'So you want to parse a PDF?', 'url': 'https://eliot-jones.com/2025/8/pdf-parsing-xref', 'rank': 3}, {'id': '44779428', 'title': 'Writing a good design document', 'url': 'https://grantslatton.com/how-to-design-document', 'rank': 4}, {'id': '44779428', 'title': 'Writing a good design document', 'url': 'https://grantslatton.com/how-to-design-document', 'rank': 4}, {'id': '44777760', 'title': 'Persona vectors: Monitoring and controlling character traits in language models', 'url': 'https://www.anthropic.com/research/persona-vectors', 'rank': 5}, {'id': '44777760', 'title': 'Persona vectors: Monitoring and controlling character traits in language models', 'url': 'https://www.anthropic.com/research/persona-vectors', 'rank': 5}, {'id': '44781523', 'title': 'A parser for TypeScript types, written in TypeScript types', 'url': 'https://github.com/easrng/tsints', 'rank': 6}, {'id': '44781523', 'title': 'A parser for TypeScript types, written in TypeScript types', 'url': 'https://github.com/easrng/tsints', 'rank': 6}, {'id': '44775563', 'title': \"If you're remote, ramble\", 'url': 'https://stephango.com/ramblings', 'rank': 7}, {'id': '44775563', 'title': \"If you're remote, ramble\", 'url': 'https://stephango.com/ramblings', 'rank': 7}, {'id': '44765562', 'title': 'Life, Work, Death and the Peasant: Family Formation', 'url': 'https://acoup.blog/2025/08/01/collections-life-work-death-and-the-peasant-part-iiia-family-formation/', 'rank': 8}, {'id': '44765562', 'title': 'Life, Work, Death and the Peasant: Family Formation', 'url': 'https://acoup.blog/2025/08/01/collections-life-work-death-and-the-peasant-part-iiia-family-formation/', 'rank': 8}, {'id': '44777766', 'title': 'How Python grew from a language to a community', 'url': 'https://thenewstack.io/how-python-grew-from-a-language-to-a-community/', 'rank': 9}, {'id': '44777766', 'title': 'How Python grew from a language to a community', 'url': 'https://thenewstack.io/how-python-grew-from-a-language-to-a-community/', 'rank': 9}, {'id': '44781116', 'title': 'Why doctors hate their computers (2018)', 'url': 'https://www.newyorker.com/magazine/2018/11/12/why-doctors-hate-their-computers', 'rank': 10}, {'id': '44781116', 'title': 'Why doctors hate their computers (2018)', 'url': 'https://www.newyorker.com/magazine/2018/11/12/why-doctors-hate-their-computers', 'rank': 10}, {'id': '44780878', 'title': 'Typed languages are better suited for vibecoding', 'url': 'https://solmaz.io/typed-languages-are-better-suited-for-vibecoding', 'rank': 11}, {'id': '44780878', 'title': 'Typed languages are better suited for vibecoding', 'url': 'https://solmaz.io/typed-languages-are-better-suited-for-vibecoding', 'rank': 11}, {'id': '44782046', 'title': 'Rising Young Worker Despair in the United States', 'url': 'https://www.nber.org/papers/w34071', 'rank': 12}, {'id': '44782046', 'title': 'Rising Young Worker Despair in the United States', 'url': 'https://www.nber.org/papers/w34071', 'rank': 12}, {'id': '44743631', 'title': 'C++: \"model of the hardware\" vs. \"model of the compiler\" (2018)', 'url': 'http://ithare.com/c-model-of-the-hardware-vs-model-of-the-compiler/', 'rank': 13}, {'id': '44743631', 'title': 'C++: \"model of the hardware\" vs. \"model of the compiler\" (2018)', 'url': 'http://ithare.com/c-model-of-the-hardware-vs-model-of-the-compiler/', 'rank': 13}, {'id': '44780540', 'title': 'How to grow almost anything', 'url': 'https://howtogrowalmostanything.notion.site/htgaa25', 'rank': 14}, {'id': '44780540', 'title': 'How to grow almost anything', 'url': 'https://howtogrowalmostanything.notion.site/htgaa25', 'rank': 14}, {'id': '44767508', 'title': 'Efficiently Generating a Number in a Range (2018)', 'url': 'https://www.pcg-random.org/posts/bounded-rands.html', 'rank': 15}, {'id': '44767508', 'title': 'Efficiently Generating a Number in a Range (2018)', 'url': 'https://www.pcg-random.org/posts/bounded-rands.html', 'rank': 15}, {'id': '44745441', 'title': \"2,500-year-old Siberian 'ice mummy' had intricate tattoos, imaging reveals\", 'url': 'https://www.bbc.com/news/articles/c4gzx0zm68vo', 'rank': 16}, {'id': '44745441', 'title': \"2,500-year-old Siberian 'ice mummy' had intricate tattoos, imaging reveals\", 'url': 'https://www.bbc.com/news/articles/c4gzx0zm68vo', 'rank': 16}, {'id': '44765730', 'title': 'Welcome to url.town, population 465', 'url': 'https://url.town/', 'rank': 17}, {'id': '44765730', 'title': 'Welcome to url.town, population 465', 'url': 'https://url.town/', 'rank': 17}, {'id': '44775700', 'title': 'Tokens are getting more expensive', 'url': 'https://ethanding.substack.com/p/ai-subscriptions-get-short-squeezed', 'rank': 18}, {'id': '44775700', 'title': 'Tokens are getting more expensive', 'url': 'https://ethanding.substack.com/p/ai-subscriptions-get-short-squeezed', 'rank': 18}, {'id': '44760583', 'title': 'Survival at High Altitudes: Wheel-Well Passengers (1996)', 'url': 'https://rosap.ntl.bts.gov/view/dot/57536', 'rank': 19}, {'id': '44760583', 'title': 'Survival at High Altitudes: Wheel-Well Passengers (1996)', 'url': 'https://rosap.ntl.bts.gov/view/dot/57536', 'rank': 19}, {'id': '44781189', 'title': 'Poorest US workers hit hardest by slowing wage growth', 'url': 'https://www.ft.com/content/cfb77a53-fef8-4382-b102-c217e0aa4b25', 'rank': 20}, {'id': '44781189', 'title': 'Poorest US workers hit hardest by slowing wage growth', 'url': 'https://www.ft.com/content/cfb77a53-fef8-4382-b102-c217e0aa4b25', 'rank': 20}, {'id': '44774104', 'title': 'Twenty Eighth International Obfuscated C Code Contest', 'url': 'https://www.ioccc.org/2024/index.html', 'rank': 21}, {'id': '44774104', 'title': 'Twenty Eighth International Obfuscated C Code Contest', 'url': 'https://www.ioccc.org/2024/index.html', 'rank': 21}, {'id': '44764696', 'title': 'A dedicated skin-to-brain circuit for cool sensation in mice', 'url': 'https://www.sciencedaily.com/releases/2025/07/250730030354.htm', 'rank': 22}, {'id': '44764696', 'title': 'A dedicated skin-to-brain circuit for cool sensation in mice', 'url': 'https://www.sciencedaily.com/releases/2025/07/250730030354.htm', 'rank': 22}, {'id': '44777055', 'title': 'This Old SGI: notes and memoirs on the Silicon Graphics 4D series (1996)', 'url': 'https://archive.irixnet.org/thisoldsgi/', 'rank': 23}, {'id': '44777055', 'title': 'This Old SGI: notes and memoirs on the Silicon Graphics 4D series (1996)', 'url': 'https://archive.irixnet.org/thisoldsgi/', 'rank': 23}, {'id': '44775830', 'title': 'How to make almost anything (2019)', 'url': 'https://fab.cba.mit.edu/classes/863.19/CBA/people/dsculley/index.html', 'rank': 24}, {'id': '44775830', 'title': 'How to make almost anything (2019)', 'url': 'https://fab.cba.mit.edu/classes/863.19/CBA/people/dsculley/index.html', 'rank': 24}, {'id': '44779839', 'title': 'Everything to know about UniFi OS Server', 'url': 'https://deluisio.com/networking/unifi/2025/08/03/everything-you-need-to-know-about-unifi-os-server-before-you-waste-time-testing-it/', 'rank': 25}, {'id': '44779839', 'title': 'Everything to know about UniFi OS Server', 'url': 'https://deluisio.com/networking/unifi/2025/08/03/everything-you-need-to-know-about-unifi-os-server-before-you-waste-time-testing-it/', 'rank': 25}, {'id': '44762397', 'title': 'Show HN: Schematra – Sinatra-inspired minimal web framework for Chicken Scheme', 'url': 'https://github.com/rolandoam/schematra', 'rank': 26}, {'id': '44762397', 'title': 'Show HN: Schematra – Sinatra-inspired minimal web framework for Chicken Scheme', 'url': 'https://github.com/rolandoam/schematra', 'rank': 26}, {'id': '44754789', 'title': 'The first lunar road trip', 'url': 'https://nautil.us/the-first-lunar-road-trip-1227738/', 'rank': 27}, {'id': '44754789', 'title': 'The first lunar road trip', 'url': 'https://nautil.us/the-first-lunar-road-trip-1227738/', 'rank': 27}, {'id': '44771808', 'title': 'Lina Khan points to Figma IPO as vindication of M&A scrutiny', 'url': 'https://techcrunch.com/2025/08/02/lina-khan-points-to-figma-ipo-as-vindication-for-ma-scrutiny/', 'rank': 28}, {'id': '44771808', 'title': 'Lina Khan points to Figma IPO as vindication of M&A scrutiny', 'url': 'https://techcrunch.com/2025/08/02/lina-khan-points-to-figma-ipo-as-vindication-for-ma-scrutiny/', 'rank': 28}, {'id': '44780552', 'title': 'Learnable Programming (2012)', 'url': 'https://worrydream.com/LearnableProgramming/', 'rank': 29}, {'id': '44780552', 'title': 'Learnable Programming (2012)', 'url': 'https://worrydream.com/LearnableProgramming/', 'rank': 29}, {'id': '44779178', 'title': 'Shrinking freshwater availability increasing land contribution to sea level rise', 'url': 'https://news.asu.edu/20250725-environment-and-sustainability-new-global-study-shows-freshwater-disappearing-alarming', 'rank': 30}, {'id': '44779178', 'title': 'Shrinking freshwater availability increasing land contribution to sea level rise', 'url': 'https://news.asu.edu/20250725-environment-and-sustainability-new-global-study-shows-freshwater-disappearing-alarming', 'rank': 30}]\n\n\n\n\nStore data as .csv\n\nimport csv\n\nfile = 'hacker_news_posts.csv'\nwith open(file, 'w', newline=\"\") as f:\n    writer = csv.DictWriter(f, fieldnames=['id', 'title', 'url', 'rank'])\n    writer.writeheader()\n    for row in formatted_links:\n        writer.writerow(row)\n\n\n\nStore data in PostgreSQL\n\nStep 1: Installing PostgreSQL\nFollow the PostgreSQL download page for downloads and installation\n\n\nStep 2: Creating a Database Table\nFirst you’ll need a table\n\n#start service\nsudo systemctl start postgresql.service\n\n#log in as a superuser\nsudo -i -u postgres\n\nCREATE DATABASE scrape_demo;\n\n\nCREATE TABLE \"hn_links\" (\n    \"id\" INTEGER NOT NULL,\n    \"title\" VARCHAR NOT NULL,\n    \"url\" VARCHAR NOT NULL,\n    \"rank\" INTEGER NOT NULL\n);\n\n\nStep 3: Install Psycopg2 to Connect to PostgreSQL\npip install psycopg2\nEstablish connection to the database\nEnsure you set password for postgres user, which logs without a password by default\n\nsudo -u postgres psql\n\npostgres=# ALTER USER postgres PASSWORD 'myPassword';\nALTER ROLE\n\n\nimport psycopg2\nimport os\nimport dotenv\ndotenv.load_dotenv()\n\np = os.getenv(\"pass\")\ntable_name = \"hn_links\"\ncsv_path = \"hacker_news_posts.csv\"\n\n\ncon = psycopg2.connect(host=\"127.0.0.1\", port=\"5432\", user=\"postgres\", password = p,database=\"scrape_demo\")\n\n# Get a database cursor\ncur = con.cursor()\n\nr = requests.get('https://news.ycombinator.com')\nsoup = BeautifulSoup(r.text, 'html.parser')\nlinks = soup.findAll('tr', class_='athing')\n\nfor link in links:\n    cur.execute(\"\"\"\n        INSERT INTO hn_links (id, title, url, rank)\n        VALUES (%s, %s, %s, %s)\n        \"\"\",\n        (\n            link['id'],\n            link.find_all('td')[2].a.text,\n            link.find_all('td')[2].a['href'],\n            int(link.find_all('td')[0].span.text.replace('.', ''))\n        )\n    )\n\n# Commit the data\ncon.commit()\n\n# Close our database connections\ncur.close()\ncon.close()\n\n/tmp/ipykernel_21147/1184676145.py:18: DeprecationWarning: Call to deprecated method findAll. (Replaced by find_all) -- Deprecated since version 4.0.0.\n  links = soup.findAll('tr', class_='athing')"
  },
  {
    "objectID": "posts/2025-08-03-Web-scraping-python/index.html#using-scapingbee-python-client",
    "href": "posts/2025-08-03-Web-scraping-python/index.html#using-scapingbee-python-client",
    "title": "Web Scraping with Python",
    "section": "Using ScapingBee Python Client",
    "text": "Using ScapingBee Python Client\nScrapingBee is a subscription API providing a way to bypass any website’s anti-scraping measures.\n\nfrom scrapingbee import ScrapingBeeClient\nimport json\nimport pandas as pd\n\ndotenv.load_dotenv()\nkey = os.getenv(\"spring_bee_api_key\")\n\nsb_client = ScrapingBeeClient(api_key=key)\nurl = \"https://www.aljazeera.com/\"\n\n\n\nclient = ScrapingBeeClient(api_key=key)\n\ndef google_news_headlines_api(country_code='US'):\n\n    extract_rules = {\n        \"news\": {\n        \"selector\": \"article\",\n        \"type\": \"list\",\n            \"output\": {\n                \"title\": \".gPFEn,.JtKRv\",\n                \"source\": \".vr1PYe\",\n                \"time\": \"time@datetime\",\n                \"author\": \".bInasb\",\n                \"link\": \".WwrzSb@href\"\n            }\n        }\n    }\n\n    js_scenario = {\n        \"instructions\":[\n            {\"evaluate\":\"document.querySelectorAll('.WwrzSb').forEach( (e) =&gt; e.href = e.href );\"}\n        ]\n    }\n\n    response =  client.get(\n        f'https://news.google.com/topics/CAAqJggKIiBDQkFTRWdvSUwyMHZNRFZxYUdjU0FtVnVHZ0pWVXlnQVAB?&gl={country_code}',\n        params={ \n            \"custom_google\": \"true\",\n            \"wait_for\": \".bInasb\",\n            \"extract_rules\": extract_rules,\n            \"js_scenario\": js_scenario, \n        },\n        retries=2\n    )\n\n    if response.text.startswith('{\"message\":\"Invalid api key:'):\n        return f\"Oops! It seems you may have missed adding your API KEY or you are using an incorrect key.\\nGet your free API KEY and 1000 free scraping credits by signing up to our platform here: https://app.scrapingbee.com/account/register\"\n    else:\n        def get_info():\n            if len(response.json()['news']) == 0:\n                return \"FAILED TO RETRIEVE NEWS\"\n            else:\n                return \"SUCCESS\"\n\n        return pd.DataFrame({\n            'count': len(response.json()['news']),\n            'news_extracts': response.json()['news'],\n            'info': f\"{response.status_code} {get_info()}\",\n        })\n#country_code: Set the news location; US, IN, etc.\ndf = google_news_headlines_api(country_code='US')\n\nprint(df.iloc[:10])\n\n   count                                      news_extracts         info\n0    263  {'title': 'Texas Democrats Leave State to Stop...  200 SUCCESS\n1    263  {'title': 'Democrats flee Texas to block Repub...  200 SUCCESS\n2    263  {'title': 'A Texas Democratic lawmaker on thei...  200 SUCCESS\n3    263  {'title': 'Texas Democrats flee state to preve...  200 SUCCESS\n4    263  {'title': 'Videos of emaciated hostages condem...  200 SUCCESS\n5    263  {'title': 'Hamas says it will allow aid for ho...  200 SUCCESS\n6    263  {'title': 'Netanyahu asks Red Cross to help ho...  200 SUCCESS\n7    263  {'title': 'Hamas says open to ICRC delivering ...  200 SUCCESS\n8    263  {'title': 'White House advisers defend Trump’s...  200 SUCCESS\n9    263  {'title': 'Trump Fired America’s Economic Data...  200 SUCCESS"
  },
  {
    "objectID": "posts/2025-08-03-Web-scraping-python/index.html#web-scraping-with-scrapy",
    "href": "posts/2025-08-03-Web-scraping-python/index.html#web-scraping-with-scrapy",
    "title": "Web Scraping with Python",
    "section": "Web scraping with Scrapy",
    "text": "Web scraping with Scrapy\nScrapy is a web scraping framework using an event-driven networking infrastracture built around an asynchronous network engine that allows for more efficiency and scalability. It is made of a crawler that handles low-level logic, and a spider that is provider by the user to help the crawler generate request, parse and retrieve data.\nIn this section we use scrapy to scrape product listings available at web-scraping.dev, but first some house-keeping.\nTo install scrapy run pip install scrapy or better still add scrapy to your project’s requirements.txt and run pip install -r requirements.txt. Start a scrapy project by running scrapy startproject &lt;project-name&gt; &lt;project-directory&gt; in terminal. Some scrapy commands below:\n\n!scrapy --help\n\nScrapy 2.13.3 - active project: webscrapingdev\n\nUsage:\n  scrapy &lt;command&gt; [options] [args]\n\nAvailable commands:\n  bench         Run quick benchmark test\n  check         Check spider contracts\n  crawl         Run a spider\n  edit          Edit spider\n  fetch         Fetch a URL using the Scrapy downloader\n  genspider     Generate new spider using pre-defined templates\n  list          List available spiders\n  parse         Parse URL (using its spider) and print the results\n  runspider     Run a self-contained spider (without creating a project)\n  settings      Get settings values\n  shell         Interactive scraping console\n  startproject  Create new project\n  version       Print Scrapy version\n  view          Open URL in browser, as seen by Scrapy\n\nUse \"scrapy &lt;command&gt; -h\" to see more info about a command\n\n\n\nCreating a spider\nrun scrapy genspider &lt;name&gt; &lt;host-to-scrape&gt;\n\n!scrapy genspider products web-scraping.dev\n\nSpider 'products' already exists in module:\n  webscrapingdev.spiders.products\n\n\n\n!scrapy list\n!tree\n\n\nproducts\n\n.\n\n├── article-class.png\n\n├── hacker-element.png\n\n├── hacker_news_posts.csv\n\n├── LICENSE\n\n├── producthunt.json\n\n├── README.md\n\n├── requirements.txt\n\n├── results.json\n\n├── scrapy.cfg\n\n├── web-scrap_files\n\n│   ├── figure-html\n\n│   │   └── cell-2-output-1.png\n\n│   └── libs\n\n│       ├── bootstrap\n\n│       │   ├── bootstrap-b9f025fa521194ab51f5de92fbd134be.min.css\n\n│       │   ├── bootstrap-icons.css\n\n│       │   ├── bootstrap-icons.woff\n\n│       │   └── bootstrap.min.js\n\n│       ├── clipboard\n\n│       │   └── clipboard.min.js\n\n│       └── quarto-html\n\n│           ├── anchor.min.js\n\n│           ├── popper.min.js\n\n│           ├── quarto.js\n\n│           ├── quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css\n\n│           ├── tabsets\n\n│           │   └── tabsets.js\n\n│           ├── tippy.css\n\n│           └── tippy.umd.min.js\n\n├── web-scrap.html\n\n├── webscrapingdev\n\n│   ├── __init__.py\n\n│   ├── items.py\n\n│   ├── middlewares.py\n\n│   ├── pipelines.py\n\n│   ├── __pycache__\n\n│   │   ├── __init__.cpython-312.pyc\n\n│   │   └── settings.cpython-312.pyc\n\n│   ├── settings.py\n\n│   └── spiders\n\n│       ├── __init__.py\n\n│       ├── products.py\n\n│       └── __pycache__\n\n│           ├── __init__.cpython-312.pyc\n\n│           └── products.cpython-312.pyc\n\n├── web-scrap.ipynb\n\n├── web-scrap.py\n\n├── web-scrap.qmd\n\n└── web-scrap.quarto_ipynb\n\n\n\n12 directories, 38 files\n\n\n\n\nif you open the generated spider - products.py, you’ll find the following\nimport scrapy\n\n\nclass ProductsSpider(scrapy.Spider):\n    name = \"products\"\n    allowed_domains = [\"web-scraping.dev\"]\n    start_urls = [\"https://web-scraping.dev\"]\n\n    def parse(self, response):\n        pass\n\n\nname is used as a reference to the spider for scrapy commands like crawl` - this would run the scraper\nallowed_domains is a safety feauture restricting this spider to crawl only particular domains.\nstart_urls indicates the spider starting point while parse() is the first callback to execute above instructions.\n\n\n\nAdding crawling logic\nWe want our start_urls to be some topic directories e.g., https://www.producthunt.com/topics/developer-tools and our parse() callback method to find all product links and schedule them to be scrapped:\n# /spiders/products.py\nimport scrapy\nfrom scrapy.http import Response, Request\n\n\nclass ProductsSpider(scrapy.Spider):\n    name = 'products'\n    allowed_domains = ['web-scraping.dev']\n    start_urls = [\n        'https://web-scraping.dev/products',\n    ]\n\n    def parse(self, response: Response):\n        product_urls = response.xpath(\n            \"//div[@class='row product']/div/h3/a/@href\"\n        ).getall()\n        for url in product_urls:\n            yield Request(url, callback=self.parse_product)\n        # or shortcut in scrapy &gt;2.0\n        # yield from response.follow_all(product_urls, callback=self.parse_product)\n    \n    def parse_product(self, response: Response):\n        print(response)\n\n\n\nAdding Parsing Logic\nPopulate parse_product()\n# /spiders/products.py\n...\n\n    def parse_product(self, response: Response):\n        yield {\n            \"title\": response.xpath(\"//h3[contains(@class, 'product-title')]/text()\").get(),\n            \"price\": response.xpath(\"//span[contains(@class, 'product-price')]/text()\").get(),\n            \"image\": response.xpath(\"//div[contains(@class, 'product-image')]/img/@src\").get(),\n            \"description\": response.xpath(\"//p[contains(@class, 'description')]/text()\").get()\n        }\n\n\n\nBasic Settings\nAdjust recommended settings:\n# settings.py\n# will ignore /robots.txt rules that might prevent scraping\nROBOTSTXT_OBEY = False\n# will cache all request to /httpcache directory which makes running spiders in development much quicker\n# tip: to refresh cache just delete /httpcache directory\nHTTPCACHE_ENABLED = True\n# while developing we want to see debug logs\nLOG_LEVEL = \"DEBUG\" # or \"INFO\" in production\n\n# to avoid basic bot detection we want to set some basic headers\nDEFAULT_REQUEST_HEADERS = {\n    # we should use headers\n    'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\",\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\n\nRunning Spiders\nEither through the scrapy command or explicitly calling scrapy using a Python script.\n\n%%capture\n!scrapy crawl products\n\n\n\nSaving results\n\n%%capture\n\n!scrapy crawl products --output results.json\n\n\n!tree\n\n\n.\n\n├── article-class.png\n\n├── hacker-element.png\n\n├── hacker_news_posts.csv\n\n├── LICENSE\n\n├── producthunt.json\n\n├── README.md\n\n├── requirements.txt\n\n├── results.json\n\n├── scrapy.cfg\n\n├── web-scrap_files\n\n│   ├── figure-html\n\n│   │   └── cell-2-output-1.png\n\n│   └── libs\n\n│       ├── bootstrap\n\n│       │   ├── bootstrap-b9f025fa521194ab51f5de92fbd134be.min.css\n\n│       │   ├── bootstrap-icons.css\n\n│       │   ├── bootstrap-icons.woff\n\n│       │   └── bootstrap.min.js\n\n│       ├── clipboard\n\n│       │   └── clipboard.min.js\n\n│       └── quarto-html\n\n│           ├── anchor.min.js\n\n│           ├── popper.min.js\n\n│           ├── quarto.js\n\n│           ├── quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css\n\n│           ├── tabsets\n\n│           │   └── tabsets.js\n\n│           ├── tippy.css\n\n│           └── tippy.umd.min.js\n\n├── web-scrap.html\n\n├── webscrapingdev\n\n│   ├── __init__.py\n\n│   ├── items.py\n\n│   ├── middlewares.py\n\n│   ├── pipelines.py\n\n│   ├── __pycache__\n\n│   │   ├── __init__.cpython-312.pyc\n\n│   │   └── settings.cpython-312.pyc\n\n│   ├── settings.py\n\n│   └── spiders\n\n│       ├── __init__.py\n\n│       ├── products.py\n\n│       └── __pycache__\n\n│           ├── __init__.cpython-312.pyc\n\n│           └── products.cpython-312.pyc\n\n├── web-scrap.ipynb\n\n├── web-scrap.py\n\n├── web-scrap.qmd\n\n└── web-scrap.quarto_ipynb\n\n\n\n12 directories, 38 files\n\n\n\n\n\nimport json\njson_file = 'results.json'\nwith open(json_file) as f:\n    j_obj = json.load(f)\n\n\njson_fmt = json.dumps(j_obj, indent=2)\nprint(json_fmt)\n\n[\n  {\n    \"title\": \"Blue Energy Potion\",\n    \"price\": \"$4.99\",\n    \"image\": \"https://web-scraping.dev/assets/products/blue-potion.webp\",\n    \"description\": \"Ignite your gaming sessions with our 'Blue Energy Potion', a premium energy drink crafted for dedicated gamers. Inspired by the classic video game potions, this energy drink provides a much-needed boost to keep you focused and energized. It's more than just an energy drink - it's an ode to the gaming culture, packaged in an aesthetically pleasing potion-like bottle that'll make you feel like you're in your favorite game world. Drink up and game on!\"\n  },\n  {\n    \"title\": \"Red Energy Potion\",\n    \"price\": \"$4.99\",\n    \"image\": \"https://web-scraping.dev/assets/products/red-potion.webp\",\n    \"description\": \"Elevate your game with our 'Red Potion', an extraordinary energy drink that's as enticing as it is effective. This fiery red potion delivers an explosive berry flavor and an energy kick that keeps you at the top of your game. Are you ready to level up?\"\n  },\n  {\n    \"title\": \"Teal Energy Potion\",\n    \"price\": \"$4.99\",\n    \"image\": \"https://web-scraping.dev/assets/products/teal-potion.webp\",\n    \"description\": \"Experience a surge of vitality with our 'Teal Potion', an exceptional energy drink designed for the gaming community. With its intriguing teal color and a flavor that keeps you asking for more, this potion is your best companion during those long gaming nights. Every sip is an adventure - let the quest begin!\"\n  },\n  {\n    \"title\": \"Dark Red Energy Potion\",\n    \"price\": \"$4.99\",\n    \"image\": \"https://web-scraping.dev/assets/products/darkred-potion.webp\",\n    \"description\": \"Unleash the power within with our 'Dark Red Potion', an energy drink as intense as the games you play. Its deep red color and bold cherry cola flavor are as inviting as they are invigorating. Bring out the best in your gaming performance, and unlock your full potential.\"\n  },\n  {\n    \"title\": \"Box of Chocolate Candy\",\n    \"price\": \"$9.99 \",\n    \"image\": \"https://web-scraping.dev/assets/products/orange-chocolate-box-small-1.webp\",\n    \"description\": \"Indulge your sweet tooth with our Box of Chocolate Candy. Each box contains an assortment of rich, flavorful chocolates with a smooth, creamy filling. Choose from a variety of flavors including zesty orange and sweet cherry. Whether you're looking for the perfect gift or just want to treat yourself, our Box of Chocolate Candy is sure to satisfy.\"\n  }\n]"
  },
  {
    "objectID": "posts/2025-08-08-data-enginnering/index.html",
    "href": "posts/2025-08-08-data-enginnering/index.html",
    "title": "End-to-end Data Engineering with Python DuckDB, Google Cloud, dbt",
    "section": "",
    "text": "BibTeX citation:@online{okola2025,\n  author = {Okola, Basil},\n  title = {End-to-End {Data} {Engineering} with {Python} {DuckDB,}\n    {Google} {Cloud,} Dbt},\n  date = {2025-08-08},\n  url = {https://bokola.github.io/posts/2025-08-08-data-engineering/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nOkola, Basil. 2025. “End-to-End Data Engineering with Python\nDuckDB, Google Cloud, Dbt.” August 8, 2025. https://bokola.github.io/posts/2025-08-08-data-engineering/.\nThis is a ‘do it along’ following DuckDB material. We will source data from PyPi, a repository of python packages providing logs of how given libraries have been used across platforms, transform it into relevant tables, and feed it into a dashboard using SQL, dtb - a tool for building modular, maintainable data pipelines to power analytics, and DuckDB - an in-process SQL online analytical processing (OLAPs) relational database management system; then use evidence to create a dashboard utilizing SQL and markdown."
  },
  {
    "objectID": "posts/2025-08-08-data-enginnering/index.html#part-1-ingestion-pipeline",
    "href": "posts/2025-08-08-data-enginnering/index.html#part-1-ingestion-pipeline",
    "title": "End-to-end Data Engineering with Python DuckDB, Google Cloud, dbt",
    "section": "Part 1: Ingestion pipeline",
    "text": "Part 1: Ingestion pipeline\n\nSetup & requirements\n\nPython 3.12\nPoetry for dependency management\nMake to run Makefile commands\nA Google Cloud account to fetch the source data. Free tier covers any computing cost.\nA host of libraries: poetry duckdb google-cloud-bigquery google-auth google-cloud-bigquery-storage pyarrow pandas fire loguru pydantic pytest ruff\n\nThe architecture woul look like below\n\nfrom PIL import Image\nfrom IPython.display import display\n\nimg_f = \"./docs/architecture.png\"\nimg = Image.open(img_f)\ndisplay(img)"
  },
  {
    "objectID": "posts/2025-08-08-data-enginnering/index.html#exploring-the-data",
    "href": "posts/2025-08-08-data-enginnering/index.html#exploring-the-data",
    "title": "End-to-end Data Engineering with Python DuckDB, Google Cloud, dbt",
    "section": "Exploring the data",
    "text": "Exploring the data\nHead to your google cloud console, then under BigQuery search “file_downloads” and click SEARCH ALL PROJECTS.\n\nimg_f = \"./docs/big_query_search.png\"\nimg = Image.open(img_f)\ndisplay(img)\n\n\n\n\n\n\n\n\nLimit your query to a given timestamp as the data is very big.\n\nimg_f = \"./docs/query.png\"\nimg = Image.open(img_f)\ndisplay(img)\n\n\n\n\n\n\n\n\nThe ingestion part is bundled in /ingestion directory with the following primary files:\n\n!tree ./ingestion\n\n\n./ingestion\n\n├── bigquery.py\n\n├── duck.py\n\n├── models.py\n\n├── pipeline.py\n\n└── __pycache__\n\n    ├── bigquery.cpython-312.pyc\n\n    └── pipeline.cpython-312.pyc\n\n\n\n2 directories, 6 files\n\n\n\n\n\nbigquery.py has BigQuery helpers to get the client, get query results and query the public dataset\nduck.py contains DuckDB helpers to create a table from a dataframe, and write to a local or MotherDuck database\nmodels.py defines table schema\npipeline runs the ingestion pipeline use pydantic syntax\n\nThe pipeline was successfuly executed and data written locally and to motherduck:\n\nimg_f = \"./docs/motherduck_db.png\"\nimg = Image.open(img_f)\ndisplay(img)"
  },
  {
    "objectID": "posts/2025-08-08-data-engineering/index.html",
    "href": "posts/2025-08-08-data-engineering/index.html",
    "title": "End-to-end Data Engineering with Python DuckDB, Google Cloud, dbt",
    "section": "",
    "text": "BibTeX citation:@online{okola2025,\n  author = {Okola, Basil},\n  title = {End-to-End {Data} {Engineering} with {Python} {DuckDB,}\n    {Google} {Cloud,} Dbt},\n  date = {2025-08-08},\n  url = {https://bokola.github.io/posts/2025-08-08-data-engineering/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nOkola, Basil. 2025. “End-to-End Data Engineering with Python\nDuckDB, Google Cloud, Dbt.” August 8, 2025. https://bokola.github.io/posts/2025-08-08-data-engineering/.\nThis is a ‘do it along’ following DuckDB material. We will source data from PyPi, a repository of python packages providing logs of how given libraries have been used across platforms, transform it into relevant tables, and feed it into a dashboard using SQL, dtb - a tool for building modular, maintainable data pipelines to power analytics, and DuckDB - an in-process SQL online analytical processing (OLAPs) relational database management system; then use evidence to create a dashboard utilizing SQL and markdown."
  },
  {
    "objectID": "posts/2025-08-08-data-engineering/index.html#part-1-ingestion-pipeline",
    "href": "posts/2025-08-08-data-engineering/index.html#part-1-ingestion-pipeline",
    "title": "End-to-end Data Engineering with Python DuckDB, Google Cloud, dbt",
    "section": "Part 1: Ingestion pipeline",
    "text": "Part 1: Ingestion pipeline\n\nSetup & requirements\n\nPython 3.12\nPoetry for dependency management\nMake to run Makefile commands\nA Google Cloud account to fetch the source data. Free tier covers any computing cost.\nA host of libraries: poetry duckdb google-cloud-bigquery google-auth google-cloud-bigquery-storage pyarrow pandas fire loguru pydantic pytest ruff\n\nThe architecture woul look like below\n\nfrom PIL import Image\nfrom IPython.display import display\n\nimg_f = \"./docs/architecture.png\"\nimg = Image.open(img_f)\ndisplay(img)"
  },
  {
    "objectID": "posts/2025-08-08-data-engineering/index.html#exploring-the-data",
    "href": "posts/2025-08-08-data-engineering/index.html#exploring-the-data",
    "title": "End-to-end Data Engineering with Python DuckDB, Google Cloud, dbt",
    "section": "Exploring the data",
    "text": "Exploring the data\nHead to your google cloud console, then under BigQuery search “file_downloads” and click SEARCH ALL PROJECTS.\n\nimg_f = \"./docs/big_query_search.png\"\nimg = Image.open(img_f)\ndisplay(img)\n\n\n\n\n\n\n\n\nLimit your query to a given timestamp as the data is very big.\n\nimg_f = \"./docs/query.png\"\nimg = Image.open(img_f)\ndisplay(img)\n\n\n\n\n\n\n\n\nThe ingestion part is bundled in /ingestion directory with the following primary files:\n\n!tree ./ingestion\n\n\n./ingestion\n\n├── bigquery.py\n\n├── duck.py\n\n├── models.py\n\n├── pipeline.py\n\n└── __pycache__\n\n    ├── bigquery.cpython-312.pyc\n\n    └── pipeline.cpython-312.pyc\n\n\n\n2 directories, 6 files\n\n\n\n\n\nbigquery.py has BigQuery helpers to get the client, get query results and query the public dataset\nduck.py contains DuckDB helpers to create a table from a dataframe, and write to a local or MotherDuck database\nmodels.py defines table schema\npipeline runs the ingestion pipeline use pydantic syntax\n\nThe pipeline was successfuly executed and data written locally and to motherduck:\n\nimg_f = \"./docs/motherduck_db.png\"\nimg = Image.open(img_f)\ndisplay(img)"
  },
  {
    "objectID": "posts/2025-10-29-data-pub-ready-viz/index.html",
    "href": "posts/2025-10-29-data-pub-ready-viz/index.html",
    "title": "Publication ready visualization in R",
    "section": "",
    "text": "CitationBibTeX citation:@online{okola2025,\n  author = {Okola, Basil},\n  title = {Publication Ready Visualization in {R}},\n  date = {2025-10-29},\n  url = {https://bokola.github.io/posts/2025-10-29-pub-ready-viz/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nOkola, Basil. 2025. “Publication Ready Visualization in R.”\nOctober 29, 2025. https://bokola.github.io/posts/2025-10-29-pub-ready-viz/."
  },
  {
    "objectID": "posts/2025-10-29-pub-ready-viz/index.html",
    "href": "posts/2025-10-29-pub-ready-viz/index.html",
    "title": "Publication ready visualization in R",
    "section": "",
    "text": "CitationBibTeX citation:@online{okola2025,\n  author = {Okola, Basil},\n  title = {Publication Ready Visualization in {R}},\n  date = {2025-10-29},\n  url = {https://bokola.github.io/posts/2025-10-29-pub-ready-viz/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nOkola, Basil. 2025. “Publication Ready Visualization in R.”\nOctober 29, 2025. https://bokola.github.io/posts/2025-10-29-pub-ready-viz/."
  },
  {
    "objectID": "posts/2025-10-30-interactive-maps/index.html",
    "href": "posts/2025-10-30-interactive-maps/index.html",
    "title": "Interactive maps (R)",
    "section": "",
    "text": "This document explores making interactive maps using R. There are a number of R packages that can be used to generate such visualizations. We consider just a small number (not exhaustively)."
  },
  {
    "objectID": "posts/2025-10-30-interactive-maps/index.html#data",
    "href": "posts/2025-10-30-interactive-maps/index.html#data",
    "title": "Interactive maps (R)",
    "section": "Data",
    "text": "Data\nWe use gapminder dataset from {gapminder} package. It is contains data from 187 countries covering the periods 1952 - 2007 with the following columns:\n\ncountry: Country name\ncontinent: Continental territory of a country\nyear: 1952 - 2007\nlifeExp: Life expectancy in years\npop: population size\ngdpPercap: GDP per capita in infaltion-adjusted dollars\n\nThe second dataset is the meteorite landings from Tidytuesday project from the Meteoritical Society of NASA. It comes with the following variables:\n\nname: Meteorite name\nmass: Mass in grams\nlat: latitude\nlong: longitude\nfall: fall or found meteorite\n\n\n\nCode\npacman::p_load(\n    leaflet\n    ,gapminder\n    ,echarts4r\n    ,tidyverse\n    ,ggiraph\n    ,widgetframe\n    ,ggthemes\n    ,plotly\n    ,viridis\n    ,DT\n)\n\n\nFirst we organize the datasets:\n\n\nCode\n# 1. gapminder\n\n# country codes\ncodes &lt;- gapminder::country_codes\n# countries with info unfiltered version\ngapminder &lt;- gapminder::gapminder_unfiltered\n# join\ngapminder &lt;- gapminder %&gt;% left_join(codes) %&gt;%\n  mutate(code = iso_alpha)\n\n\nJoining with `by = join_by(country)`\n\n\nCode\n# a map of the world - Antarctica removed\nworld &lt;- map_data(\"world\") %&gt;%\n  filter(!grepl(\"antarctica\", region, ignore.case = T))\ngapminder_df &lt;- gapminder %&gt;%\n  inner_join(maps::iso3166 %&gt;% \n  select(a3, mapname), by = c(code = \"a3\")\n  ) %&gt;%\n    mutate(\n      mapname = str_remove(mapname, \"\\\\(.*\")\n    )\n\n\nWarning in inner_join(., maps::iso3166 %&gt;% select(a3, mapname), by = c(code = \"a3\")): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 605 of `x` matches multiple rows in `y`.\nℹ Row 2 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\nGapminder dataset:\n\n\nCode\ndatatable(gapminder_df)\n\n\n\n\n\n\n\n\nCode\nmeteorites &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-06-11/meteorites.csv\")\n\n\nRows: 45716 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): name, name_type, class, fall, geolocation\ndbl (5): id, mass, year, lat, long\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nMeteorite dataset:\n\n\nCode\ndatatable(meteorites)\n\n\nWarning in instance$preRenderHook(instance): It seems your data is too big for\nclient-side DataTables. You may consider server-side processing:\nhttps://rstudio.github.io/DT/server.html"
  },
  {
    "objectID": "posts/2025-10-30-interactive-maps/index.html#interactive-choropleth-map-with-ggiraph",
    "href": "posts/2025-10-30-interactive-maps/index.html#interactive-choropleth-map-with-ggiraph",
    "title": "Interactive maps (R)",
    "section": "Interactive choropleth map with {ggiraph}",
    "text": "Interactive choropleth map with {ggiraph}\nTo turn a static choropleth map by invoking a tooltip when we hover the pointer over a country, we can use {ggiraph} - geom_polygon_interactive() & girafe and {widgetframe} - framewidget() packages.\nIt is useful creating a reusable theming function:\n\n\nCode\ntheme_helper &lt;- function(){\n   theme(\n    axis.line = element_blank(),\n    axis.text = element_blank(),\n    axis.title = element_blank(),\n    axis.ticks = element_blank(),\n    plot.background = element_rect(fill = \"snow\", color = NA),\n    panel.background = element_rect(fill= \"snow\", color = NA),\n    plot.title = element_text(size = 16, hjust = 0.5),\n    plot.subtitle = element_text(size = 12, hjust = 0.5),\n    plot.caption = element_text(size = 8, hjust = 1),\n    legend.title = element_text(color = \"grey40\", size = 8),\n    legend.text = element_text(color = \"grey40\", size = 7, hjust = 0),\n    legend.position = c(0.05, 0.25),\n    plot.margin = unit(c(0.5,2,0.5,1), \"cm\")) \n}\n\n\n\n\nCode\nlife_exp_map &lt;- gapminder_df %&gt;%\n  filter(year == 2007) %&gt;%\n  right_join(world, by = c(mapname = \"region\")) %&gt;%\n  ggplot() +\n  geom_polygon_interactive(\n    color = \"white\", size = 0.01, \n    aes(long, lat, group = group, fill = lifeExp,\n    tooltip = sprintf(\"%s&lt;br/&gt;%s\", country, lifeExp))\n  ) +\n    theme_void() + \n    scale_fill_viridis(option = \"B\") +\n    labs(\n      title = \"Life Expectancy\",\n      subtitle = \"Year: 2007\",\n      caption = \"Source: gapminder.org\",\n      fill = \"Years\"\n    ) \n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\nℹ The deprecated feature was likely used in the ggiraph package.\n  Please report the issue at &lt;https://github.com/davidgohel/ggiraph/issues&gt;.\n\n\nCode\nlife_exp_map &lt;- life_exp_map + theme_helper() + coord_fixed(ratio = 1.3)\n\n\nPrint it interactively\n\nCode\nwidgetframe::frameWidget(girafe(code = print(life_exp_map)))"
  },
  {
    "objectID": "posts/2025-10-30-interactive-maps/index.html#interactive-choropleth-maps-with-plotly",
    "href": "posts/2025-10-30-interactive-maps/index.html#interactive-choropleth-maps-with-plotly",
    "title": "Interactive maps (R)",
    "section": "Interactive choropleth maps with {plotly}",
    "text": "Interactive choropleth maps with {plotly}\n{plotly} on top of tooltips also allows zooming, lasso/box selections or downloading the map as .png.\n\nCode\nlife_exp07 &lt;- gapminder_df %&gt;%\n  filter(year == 2007) %&gt;% \n  select(mapname, code, lifeExp)\n\np_07 &lt;- plot_geo(life_exp07)\n\np_07 &lt;- p_07 %&gt;% add_trace(\n  z = ~lifeExp, color = ~ lifeExp, colors = 'Oranges',\n  text = ~mapname, locations = ~ code\n) %&gt;% colorbar(title = \"Years\")\n\np_07 &lt;- p_07 %&gt;%\n  layout(\n    title = 'Life Expectancy in 2007 &lt;br&gt;Source:&lt;a href= \"https://www.gapminder.org\"&gt; gapminder.org&lt;/a&gt;', geo = p_07\n  )\n\np_07"
  },
  {
    "objectID": "posts/2025-10-30-interactive-maps/index.html#interactive-points-using-plotly",
    "href": "posts/2025-10-30-interactive-maps/index.html#interactive-points-using-plotly",
    "title": "Interactive maps (R)",
    "section": "Interactive points using {plotly}",
    "text": "Interactive points using {plotly}\n\n\nCode\nmeteorites_fell &lt;- meteorites %&gt;%\n  filter(fall == \"Fell\")\n\nmeteorites_map &lt;- list(\n  #scope = 'usa',\n  projection = list(type = 'Mercator'),\n  showland = TRUE,\n  landcolor = toRGB(\"grey80\")\n)\n\n\nmeteo_map &lt;- plot_geo(meteorites_fell, lat = ~lat, lon = ~long)\nmeteo_map &lt;- meteo_map %&gt;% add_markers(\n  text = ~paste(paste(\"Name:\", name), \n                paste(\"Year:\", year), \n                paste(\"Mass:\", mass), sep = \"&lt;br /&gt;\"),\n  color = ~mass, symbol = I(\"circle-dot\"), size = I(8), hoverinfo = \"text\"\n)\nmeteo_map &lt;- meteo_map %&gt;% colorbar(title = \"Mass\")\n\n\nWarning: Ignoring 10 observations\n\n\nCode\nmeteo_map &lt;- meteo_map %&gt;% layout(\n  title = 'Meteorite Landings&lt;br /&gt;(Meteorite falls)', geo = meteorites_map\n)\n\nmeteo_map"
  },
  {
    "objectID": "posts/2025-10-30-interactive-maps/index.html#interactive-maps-with-echarts4r",
    "href": "posts/2025-10-30-interactive-maps/index.html#interactive-maps-with-echarts4r",
    "title": "Interactive maps (R)",
    "section": "interactive maps with {echarts4r}",
    "text": "interactive maps with {echarts4r}\n\n\nCode\ndf &lt;- gapminder %&gt;% \n    mutate(Name = recode_factor(country,\n                              `Congo, Dem. Rep.`= \"Dem. Rep. Congo\",\n                              `Congo, Rep.`= \"Congo\",\n                              `Cote d'Ivoire`= \"Côte d'Ivoire\",\n                              `Central African Republic`= \"Central African Rep.\",\n                              `Yemen, Rep.`= \"Yemen\",\n                              `Korea, Rep.`= \"Korea\",\n                              `Korea, Dem. Rep.`= \"Dem. Rep. Korea\",\n                              `Czech Republic`= \"Czech Rep.\",\n                              `Slovak Republic`= \"Slovakia\",\n                              `Dominican Republic`= \"Dominican Rep.\",\n                              `Equatorial Guinea`= \"Eq. Guinea\"))\n\ndf %&gt;% group_by(year) %&gt;%\n  e_chart(Name, timeline = TRUE) %&gt;%\n  e_map(lifeExp) %&gt;%\n  e_visual_map(\n    min = 30, max = 90, type = \"piecewise\"\n  ) %&gt;%\n    e_title(\"Life expectancy by country and year\", left = \"center\") %&gt;%\n    e_tooltip(\n      trigger = \"item\", formatter = e_tooltip_choro_formatter()\n    )"
  },
  {
    "objectID": "posts/2025-10-30-interactive-maps/interactive-maps.html",
    "href": "posts/2025-10-30-interactive-maps/interactive-maps.html",
    "title": "Interactive maps (R)",
    "section": "",
    "text": "This document explores making interactive maps using R. There are a number of R packages that can be used to generate such visualizations. We consider just a small number (not exhaustively)."
  },
  {
    "objectID": "posts/2025-10-30-interactive-maps/interactive-maps.html#data",
    "href": "posts/2025-10-30-interactive-maps/interactive-maps.html#data",
    "title": "Interactive maps (R)",
    "section": "Data",
    "text": "Data\nWe use gapminder dataset from {gapminder} package. It is contains data from 187 countries covering the periods 1952 - 2007 with the following columns:\n\ncountry: Country name\ncontinent: Continental territory of a country\nyear: 1952 - 2007\nlifeExp: Life expectancy in years\npop: population size\ngdpPercap: GDP per capita in infaltion-adjusted dollars\n\nThe second dataset is the meteorite landings from Tidytuesday project from the Meteoritical Society of NASA. It comes with the following variables:\n\nname: Meteorite name\nmass: Mass in grams\nlat: latitude\nlong: longitude\nfall: fall or found meteorite\n\n\n\nCode\npacman::p_load(\n    leaflet\n    ,gapminder\n    ,echarts4r\n    ,tidyverse\n    ,ggiraph\n    ,widgetframe\n    ,ggthemes\n    ,plotly\n    ,viridis\n    ,DT\n)\n\n\nFirst we organize the datasets:\n\n\nCode\n# 1. gapminder\n\n# country codes\ncodes &lt;- gapminder::country_codes\n# countries with info unfiltered version\ngapminder &lt;- gapminder::gapminder_unfiltered\n# join\ngapminder &lt;- gapminder %&gt;% left_join(codes) %&gt;%\n  mutate(code = iso_alpha)\n# a map of the world - Antarctica removed\nworld &lt;- map_data(\"world\") %&gt;%\n  filter(!grepl(\"antarctica\", region, ignore.case = T))\ngapminder_df &lt;- gapminder %&gt;%\n  inner_join(maps::iso3166 %&gt;% \n  select(a3, mapname), by = c(code = \"a3\")\n  ) %&gt;%\n    mutate(\n      mapname = str_remove(mapname, \"\\\\(.*\")\n    )\n\n\nGapminder dataset:\n\n\nCode\ndatatable(gapminder_df)\n\n\n\n\n\n\n\n\nCode\nmeteorites &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-06-11/meteorites.csv\")\n\n\nMeteorite dataset:\n\n\nCode\ndatatable(meteorites)"
  },
  {
    "objectID": "posts/2025-10-30-interactive-maps/interactive-maps.html#interactive-choropleth-map-with-ggiraph",
    "href": "posts/2025-10-30-interactive-maps/interactive-maps.html#interactive-choropleth-map-with-ggiraph",
    "title": "Interactive maps (R)",
    "section": "Interactive choropleth map with {ggiraph}",
    "text": "Interactive choropleth map with {ggiraph}\nTo turn a static choropleth map by invoking a tooltip when we hover the pointer over a country, we can use {ggiraph} - geom_polygon_interactive() & girafe and {widgetframe} - framewidget() packages.\nIt is useful creating a reusable theming function:\n\n\nCode\ntheme_helper &lt;- function(){\n   theme(\n    axis.line = element_blank(),\n    axis.text = element_blank(),\n    axis.title = element_blank(),\n    axis.ticks = element_blank(),\n    plot.background = element_rect(fill = \"snow\", color = NA),\n    panel.background = element_rect(fill= \"snow\", color = NA),\n    plot.title = element_text(size = 16, hjust = 0.5),\n    plot.subtitle = element_text(size = 12, hjust = 0.5),\n    plot.caption = element_text(size = 8, hjust = 1),\n    legend.title = element_text(color = \"grey40\", size = 8),\n    legend.text = element_text(color = \"grey40\", size = 7, hjust = 0),\n    legend.position = c(0.05, 0.25),\n    plot.margin = unit(c(0.5,2,0.5,1), \"cm\")) \n}\n\n\n\n\nCode\nlife_exp_map &lt;- gapminder_df %&gt;%\n  filter(year == 2007) %&gt;%\n  right_join(world, by = c(mapname = \"region\")) %&gt;%\n  ggplot() +\n  geom_polygon_interactive(\n    color = \"white\", size = 0.01, \n    aes(long, lat, group = group, fill = lifeExp,\n    tooltip = sprintf(\"%s&lt;br/&gt;%s\", country, lifeExp))\n  ) +\n    theme_void() + \n    scale_fill_viridis(option = \"B\") +\n    labs(\n      title = \"Life Expectancy\",\n      subtitle = \"Year: 2007\",\n      caption = \"Source: gapminder.org\",\n      fill = \"Years\"\n    ) \n\nlife_exp_map &lt;- life_exp_map + theme_helper() + coord_fixed(ratio = 1.3)\n\n\nPrint it interactively\n\nCode\nwidgetframe::frameWidget(girafe(code = print(life_exp_map)))"
  },
  {
    "objectID": "posts/2025-10-30-interactive-maps/interactive-maps.html#interactive-choropleth-maps-with-plotly",
    "href": "posts/2025-10-30-interactive-maps/interactive-maps.html#interactive-choropleth-maps-with-plotly",
    "title": "Interactive maps (R)",
    "section": "Interactive choropleth maps with {plotly}",
    "text": "Interactive choropleth maps with {plotly}\n{plotly} on top of tooltips also allows zooming, lasso/box selections or downloading the map as .png.\n\nCode\nlife_exp07 &lt;- gapminder_df %&gt;%\n  filter(year == 2007) %&gt;% \n  select(mapname, code, lifeExp)\n\np_07 &lt;- plot_geo(life_exp07)\n\np_07 &lt;- p_07 %&gt;% add_trace(\n  z = ~lifeExp, color = ~ lifeExp, colors = 'Oranges',\n  text = ~mapname, locations = ~ code\n) %&gt;% colorbar(title = \"Years\")\n\np_07 &lt;- p_07 %&gt;%\n  layout(\n    title = 'Life Expectancy in 2007 &lt;br&gt;Source:&lt;a href= \"https://www.gapminder.org\"&gt; gapminder.org&lt;/a&gt;', geo = p_07\n  )\n\np_07"
  },
  {
    "objectID": "posts/2025-10-30-interactive-maps/interactive-maps.html#interactive-points-using-plotly",
    "href": "posts/2025-10-30-interactive-maps/interactive-maps.html#interactive-points-using-plotly",
    "title": "Interactive maps (R)",
    "section": "Interactive points using {plotly}",
    "text": "Interactive points using {plotly}\n\n\nCode\nmeteorites_fell &lt;- meteorites %&gt;%\n  filter(fall == \"Fell\")\n\nmeteorites_map &lt;- list(\n  #scope = 'usa',\n  projection = list(type = 'Mercator'),\n  showland = TRUE,\n  landcolor = toRGB(\"grey80\")\n)\n\n\nmeteo_map &lt;- plot_geo(meteorites_fell, lat = ~lat, lon = ~long)\nmeteo_map &lt;- meteo_map %&gt;% add_markers(\n  text = ~paste(paste(\"Name:\", name), \n                paste(\"Year:\", year), \n                paste(\"Mass:\", mass), sep = \"&lt;br /&gt;\"),\n  color = ~mass, symbol = I(\"circle-dot\"), size = I(8), hoverinfo = \"text\"\n)\nmeteo_map &lt;- meteo_map %&gt;% colorbar(title = \"Mass\")\nmeteo_map &lt;- meteo_map %&gt;% layout(\n  title = 'Meteorite Landings&lt;br /&gt;(Meteorite falls)', geo = meteorites_map\n)\n\nmeteo_map"
  },
  {
    "objectID": "posts/2025-10-30-interactive-maps/interactive-maps.html#interactive-maps-with-echarts4r",
    "href": "posts/2025-10-30-interactive-maps/interactive-maps.html#interactive-maps-with-echarts4r",
    "title": "Interactive maps (R)",
    "section": "interactive maps with {echarts4r}",
    "text": "interactive maps with {echarts4r}\n\n\nCode\ndf &lt;- gapminder %&gt;% \n    mutate(Name = recode_factor(country,\n                              `Congo, Dem. Rep.`= \"Dem. Rep. Congo\",\n                              `Congo, Rep.`= \"Congo\",\n                              `Cote d'Ivoire`= \"Côte d'Ivoire\",\n                              `Central African Republic`= \"Central African Rep.\",\n                              `Yemen, Rep.`= \"Yemen\",\n                              `Korea, Rep.`= \"Korea\",\n                              `Korea, Dem. Rep.`= \"Dem. Rep. Korea\",\n                              `Czech Republic`= \"Czech Rep.\",\n                              `Slovak Republic`= \"Slovakia\",\n                              `Dominican Republic`= \"Dominican Rep.\",\n                              `Equatorial Guinea`= \"Eq. Guinea\"))\n\ndf %&gt;% group_by(year) %&gt;%\n  e_chart(Name, timeline = TRUE) %&gt;%\n  e_map(lifeExp) %&gt;%\n  e_visual_map(\n    min = 30, max = 90, type = \"piecewise\"\n  ) %&gt;%\n    e_title(\"Life expectancy by country and year\", left = \"center\") %&gt;%\n    e_tooltip(\n      trigger = \"item\", formatter = e_tooltip_choro_formatter()\n    )"
  },
  {
    "objectID": "posts/2026-01-12-Adaptive-designs/index.html",
    "href": "posts/2026-01-12-Adaptive-designs/index.html",
    "title": "Adaptive Methods for Clinical Trials",
    "section": "",
    "text": "Adaptive trials allow for modifications along the course of the trials. Common modifications include sample size adjustment, dropping an arm (if futile), or even adjusting allocation probabilities. Group sequential design is one of the most utilized forms of adaptive designs allowing for multiple planned interim analyses with pre-specified rules for early stopping. Here, we take a simulation approach to understand adaptive designs.\n\n\nCode\npacman::p_load(flextable, dplyr, future.apply, progressr)\n\n\n\n\nFor simplicity, we use 1:1 treatment allocation ratio using block randomisation of size 4. Uniform random numbers are used to to order treatment allocations thereby producing a sequential list of treatment allocations for consecutively recruited participants.\n\n\nCode\n#' Randomize participants to treatment arms\n#'\n#' @param n sample size\n#' @param blocks randomisation blocks\n#'\n#' @returns a dataframe\n#' @export\n#'\n#' @examples\nsimRandomisation &lt;- function(n=584, blocks = 4){\n  # blocking sequnce\n  block &lt;- rep(seq(1:n), each = blocks, length.out = n)\n  # treatment sequence\n  trt &lt;- rep(0:1, length.out = n)\n  # a random number on a unit interval\n  set.seed(as.numeric(Sys.Date()))\n  random &lt;- runif(n)\n  df &lt;- data.frame(block, trt, random)\n  df &lt;- df[order(df$block, df$random),]\n  df$obs &lt;- 1:n\n  df &lt;- df[, c(\"obs\", \"trt\")]\n  return(df)\n}\n\n\nIf we wanted to adjust allocation ratios:\n\n\nCode\n#' Randomize participants to treatment arms\n#'\n#' @param n sample size\n#' @param blocks randomisation blocks\n#' @param ratio randomization ratio\n#' \n#' @returns a dataframe\n#' @export\n#'\n#' @examples\nsimRandomisation &lt;- function(n = 584, blocks = 4, ratio = c(1, 1)) {\n\n  # 1. Define the possible treatment groups (0, 1, 2...) based on ratio length\n  trts &lt;- 0:(length(ratio) - 1)\n  \n  # 2. Create the sequence by sampling within each block\n  # We use 'replicate' to generate each block independently\n  num_blocks &lt;- ceiling(n / blocks)\n  \n  result &lt;- replicate(num_blocks, {\n    sample(trts, size = blocks, replace = TRUE, prob = ratio)\n  })\n  \n  # 3. Flatten the matrix into a vector and trim to length n\n  trt_vector &lt;- as.vector(result)[1:n]\n  \n  # 4. Return as a clean data frame\n  return(data.frame(obs = 1:n, trt = trt_vector))\n}\n\n\n\n\n\nWe assume participant accrual is constant over time and would take 928 days.\n\n\nCode\n#' Simulate participant accrual\n#'\n#' @param n sample size\n#' @param recruit_period accrual period in days\n#'\n#' @returns a vector\n#' @export\n#'\n#' @examples\nsimAccrual &lt;- function(n = 584, recruit_period = 928){\n  # simulate trial times that patient enters the trial\n  # adding 0.5 to ensure recruitment times &gt; day 1 when rounded\n  accrual_time &lt;- round(runif(n) * recruit_period + 0.5)\n  accrual_time &lt;- sort(accrual_time)\n  return(accrual_time)\n}\n\n\nIf patient accrual was not constant over time, we’d consider a beta distribution\n\n\nCode\n#' Simulate participant accrual\n#'\n#' @param n sample size\n#' @param recruit_period accrual period (days)\n#' @param alpha alpha shape parameter\n#' @param beta beta shape parameter\n#'\n#' @returnsa vector\n#' @export\n#'\n#' @examples\nsimAccrual &lt;- function(n = 584, recruit_period = 928, alpha = 1, beta = 1) {\n  \n  # 1. Generate random numbers using a Beta distribution\n  # rbeta(n, shape1, beta) returns values between 0 and 1\n  # alpha= beta = 1 is equivalent to runif (constant accrual)\n  # alpha= 2, beta = 1 creates a \"late peak\" (accelerating accrual)\n  # alpha= 1, beta = 2 creates an \"early peak\" (decelerating accrual)\n  # alpha= 2, beta = 2 creates a \"bell curve\" (peak in middle)\n  \n  accrual_proportions &lt;- rbeta(n, alpha, beta)\n  \n  # 2. Scale the 0-1 values to the actual recruit_period\n  accrual_time &lt;- round(accrual_proportions * recruit_period + 0.5)\n  \n  # 3. Sort to represent chronological entry\n  accrual_time &lt;- sort(accrual_time)\n  \n  return(accrual_time)\n}\n\n\n\n\n\nChoice of probability distribution depends on the outcome variable. You could use the binomial distribution for a binary outcome, normal distribution for a continuous outcome, or Weibull/exponential for time-to-event outcome.\n\n\nCode\n#' Simulate trial data with outcome variable\n#'\n#' @param n sample size\n#' @param recruit_period recruitment period (days)\n#' @param p vactor of event probabilities\n#'\n#' @returns a dataframe\n#' @export\n#'\n#' @examples\nsimTrialData &lt;- function(n = 584, recruit_period = 928, p){\n  # simulate random allocations\n  data &lt;- simRandomisation()\n  # simulate accrual\n  data$accrual &lt;- simAccrual()\n  # simulate events from binomial dist\n  data$event &lt;- rbinom(n, 1, p[data$trt+1])\n  \n  return(data)\n}\n\n\n\n\n\nWe assume outcome to be available immediately after recruitment and the time of interim analysis to be when a pre-determined number of events to be 20. As the data is ordered by accrual time, we can calculate cumulative number of events cum_events with cum_events=20 indicating the planned time of the interim analysis. Since we assume no time lag between recruitment and outcome assessment, the outcome at the interim (event_iterim: 0,1) would be the same as the outcome at final time point (event) for participants included in the interim analysis (i.e., for observations where obs &lt;= interim_ind)\n\n\nCode\n#' Identify interim data  \n#'\n#' @param data a dataframe\n#' @param events_at_interim number of events at interim\n#'\n#' @returns a dataframe\n#' @export\n#'\n#' @examples\nsimInterimData &lt;- function(data, events_at_interim = 20){\n  # cumulative events\n  data$cum_events &lt;- cumsum(data$event)\n  # when does interim occur\n  data$interim_ind &lt;- data$obs[min(which(data$cum_events == events_at_interim))]\n  # events at interim\n  data$event_interim &lt;- with(data, ifelse(obs &lt;= interim_ind, event, NA))\n  \n  return(data)\n}\n\n\n\n\n\nDuring interim analysis we seek to find out:\n\nif trial would have stopped before maximum recruitment\npoint estimates and their CIs for treatment effect\nsample size at the point of stopping the trial\nif the study would have found evidence of clinically relevant treatment effect or if it was a futile trial\n\n\n\nCode\n#' Analyze trial data\n#'\n#' @param data trial dataframe\n#' @param alpha_interim alpha spend at interim analysis\n#' @param alpha_final alpha spend at final analysis\n#'\n#' @returns a dataframe\n#' @export\n#'\n#' @examples\nanalyseData &lt;- function(data, alpha_interim, alpha_final){\n  nevents0 &lt;- sum(data$event[data$trt == 0])\n  nevents1 &lt;- sum(data$event[data$trt == 1])\n  pevents0 &lt;- nevents0/sum(data$trt ==0)\n  pevents1 &lt;- nevents1/sum(data$trt ==1)\n  \n  # logistic reg at interim\n  data$event_interim &lt;- factor(data$event_interim)\n  logit_interim &lt;- glm(event_interim ~ trt, data = data, family = \"binomial\")\n  conf_int &lt;- confint(logit_interim)\n  \n  # results at interim\n  \n  interim_or &lt;- exp(coef(logit_interim)[\"trt\"])\n  interim_lci &lt;- exp(conf_int[\"trt\", \"2.5 %\"])\n  interim_uci &lt;- exp(conf_int[\"trt\", \"97.5 %\"])\n  interim_p &lt;- coef(summary(logit_interim))[\"trt\", \"Pr(&gt;|z|)\"]\n  \n  # stop for trial success at interim\n  interim_stop &lt;- ifelse(interim_p &lt; alpha_interim, 1, 0)\n  \n  # the proportion of events if the trial stopped at interim\n  \n  if(interim_stop == 1){\n    nevents0 &lt;- sum(data$event[data$trt == 0 & !is.na(data$event_interim)])\n    nevents1 &lt;- sum(data$event[data$trt == 1 & !is.na(data$event_interim)])\n    pevents0 &lt;- nevents0/sum(data$trt == 0 & !is.na(data$event_interim))\n    pevents1 &lt;- nevents1/sum(data$trt == 1 & !is.na(data$event_interim))\n  }\n  # final analysis\n  data$event &lt;- factor(data$event)\n  logit &lt;- glm(event ~ trt, data = data, family = \"binomial\")\n  conf &lt;- confint(logit)\n  \n  # results at final analysis\n  final_or &lt;- exp(coef(logit)[\"trt\"])\n  final_lci &lt;- exp(conf[\"trt\", \"2.5 %\"])\n  final_uci &lt;- exp(conf[\"trt\", \"97.5 %\"])\n  final_p &lt;- coef(summary(logit))[\"trt\", \"Pr(&gt;|z|)\"]\n  \n  final_stop &lt;- ifelse(final_p &lt; alpha_final, 1, 0)\n  \n  # is the trial conclusive?\n  stop &lt;- ifelse(interim_stop == 1, interim_stop, final_stop)\n  \n  # sample size (used to determine average sample size)\n  \n  if(interim_stop == 1){\n    sample_size &lt;- unique(data$interim_ind)\n  } else{\n    sample_size &lt;- nrow(data)\n  }\n  \n  # determine trial flip-flop probability\n  flipflop &lt;- ifelse(interim_stop == 1 & final_stop == 0, 1, 0)\n  \n  results &lt;- data.frame(\n    nevents0, nevents1, pevents0, pevents1, sample_size,\n    interim_time = unique(data$interim_ind),\n    interim_or, interim_lci, interim_uci, interim_p, interim_stop,\n    final_or, final_lci, final_uci, final_p, final_stop, stop, flipflop\n  )\n  return(results)\n}\n\n\nAvoid in model.matrix.default(mt, mf, contrasts) : variable 1 has no levels errors\n\n\nCode\n#' Analyze trial data\n#'\n#' @param data trial dataframe\n#' @param alpha_interim alpha spend at interim analysis\n#' @param alpha_final alpha spend at final analysis\n#'\n#' @returns a dataframe\n#' @export\n#'\n#' @examples\n#' \n\n\nanalyseData &lt;- function(data, alpha_interim, alpha_final){\n  \n  # Helper to check if GLM can run (needs variation in both Y and X)\n  is_estimable &lt;- function(df, outcome_col) {\n    if (nrow(df) == 0) return(FALSE)\n    has_outcome_var &lt;- length(unique(df[[outcome_col]])) &gt; 1\n    has_trt_var     &lt;- length(unique(df$trt)) &gt; 1\n    return(has_outcome_var && has_trt_var)\n  }\n\n  # --- Initial Calculations (Total Trial potential) ---\n  nevents0 &lt;- sum(data$event[data$trt == 0], na.rm = TRUE)\n  nevents1 &lt;- sum(data$event[data$trt == 1], na.rm = TRUE)\n  pevents0 &lt;- nevents0 / sum(data$trt == 0)\n  pevents1 &lt;- nevents1 / sum(data$trt == 1)\n  \n  # --- Interim Analysis ---\n  # Filter data to only those available at interim for the model\n  interim_data &lt;- data[!is.na(data$event_interim), ]\n  interim_estimable &lt;- is_estimable(interim_data, \"event_interim\")\n  \n  if(interim_estimable){\n    logit_interim &lt;- glm(factor(event_interim) ~ trt, data = interim_data, family = \"binomial\")\n    interim_p     &lt;- coef(summary(logit_interim))[\"trt\", \"Pr(&gt;|z|)\"]\n    conf_int      &lt;- confint.default(logit_interim) \n    interim_or    &lt;- exp(coef(logit_interim)[\"trt\"])\n    interim_lci   &lt;- exp(conf_int[\"trt\", 1])\n    interim_uci   &lt;- exp(conf_int[\"trt\", 2])\n  } else {\n    interim_p &lt;- interim_or &lt;- interim_lci &lt;- interim_uci &lt;- NA\n  }\n  \n  # Stop for trial success at interim\n  interim_stop &lt;- ifelse(!is.na(interim_p) && interim_p &lt; alpha_interim, 1, 0)\n  \n  # Update event stats if the trial stopped at interim\n  if(interim_stop == 1){\n    nevents0 &lt;- sum(interim_data$event[interim_data$trt == 0])\n    nevents1 &lt;- sum(interim_data$event[interim_data$trt == 1])\n    pevents0 &lt;- nevents0 / sum(interim_data$trt == 0)\n    pevents1 &lt;- nevents1 / sum(interim_data$trt == 1)\n    sample_size &lt;- nrow(interim_data)\n  } else {\n    sample_size &lt;- nrow(data)\n  }\n  \n  # --- Final Analysis ---\n  final_estimable &lt;- is_estimable(data, \"event\")\n  \n  if(final_estimable){\n    logit    &lt;- glm(factor(event) ~ trt, data = data, family = \"binomial\")\n    final_p  &lt;- coef(summary(logit))[\"trt\", \"Pr(&gt;|z|)\"]\n    conf     &lt;- confint.default(logit)\n    final_or  &lt;- exp(coef(logit)[\"trt\"])\n    final_lci &lt;- exp(conf[\"trt\", 1])\n    final_uci &lt;- exp(conf[\"trt\", 2])\n  } else {\n    final_p &lt;- final_or &lt;- final_lci &lt;- final_uci &lt;- NA\n  }\n  \n  final_stop &lt;- ifelse(!is.na(final_p) && final_p &lt; alpha_final, 1, 0)\n  \n  # --- Logic Summary ---\n  stop     &lt;- ifelse(interim_stop == 1, 1, final_stop)\n  flipflop &lt;- ifelse(interim_stop == 1 & final_stop == 0, 1, 0)\n  \n  # Build the final data frame\n  results &lt;- data.frame(\n    nevents0, \n    nevents1, \n    pevents0, \n    pevents1, \n    sample_size,\n    interim_time = max(data$interim_ind, na.rm = TRUE), # Assuming this is a time-point or ID\n    interim_or, \n    interim_lci, \n    interim_uci, \n    interim_p, \n    interim_stop,\n    final_or, \n    final_lci, \n    final_uci, \n    final_p, \n    final_stop, \n    stop, \n    flipflop\n  )\n  \n  return(results)\n}\n\n\n\n\n\n\n\nCode\n#' Run a single experiment\n#'\n#' @param n sample size\n#' @param recruit_period accrual period\n#' @param p vector of treatment success probability\n#' @param events_at_interim number of events at interim\n#' @param alpha_interim interim alpha spend\n#' @param alpha_final final alpha spend\n#'\n#' @returns a dataframe\n#' @export\n#'\n#' @examples\nrunTrial &lt;- function(n, recruit_period, p, events_at_interim, alpha_interim, alpha_final){\n  #1. simulate trial data\n  df &lt;- simTrialData(n, recruit_period, p)\n  #2. create interim data\n  df &lt;- simInterimData(df, events_at_interim)\n  #3. Analyze data\n  results &lt;- analyseData(df, alpha_interim, alpha_final)\n  return(list(data = df, results = results))\n}\n\n\n\n\nCode\n# inputs\ninputs &lt;- data.frame(\n  seed = as.numeric(Sys.Date()),\n  # recruitment period = Days in 2.5 years   \n  #584/690 is the target/total possible = efficiency ratio\n  recruit_period = 365.25 * 3 * 584/ 690,\n  n = 584,\n  events_at_interim = 20,\n  p0 = 0.10,\n  p1 = 0.04,\n  # p = c(p0, p1)\n  alpha_final = 0.045,\n  alpha_interim = 0.005\n  \n)\np &lt;- c(inputs$p0, inputs$p1)\n\ndict &lt;- data.frame(\n  variable = c(\n    'nevents0','nevents1', 'pevents0', 'pevents1', 'sample_size', 'interim_time',\n    \"interim_or\", \"interim_lci\", \"interim_uci\", \"interim_p\", \"interim_stop\",\n    \"final_or\", \"final_lci\", \"final_uci\", \"final_p\", \"final_stop\", \"stop\",\n    \"flipflop\"\n  ),\n  label = c(\n    \"Number of events in control group\",\n    \"Number of events in experimental group\",\n    \"Proportion of events in control group\",\n    \"Proportion of events in experimental group\",\n    \"Sample size\",\n    \"Time (days) at interim\",\n    \"Odds ratio at interim\",\n    \"Lower CI for OR at interim\",\n    \"Upper CI for OR at interim\",\n    \"P-value of any difference in treatments at interim\",\n    \"Whether the trial would have stopped at interim\",\n    \"Odds ratio at final analysis\",\n    \"Lower CI for odds ratio\",\n    \"Upper CI for odds ratio\",\n    \"P-value for treatment difference at final analysis\",\n    \"Whether the trial is conclusive at final analysis\",\n    \"Whether the trial was conclusive (at final or interim)\",\n    \"The probability of trial flip-flopping\"\n    \n    \n  )\n)\n\n\n\n\nCode\nset.seed(inputs$seed)\n\nresults_singleTrial &lt;- runTrial(\n  inputs$n, inputs$recruit_period, p, inputs$events_at_interim,\n  inputs$alpha_interim, inputs$alpha_final\n)\nresults &lt;- results_singleTrial$results|&gt; round(3)  |&gt; t() |&gt; as.data.frame() \nnames(results)[1] &lt;- \"value\"\nresults$variable &lt;- rownames(results)\nrownames(results) &lt;- NULL\nresults &lt;- left_join(results, dict)\n\n\nJoining with `by = join_by(variable)`\n\n\nCode\nresults &lt;- results[, c(\"variable\", \"label\", \"value\")]\n\n\n\n\nCode\ntab_singleT &lt;- flextable(results) |&gt; theme_vanilla() \ntab_singleT\n\n\n\n\nTable 1: Results from a single simulated trial\n\n\n\nvariablelabelvaluenevents0Number of events in control group16.000nevents1Number of events in experimental group4.000pevents0Proportion of events in control group0.140pevents1Proportion of events in experimental group0.030sample_sizeSample size249.000interim_timeTime (days) at interim249.000interim_orOdds ratio at interim0.187interim_lciLower CI for OR at interim0.061interim_uciUpper CI for OR at interim0.577interim_pP-value of any difference in treatments at interim0.004interim_stopWhether the trial would have stopped at interim1.000final_orOdds ratio at final analysis0.305final_lciLower CI for odds ratio0.141final_uciUpper CI for odds ratio0.661final_pP-value for treatment difference at final analysis0.003final_stopWhether the trial is conclusive at final analysis1.000stopWhether the trial was conclusive (at final or interim)1.000flipflopThe probability of trial flip-flopping0.000\n\n\n\n\n\n\n\n\n\n\nCode\nrunMultipleTrials &lt;- function(nsims, seed, n, recruit_period, p, \n                              events_at_interim, alpha_interim, alpha_final){\n  # random seed\n  seeds &lt;- seed + seq(1: nsims)\n  # simulate\n  multiple_trials &lt;- lapply(1:nsims, function(x){\n    set.seed(seeds[x])\n    y &lt;- runTrial(n, recruit_period, p, events_at_interim, alpha_interim,\n                  alpha_final)\n    return(y)\n  })\n  \n  # summarise\n  results &lt;- lapply(multiple_trials, function(x) return(x$results))\n  results_all &lt;- do.call(rbind, results)\n  # saveRDS(multiple_trials, file = \"results_multTrials.rds\")\n  \n  # remove any trials with non-estimable CIs and calculate summary\n  x &lt;- which(apply(results_all, 1, function(x) any(is.na(x))))\n  if(length(x) &gt; 0){\n    result_summary &lt;- apply(results_all[-x,], 2, summary)\n  } else{\n    result_summary &lt;- apply(results_all, 2, summary)\n  }\n  \n  return(\n    list(\n      results_all = results_all,\n      results_summary = results_summary,\n      seeds = seeds\n    )\n  )\n}\n\n\nParallelize computations using future.apply}\n\n\nCode\nlibrary(future.apply)\nlibrary(progressr)\n\nrunMultipleTrials_Parallel &lt;- function(nsims, seed, n, recruit_period, p_param, \n                                       events_at_interim, alpha_interim, alpha_final) {\n  \n  # 1. Setup Parallel Backend\n  # Using 'multisession' is the most stable across Windows/Mac/Linux\n  plan(multisession, workers = parallelly::availableCores() - 1)\n  \n  # Ensure cleanup of workers when function finishes or crashes\n  on.exit(plan(sequential))\n  \n  seeds &lt;- seed + seq_len(nsims)\n  \n  # 2. Access the progressor from the calling environment\n  p &lt;- progressor(steps = nsims)\n  \n  # 3. Run Simulations\n  multiple_trials &lt;- future_lapply(1:nsims, function(i) {\n    # Increment progress bar\n    p(message = sprintf(\"Simulating trial %g\", i))\n    \n    set.seed(seeds[i])\n    \n    # Run the individual trial\n    # Note: I renamed your 'p' to 'p_param' to avoid conflict with progressor 'p'\n    y &lt;- runTrial(n, recruit_period, p_param, events_at_interim, alpha_interim, alpha_final)\n    \n    return(y)\n  }, future.seed = TRUE) \n  \n  # 4. Summarize Results\n  results &lt;- lapply(multiple_trials, function(x) x$results)\n  results_all &lt;- do.call(rbind, results)\n  \n  # Clean up non-estimable rows (NAs from glm failures)\n  results_clean &lt;- results_all[complete.cases(results_all), ]\n  result_summary &lt;- apply(results_clean, 2, summary)\n  \n  return(list(\n    results_all = results_all,\n    results_summary = result_summary,\n    seeds = seeds\n  ))\n}\n\n\nPrint results\n\n\nCode\ntab_mulT &lt;- results_mulT$results_summary |&gt; round(3) |&gt; t() |&gt; as.data.frame()\ntab_mulT$variable &lt;- rownames(tab_mulT)\nrownames(tab_mulT) &lt;- NULL\nnames(tab_mulT) &lt;- c(\"Minimum\", \"Q1\", \"Median\", \"Mean\", \"Q3\", \"Maximum\", \"variable\")\ntab_mulT &lt;- left_join(tab_mulT, dict) \n\n\nJoining with `by = join_by(variable)`\n\n\nCode\ntab_mulT &lt;- tab_mulT |&gt; select(variable, label, everything())\nnames(tab_mulT) &lt;- stringr::str_to_sentence(names(tab_mulT))\n\n\n\n\nCode\nt_mulT &lt;- flextable(tab_mulT) |&gt; theme_vanilla() \nt_mulT\n\n\n\n\nTable 2: Results from a multiple simulated trial\n\n\n\nVariableLabelMinimumQ1MedianMeanQ3Maximumnevents0Number of events in control group12.00024.00028.00027.58232.00047.000nevents1Number of events in experimental group1.0009.00011.00011.03714.00025.000pevents0Proportion of events in control group0.0430.0880.0990.1010.1120.254pevents1Proportion of events in experimental group0.0050.0300.0390.0390.0470.082sample_sizeSample size138.000584.000584.000554.664584.000584.000interim_timeTime (days) at interim116.000243.000281.000286.926327.000565.000interim_orOdds ratio at interim0.0000.2520.3680.4080.5181.626interim_lciLower CI for OR at interim0.0000.0830.1350.1520.2010.650interim_uciUpper CI for OR at interim0.3020.7601.009Inf1.340Infinterim_pP-value of any difference in treatments at interim0.0000.0140.0520.1410.1751.000interim_stopWhether the trial would have stopped at interim0.0000.0000.0000.0980.0001.000final_orOdds ratio at final analysis0.0590.2860.3710.3910.4731.304final_lciLower CI for odds ratio0.0140.1350.1840.1940.2400.651final_uciUpper CI for odds ratio0.2410.6060.7500.7930.9372.653final_pP-value for treatment difference at final analysis0.0000.0010.0060.0420.0320.994final_stopWhether the trial is conclusive at final analysis0.0001.0001.0000.8041.0001.000stopWhether the trial was conclusive (at final or interim)0.0001.0001.0000.8051.0001.000flipflopThe probability of trial flip-flopping0.0000.0000.0000.0010.0001.000"
  },
  {
    "objectID": "posts/2026-01-12-Adaptive-designs/index.html#randomisation",
    "href": "posts/2026-01-12-Adaptive-designs/index.html#randomisation",
    "title": "Adaptive Methods for Clinical Trials",
    "section": "",
    "text": "For simplicity, we use 1:1 treatment allocation ratio using block randomisation of size 4. Uniform random numbers are used to to order treatment allocations thereby producing a sequential list of treatment allocations for consecutively recruited participants.\n\n\nCode\n#' Randomize participants to treatment arms\n#'\n#' @param n sample size\n#' @param blocks randomisation blocks\n#'\n#' @returns a dataframe\n#' @export\n#'\n#' @examples\nsimRandomisation &lt;- function(n=584, blocks = 4){\n  # blocking sequnce\n  block &lt;- rep(seq(1:n), each = blocks, length.out = n)\n  # treatment sequence\n  trt &lt;- rep(0:1, length.out = n)\n  # a random number on a unit interval\n  set.seed(as.numeric(Sys.Date()))\n  random &lt;- runif(n)\n  df &lt;- data.frame(block, trt, random)\n  df &lt;- df[order(df$block, df$random),]\n  df$obs &lt;- 1:n\n  df &lt;- df[, c(\"obs\", \"trt\")]\n  return(df)\n}\n\n\nIf we wanted to adjust allocation ratios:\n\n\nCode\n#' Randomize participants to treatment arms\n#'\n#' @param n sample size\n#' @param blocks randomisation blocks\n#' @param ratio randomization ratio\n#' \n#' @returns a dataframe\n#' @export\n#'\n#' @examples\nsimRandomisation &lt;- function(n = 584, blocks = 4, ratio = c(1, 1)) {\n\n  # 1. Define the possible treatment groups (0, 1, 2...) based on ratio length\n  trts &lt;- 0:(length(ratio) - 1)\n  \n  # 2. Create the sequence by sampling within each block\n  # We use 'replicate' to generate each block independently\n  num_blocks &lt;- ceiling(n / blocks)\n  \n  result &lt;- replicate(num_blocks, {\n    sample(trts, size = blocks, replace = TRUE, prob = ratio)\n  })\n  \n  # 3. Flatten the matrix into a vector and trim to length n\n  trt_vector &lt;- as.vector(result)[1:n]\n  \n  # 4. Return as a clean data frame\n  return(data.frame(obs = 1:n, trt = trt_vector))\n}"
  },
  {
    "objectID": "posts/2026-01-12-Adaptive-designs/index.html#participant-accrual",
    "href": "posts/2026-01-12-Adaptive-designs/index.html#participant-accrual",
    "title": "Adaptive Methods for Clinical Trials",
    "section": "",
    "text": "We assume participant accrual is constant over time and would take 928 days.\n\n\nCode\n#' Simulate participant accrual\n#'\n#' @param n sample size\n#' @param recruit_period accrual period in days\n#'\n#' @returns a vector\n#' @export\n#'\n#' @examples\nsimAccrual &lt;- function(n = 584, recruit_period = 928){\n  # simulate trial times that patient enters the trial\n  # adding 0.5 to ensure recruitment times &gt; day 1 when rounded\n  accrual_time &lt;- round(runif(n) * recruit_period + 0.5)\n  accrual_time &lt;- sort(accrual_time)\n  return(accrual_time)\n}\n\n\nIf patient accrual was not constant over time, we’d consider a beta distribution\n\n\nCode\n#' Simulate participant accrual\n#'\n#' @param n sample size\n#' @param recruit_period accrual period (days)\n#' @param alpha alpha shape parameter\n#' @param beta beta shape parameter\n#'\n#' @returnsa vector\n#' @export\n#'\n#' @examples\nsimAccrual &lt;- function(n = 584, recruit_period = 928, alpha = 1, beta = 1) {\n  \n  # 1. Generate random numbers using a Beta distribution\n  # rbeta(n, shape1, beta) returns values between 0 and 1\n  # alpha= beta = 1 is equivalent to runif (constant accrual)\n  # alpha= 2, beta = 1 creates a \"late peak\" (accelerating accrual)\n  # alpha= 1, beta = 2 creates an \"early peak\" (decelerating accrual)\n  # alpha= 2, beta = 2 creates a \"bell curve\" (peak in middle)\n  \n  accrual_proportions &lt;- rbeta(n, alpha, beta)\n  \n  # 2. Scale the 0-1 values to the actual recruit_period\n  accrual_time &lt;- round(accrual_proportions * recruit_period + 0.5)\n  \n  # 3. Sort to represent chronological entry\n  accrual_time &lt;- sort(accrual_time)\n  \n  return(accrual_time)\n}"
  },
  {
    "objectID": "posts/2026-01-12-Adaptive-designs/index.html#generate-participant-outcomes",
    "href": "posts/2026-01-12-Adaptive-designs/index.html#generate-participant-outcomes",
    "title": "Adaptive Methods for Clinical Trials",
    "section": "",
    "text": "Choice of probability distribution depends on the outcome variable. You could use the binomial distribution for a binary outcome, normal distribution for a continuous outcome, or Weibull/exponential for time-to-event outcome.\n\n\nCode\n#' Simulate trial data with outcome variable\n#'\n#' @param n sample size\n#' @param recruit_period recruitment period (days)\n#' @param p vactor of event probabilities\n#'\n#' @returns a dataframe\n#' @export\n#'\n#' @examples\nsimTrialData &lt;- function(n = 584, recruit_period = 928, p){\n  # simulate random allocations\n  data &lt;- simRandomisation()\n  # simulate accrual\n  data$accrual &lt;- simAccrual()\n  # simulate events from binomial dist\n  data$event &lt;- rbinom(n, 1, p[data$trt+1])\n  \n  return(data)\n}"
  },
  {
    "objectID": "posts/2026-01-12-Adaptive-designs/index.html#identify-data-available-at-interim-analysis",
    "href": "posts/2026-01-12-Adaptive-designs/index.html#identify-data-available-at-interim-analysis",
    "title": "Adaptive Methods for Clinical Trials",
    "section": "",
    "text": "We assume outcome to be available immediately after recruitment and the time of interim analysis to be when a pre-determined number of events to be 20. As the data is ordered by accrual time, we can calculate cumulative number of events cum_events with cum_events=20 indicating the planned time of the interim analysis. Since we assume no time lag between recruitment and outcome assessment, the outcome at the interim (event_iterim: 0,1) would be the same as the outcome at final time point (event) for participants included in the interim analysis (i.e., for observations where obs &lt;= interim_ind)\n\n\nCode\n#' Identify interim data  \n#'\n#' @param data a dataframe\n#' @param events_at_interim number of events at interim\n#'\n#' @returns a dataframe\n#' @export\n#'\n#' @examples\nsimInterimData &lt;- function(data, events_at_interim = 20){\n  # cumulative events\n  data$cum_events &lt;- cumsum(data$event)\n  # when does interim occur\n  data$interim_ind &lt;- data$obs[min(which(data$cum_events == events_at_interim))]\n  # events at interim\n  data$event_interim &lt;- with(data, ifelse(obs &lt;= interim_ind, event, NA))\n  \n  return(data)\n}"
  },
  {
    "objectID": "posts/2026-01-12-Adaptive-designs/index.html#analyze-trial-data",
    "href": "posts/2026-01-12-Adaptive-designs/index.html#analyze-trial-data",
    "title": "Adaptive Methods for Clinical Trials",
    "section": "",
    "text": "During interim analysis we seek to find out:\n\nif trial would have stopped before maximum recruitment\npoint estimates and their CIs for treatment effect\nsample size at the point of stopping the trial\nif the study would have found evidence of clinically relevant treatment effect or if it was a futile trial\n\n\n\nCode\n#' Analyze trial data\n#'\n#' @param data trial dataframe\n#' @param alpha_interim alpha spend at interim analysis\n#' @param alpha_final alpha spend at final analysis\n#'\n#' @returns a dataframe\n#' @export\n#'\n#' @examples\nanalyseData &lt;- function(data, alpha_interim, alpha_final){\n  nevents0 &lt;- sum(data$event[data$trt == 0])\n  nevents1 &lt;- sum(data$event[data$trt == 1])\n  pevents0 &lt;- nevents0/sum(data$trt ==0)\n  pevents1 &lt;- nevents1/sum(data$trt ==1)\n  \n  # logistic reg at interim\n  data$event_interim &lt;- factor(data$event_interim)\n  logit_interim &lt;- glm(event_interim ~ trt, data = data, family = \"binomial\")\n  conf_int &lt;- confint(logit_interim)\n  \n  # results at interim\n  \n  interim_or &lt;- exp(coef(logit_interim)[\"trt\"])\n  interim_lci &lt;- exp(conf_int[\"trt\", \"2.5 %\"])\n  interim_uci &lt;- exp(conf_int[\"trt\", \"97.5 %\"])\n  interim_p &lt;- coef(summary(logit_interim))[\"trt\", \"Pr(&gt;|z|)\"]\n  \n  # stop for trial success at interim\n  interim_stop &lt;- ifelse(interim_p &lt; alpha_interim, 1, 0)\n  \n  # the proportion of events if the trial stopped at interim\n  \n  if(interim_stop == 1){\n    nevents0 &lt;- sum(data$event[data$trt == 0 & !is.na(data$event_interim)])\n    nevents1 &lt;- sum(data$event[data$trt == 1 & !is.na(data$event_interim)])\n    pevents0 &lt;- nevents0/sum(data$trt == 0 & !is.na(data$event_interim))\n    pevents1 &lt;- nevents1/sum(data$trt == 1 & !is.na(data$event_interim))\n  }\n  # final analysis\n  data$event &lt;- factor(data$event)\n  logit &lt;- glm(event ~ trt, data = data, family = \"binomial\")\n  conf &lt;- confint(logit)\n  \n  # results at final analysis\n  final_or &lt;- exp(coef(logit)[\"trt\"])\n  final_lci &lt;- exp(conf[\"trt\", \"2.5 %\"])\n  final_uci &lt;- exp(conf[\"trt\", \"97.5 %\"])\n  final_p &lt;- coef(summary(logit))[\"trt\", \"Pr(&gt;|z|)\"]\n  \n  final_stop &lt;- ifelse(final_p &lt; alpha_final, 1, 0)\n  \n  # is the trial conclusive?\n  stop &lt;- ifelse(interim_stop == 1, interim_stop, final_stop)\n  \n  # sample size (used to determine average sample size)\n  \n  if(interim_stop == 1){\n    sample_size &lt;- unique(data$interim_ind)\n  } else{\n    sample_size &lt;- nrow(data)\n  }\n  \n  # determine trial flip-flop probability\n  flipflop &lt;- ifelse(interim_stop == 1 & final_stop == 0, 1, 0)\n  \n  results &lt;- data.frame(\n    nevents0, nevents1, pevents0, pevents1, sample_size,\n    interim_time = unique(data$interim_ind),\n    interim_or, interim_lci, interim_uci, interim_p, interim_stop,\n    final_or, final_lci, final_uci, final_p, final_stop, stop, flipflop\n  )\n  return(results)\n}\n\n\nAvoid in model.matrix.default(mt, mf, contrasts) : variable 1 has no levels errors\n\n\nCode\n#' Analyze trial data\n#'\n#' @param data trial dataframe\n#' @param alpha_interim alpha spend at interim analysis\n#' @param alpha_final alpha spend at final analysis\n#'\n#' @returns a dataframe\n#' @export\n#'\n#' @examples\n#' \n\n\nanalyseData &lt;- function(data, alpha_interim, alpha_final){\n  \n  # Helper to check if GLM can run (needs variation in both Y and X)\n  is_estimable &lt;- function(df, outcome_col) {\n    if (nrow(df) == 0) return(FALSE)\n    has_outcome_var &lt;- length(unique(df[[outcome_col]])) &gt; 1\n    has_trt_var     &lt;- length(unique(df$trt)) &gt; 1\n    return(has_outcome_var && has_trt_var)\n  }\n\n  # --- Initial Calculations (Total Trial potential) ---\n  nevents0 &lt;- sum(data$event[data$trt == 0], na.rm = TRUE)\n  nevents1 &lt;- sum(data$event[data$trt == 1], na.rm = TRUE)\n  pevents0 &lt;- nevents0 / sum(data$trt == 0)\n  pevents1 &lt;- nevents1 / sum(data$trt == 1)\n  \n  # --- Interim Analysis ---\n  # Filter data to only those available at interim for the model\n  interim_data &lt;- data[!is.na(data$event_interim), ]\n  interim_estimable &lt;- is_estimable(interim_data, \"event_interim\")\n  \n  if(interim_estimable){\n    logit_interim &lt;- glm(factor(event_interim) ~ trt, data = interim_data, family = \"binomial\")\n    interim_p     &lt;- coef(summary(logit_interim))[\"trt\", \"Pr(&gt;|z|)\"]\n    conf_int      &lt;- confint.default(logit_interim) \n    interim_or    &lt;- exp(coef(logit_interim)[\"trt\"])\n    interim_lci   &lt;- exp(conf_int[\"trt\", 1])\n    interim_uci   &lt;- exp(conf_int[\"trt\", 2])\n  } else {\n    interim_p &lt;- interim_or &lt;- interim_lci &lt;- interim_uci &lt;- NA\n  }\n  \n  # Stop for trial success at interim\n  interim_stop &lt;- ifelse(!is.na(interim_p) && interim_p &lt; alpha_interim, 1, 0)\n  \n  # Update event stats if the trial stopped at interim\n  if(interim_stop == 1){\n    nevents0 &lt;- sum(interim_data$event[interim_data$trt == 0])\n    nevents1 &lt;- sum(interim_data$event[interim_data$trt == 1])\n    pevents0 &lt;- nevents0 / sum(interim_data$trt == 0)\n    pevents1 &lt;- nevents1 / sum(interim_data$trt == 1)\n    sample_size &lt;- nrow(interim_data)\n  } else {\n    sample_size &lt;- nrow(data)\n  }\n  \n  # --- Final Analysis ---\n  final_estimable &lt;- is_estimable(data, \"event\")\n  \n  if(final_estimable){\n    logit    &lt;- glm(factor(event) ~ trt, data = data, family = \"binomial\")\n    final_p  &lt;- coef(summary(logit))[\"trt\", \"Pr(&gt;|z|)\"]\n    conf     &lt;- confint.default(logit)\n    final_or  &lt;- exp(coef(logit)[\"trt\"])\n    final_lci &lt;- exp(conf[\"trt\", 1])\n    final_uci &lt;- exp(conf[\"trt\", 2])\n  } else {\n    final_p &lt;- final_or &lt;- final_lci &lt;- final_uci &lt;- NA\n  }\n  \n  final_stop &lt;- ifelse(!is.na(final_p) && final_p &lt; alpha_final, 1, 0)\n  \n  # --- Logic Summary ---\n  stop     &lt;- ifelse(interim_stop == 1, 1, final_stop)\n  flipflop &lt;- ifelse(interim_stop == 1 & final_stop == 0, 1, 0)\n  \n  # Build the final data frame\n  results &lt;- data.frame(\n    nevents0, \n    nevents1, \n    pevents0, \n    pevents1, \n    sample_size,\n    interim_time = max(data$interim_ind, na.rm = TRUE), # Assuming this is a time-point or ID\n    interim_or, \n    interim_lci, \n    interim_uci, \n    interim_p, \n    interim_stop,\n    final_or, \n    final_lci, \n    final_uci, \n    final_p, \n    final_stop, \n    stop, \n    flipflop\n  )\n  \n  return(results)\n}"
  },
  {
    "objectID": "posts/2026-01-12-Adaptive-designs/index.html#simulate-a-single-trial",
    "href": "posts/2026-01-12-Adaptive-designs/index.html#simulate-a-single-trial",
    "title": "Adaptive Methods for Clinical Trials",
    "section": "",
    "text": "Code\n#' Run a single experiment\n#'\n#' @param n sample size\n#' @param recruit_period accrual period\n#' @param p vector of treatment success probability\n#' @param events_at_interim number of events at interim\n#' @param alpha_interim interim alpha spend\n#' @param alpha_final final alpha spend\n#'\n#' @returns a dataframe\n#' @export\n#'\n#' @examples\nrunTrial &lt;- function(n, recruit_period, p, events_at_interim, alpha_interim, alpha_final){\n  #1. simulate trial data\n  df &lt;- simTrialData(n, recruit_period, p)\n  #2. create interim data\n  df &lt;- simInterimData(df, events_at_interim)\n  #3. Analyze data\n  results &lt;- analyseData(df, alpha_interim, alpha_final)\n  return(list(data = df, results = results))\n}\n\n\n\n\nCode\n# inputs\ninputs &lt;- data.frame(\n  seed = as.numeric(Sys.Date()),\n  # recruitment period = Days in 2.5 years   \n  #584/690 is the target/total possible = efficiency ratio\n  recruit_period = 365.25 * 3 * 584/ 690,\n  n = 584,\n  events_at_interim = 20,\n  p0 = 0.10,\n  p1 = 0.04,\n  # p = c(p0, p1)\n  alpha_final = 0.045,\n  alpha_interim = 0.005\n  \n)\np &lt;- c(inputs$p0, inputs$p1)\n\ndict &lt;- data.frame(\n  variable = c(\n    'nevents0','nevents1', 'pevents0', 'pevents1', 'sample_size', 'interim_time',\n    \"interim_or\", \"interim_lci\", \"interim_uci\", \"interim_p\", \"interim_stop\",\n    \"final_or\", \"final_lci\", \"final_uci\", \"final_p\", \"final_stop\", \"stop\",\n    \"flipflop\"\n  ),\n  label = c(\n    \"Number of events in control group\",\n    \"Number of events in experimental group\",\n    \"Proportion of events in control group\",\n    \"Proportion of events in experimental group\",\n    \"Sample size\",\n    \"Time (days) at interim\",\n    \"Odds ratio at interim\",\n    \"Lower CI for OR at interim\",\n    \"Upper CI for OR at interim\",\n    \"P-value of any difference in treatments at interim\",\n    \"Whether the trial would have stopped at interim\",\n    \"Odds ratio at final analysis\",\n    \"Lower CI for odds ratio\",\n    \"Upper CI for odds ratio\",\n    \"P-value for treatment difference at final analysis\",\n    \"Whether the trial is conclusive at final analysis\",\n    \"Whether the trial was conclusive (at final or interim)\",\n    \"The probability of trial flip-flopping\"\n    \n    \n  )\n)\n\n\n\n\nCode\nset.seed(inputs$seed)\n\nresults_singleTrial &lt;- runTrial(\n  inputs$n, inputs$recruit_period, p, inputs$events_at_interim,\n  inputs$alpha_interim, inputs$alpha_final\n)\nresults &lt;- results_singleTrial$results|&gt; round(3)  |&gt; t() |&gt; as.data.frame() \nnames(results)[1] &lt;- \"value\"\nresults$variable &lt;- rownames(results)\nrownames(results) &lt;- NULL\nresults &lt;- left_join(results, dict)\n\n\nJoining with `by = join_by(variable)`\n\n\nCode\nresults &lt;- results[, c(\"variable\", \"label\", \"value\")]\n\n\n\n\nCode\ntab_singleT &lt;- flextable(results) |&gt; theme_vanilla() \ntab_singleT\n\n\n\n\nTable 1: Results from a single simulated trial\n\n\n\nvariablelabelvaluenevents0Number of events in control group16.000nevents1Number of events in experimental group4.000pevents0Proportion of events in control group0.140pevents1Proportion of events in experimental group0.030sample_sizeSample size249.000interim_timeTime (days) at interim249.000interim_orOdds ratio at interim0.187interim_lciLower CI for OR at interim0.061interim_uciUpper CI for OR at interim0.577interim_pP-value of any difference in treatments at interim0.004interim_stopWhether the trial would have stopped at interim1.000final_orOdds ratio at final analysis0.305final_lciLower CI for odds ratio0.141final_uciUpper CI for odds ratio0.661final_pP-value for treatment difference at final analysis0.003final_stopWhether the trial is conclusive at final analysis1.000stopWhether the trial was conclusive (at final or interim)1.000flipflopThe probability of trial flip-flopping0.000"
  },
  {
    "objectID": "posts/2026-01-12-Adaptive-designs/index.html#simulate-multiple-trials",
    "href": "posts/2026-01-12-Adaptive-designs/index.html#simulate-multiple-trials",
    "title": "Adaptive Methods for Clinical Trials",
    "section": "",
    "text": "Code\nrunMultipleTrials &lt;- function(nsims, seed, n, recruit_period, p, \n                              events_at_interim, alpha_interim, alpha_final){\n  # random seed\n  seeds &lt;- seed + seq(1: nsims)\n  # simulate\n  multiple_trials &lt;- lapply(1:nsims, function(x){\n    set.seed(seeds[x])\n    y &lt;- runTrial(n, recruit_period, p, events_at_interim, alpha_interim,\n                  alpha_final)\n    return(y)\n  })\n  \n  # summarise\n  results &lt;- lapply(multiple_trials, function(x) return(x$results))\n  results_all &lt;- do.call(rbind, results)\n  # saveRDS(multiple_trials, file = \"results_multTrials.rds\")\n  \n  # remove any trials with non-estimable CIs and calculate summary\n  x &lt;- which(apply(results_all, 1, function(x) any(is.na(x))))\n  if(length(x) &gt; 0){\n    result_summary &lt;- apply(results_all[-x,], 2, summary)\n  } else{\n    result_summary &lt;- apply(results_all, 2, summary)\n  }\n  \n  return(\n    list(\n      results_all = results_all,\n      results_summary = results_summary,\n      seeds = seeds\n    )\n  )\n}\n\n\nParallelize computations using future.apply}\n\n\nCode\nlibrary(future.apply)\nlibrary(progressr)\n\nrunMultipleTrials_Parallel &lt;- function(nsims, seed, n, recruit_period, p_param, \n                                       events_at_interim, alpha_interim, alpha_final) {\n  \n  # 1. Setup Parallel Backend\n  # Using 'multisession' is the most stable across Windows/Mac/Linux\n  plan(multisession, workers = parallelly::availableCores() - 1)\n  \n  # Ensure cleanup of workers when function finishes or crashes\n  on.exit(plan(sequential))\n  \n  seeds &lt;- seed + seq_len(nsims)\n  \n  # 2. Access the progressor from the calling environment\n  p &lt;- progressor(steps = nsims)\n  \n  # 3. Run Simulations\n  multiple_trials &lt;- future_lapply(1:nsims, function(i) {\n    # Increment progress bar\n    p(message = sprintf(\"Simulating trial %g\", i))\n    \n    set.seed(seeds[i])\n    \n    # Run the individual trial\n    # Note: I renamed your 'p' to 'p_param' to avoid conflict with progressor 'p'\n    y &lt;- runTrial(n, recruit_period, p_param, events_at_interim, alpha_interim, alpha_final)\n    \n    return(y)\n  }, future.seed = TRUE) \n  \n  # 4. Summarize Results\n  results &lt;- lapply(multiple_trials, function(x) x$results)\n  results_all &lt;- do.call(rbind, results)\n  \n  # Clean up non-estimable rows (NAs from glm failures)\n  results_clean &lt;- results_all[complete.cases(results_all), ]\n  result_summary &lt;- apply(results_clean, 2, summary)\n  \n  return(list(\n    results_all = results_all,\n    results_summary = result_summary,\n    seeds = seeds\n  ))\n}\n\n\nPrint results\n\n\nCode\ntab_mulT &lt;- results_mulT$results_summary |&gt; round(3) |&gt; t() |&gt; as.data.frame()\ntab_mulT$variable &lt;- rownames(tab_mulT)\nrownames(tab_mulT) &lt;- NULL\nnames(tab_mulT) &lt;- c(\"Minimum\", \"Q1\", \"Median\", \"Mean\", \"Q3\", \"Maximum\", \"variable\")\ntab_mulT &lt;- left_join(tab_mulT, dict) \n\n\nJoining with `by = join_by(variable)`\n\n\nCode\ntab_mulT &lt;- tab_mulT |&gt; select(variable, label, everything())\nnames(tab_mulT) &lt;- stringr::str_to_sentence(names(tab_mulT))\n\n\n\n\nCode\nt_mulT &lt;- flextable(tab_mulT) |&gt; theme_vanilla() \nt_mulT\n\n\n\n\nTable 2: Results from a multiple simulated trial\n\n\n\nVariableLabelMinimumQ1MedianMeanQ3Maximumnevents0Number of events in control group12.00024.00028.00027.58232.00047.000nevents1Number of events in experimental group1.0009.00011.00011.03714.00025.000pevents0Proportion of events in control group0.0430.0880.0990.1010.1120.254pevents1Proportion of events in experimental group0.0050.0300.0390.0390.0470.082sample_sizeSample size138.000584.000584.000554.664584.000584.000interim_timeTime (days) at interim116.000243.000281.000286.926327.000565.000interim_orOdds ratio at interim0.0000.2520.3680.4080.5181.626interim_lciLower CI for OR at interim0.0000.0830.1350.1520.2010.650interim_uciUpper CI for OR at interim0.3020.7601.009Inf1.340Infinterim_pP-value of any difference in treatments at interim0.0000.0140.0520.1410.1751.000interim_stopWhether the trial would have stopped at interim0.0000.0000.0000.0980.0001.000final_orOdds ratio at final analysis0.0590.2860.3710.3910.4731.304final_lciLower CI for odds ratio0.0140.1350.1840.1940.2400.651final_uciUpper CI for odds ratio0.2410.6060.7500.7930.9372.653final_pP-value for treatment difference at final analysis0.0000.0010.0060.0420.0320.994final_stopWhether the trial is conclusive at final analysis0.0001.0001.0000.8041.0001.000stopWhether the trial was conclusive (at final or interim)0.0001.0001.0000.8051.0001.000flipflopThe probability of trial flip-flopping0.0000.0000.0000.0010.0001.000"
  },
  {
    "objectID": "posts/2026-01-15-survey-design/index.html",
    "href": "posts/2026-01-15-survey-design/index.html",
    "title": "Surveys: Design and Analysis",
    "section": "",
    "text": "Provided by {srvyrexploR} library. The first is America’s elections data.\n\n\nCode\ndata('anes_2020', package = \"srvyrexploR\")\nanes_2020 %&gt;% select(-matches(\"^V\\\\d\")) %&gt;%# starts with V followed by a digit\nglimpse()\n\n\nRows: 7,453\nColumns: 21\n$ CaseID                  &lt;dbl&gt; 200015, 200022, 200039, 200046, 200053, 200060…\n$ InterviewMode           &lt;fct&gt; Web, Web, Web, Web, Web, Web, Web, Web, Web, W…\n$ Weight                  &lt;dbl&gt; 1.0057375, 1.1634731, 0.7686811, 0.5210195, 0.…\n$ VarUnit                 &lt;fct&gt; 2, 2, 1, 2, 1, 2, 1, 2, 2, 2, 1, 1, 2, 2, 2, 2…\n$ Stratum                 &lt;fct&gt; 9, 26, 41, 29, 23, 37, 7, 37, 32, 41, 22, 7, 3…\n$ CampaignInterest        &lt;fct&gt; Somewhat interested, Not much interested, Some…\n$ EarlyVote2020           &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, Yes, NA, NA, N…\n$ VotedPres2016           &lt;fct&gt; Yes, Yes, Yes, Yes, Yes, No, Yes, No, Yes, Yes…\n$ VotedPres2016_selection &lt;fct&gt; Trump, Other, Clinton, Clinton, Trump, NA, Oth…\n$ PartyID                 &lt;fct&gt; Strong republican, Independent, Independent-de…\n$ TrustGovernment         &lt;fct&gt; Never, Never, Some of the time, About half the…\n$ TrustPeople             &lt;fct&gt; About half the time, Some of the time, Some of…\n$ Age                     &lt;dbl&gt; 46, 37, 40, 41, 72, 71, 37, 45, 70, 43, 37, 55…\n$ AgeGroup                &lt;fct&gt; 40-49, 30-39, 40-49, 40-49, 70 or older, 70 or…\n$ Education               &lt;fct&gt; Bachelor's, Post HS, High school, Post HS, Gra…\n$ RaceEth                 &lt;fct&gt; \"Hispanic\", \"Asian, NH/PI\", \"White\", \"Asian, N…\n$ Gender                  &lt;fct&gt; Male, Female, Female, Male, Male, Female, Fema…\n$ Income                  &lt;fct&gt; \"$175,000-249,999\", \"$70,000-74,999\", \"$100,00…\n$ Income7                 &lt;fct&gt; $125k or more, $60k to &lt; 80k, $100k to &lt; 125k,…\n$ VotedPres2020           &lt;fct&gt; NA, Yes, Yes, Yes, Yes, Yes, Yes, NA, Yes, Yes…\n$ VotedPres2020_selection &lt;fct&gt; NA, Other, Biden, Biden, Trump, Biden, Trump, …\n\n\n\n\nCode\nanes_2020 %&gt;% select(-matches(\"^V\\\\d\")) %&gt;% skimr::skim()\n\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n7453\n\n\nNumber of columns\n21\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n18\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nInterviewMode\n0\n1.00\nFALSE\n3\nWeb: 7064, Vid: 274, Tel: 115\n\n\nVarUnit\n0\n1.00\nFALSE\n3\n2: 3750, 1: 3689, 3: 14\n\n\nStratum\n0\n1.00\nFALSE\n50\n12: 179, 6: 172, 27: 172, 21: 170\n\n\nCampaignInterest\n1\n1.00\nFALSE\n3\nVer: 3940, Som: 2569, Not: 943\n\n\nEarlyVote2020\n6963\n0.07\nFALSE\n2\nYes: 375, No: 115\n\n\nVotedPres2016\n21\n1.00\nFALSE\n2\nYes: 5810, No: 1622\n\n\nVotedPres2016_selection\n1686\n0.77\nFALSE\n3\nCli: 2911, Tru: 2466, Oth: 390\n\n\nPartyID\n25\n1.00\nFALSE\n7\nStr: 1796, Str: 1545, Ind: 881, Ind: 876\n\n\nTrustGovernment\n29\n1.00\nFALSE\n5\nSom: 3313, Abo: 2313, Mos: 1016, Nev: 702\n\n\nTrustPeople\n13\n1.00\nFALSE\n5\nMos: 3511, Abo: 2020, Som: 1597, Nev: 264\n\n\nAgeGroup\n294\n0.96\nFALSE\n6\n60-: 1436, 70 : 1330, 30-: 1241, 50-: 1200\n\n\nEducation\n116\n0.98\nFALSE\n5\nPos: 2514, Bac: 1877, Gra: 1474, Hig: 1160\n\n\nRaceEth\n81\n0.99\nFALSE\n6\nWhi: 5420, His: 662, Bla: 650, Asi: 248\n\n\nGender\n51\n0.99\nFALSE\n2\nFem: 4027, Mal: 3375\n\n\nIncome\n517\n0.93\nFALSE\n22\nUnd: 647, $50: 485, $10: 451, $25: 405\n\n\nIncome7\n517\n0.93\nFALSE\n7\n$12: 1468, Und: 1076, $20: 1051, $40: 984\n\n\nVotedPres2020\n1053\n0.86\nFALSE\n2\nYes: 6313, No: 87\n\n\nVotedPres2020_selection\n1219\n0.84\nFALSE\n3\nBid: 3509, Tru: 2567, Oth: 158\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nCaseID\n0\n1.00\n336416.23\n103653.12\n200015.00\n225427.00\n335416.00\n427865.00\n535469.00\n▇▃▃▆▂\n\n\nWeight\n0\n1.00\n1.00\n1.02\n0.01\n0.39\n0.69\n1.21\n6.65\n▇▂▁▁▁\n\n\nAge\n294\n0.96\n51.83\n17.14\n18.00\n37.00\n53.00\n66.00\n80.00\n▅▇▇▇▇\n\n\n\n\n\nResidential energy consumption survey\n\n\nCode\ndata(\"recs_2020\", package = \"srvyrexploR\")\nrecs_2020 %&gt;% \n  select(-matches(\"^NWEIGHT\")) %&gt;% skim()\n\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n18496\n\n\nNumber of columns\n39\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nfactor\n10\n\n\nlogical\n2\n\n\nnumeric\n25\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nREGIONC\n0\n1\n4\n9\n0\n4\n0\n\n\nSTATE_FIPS\n0\n1\n2\n2\n0\n51\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nClimateRegion_BA\n0\n1.00\nFALSE\n8\nCol: 7116, Mix: 5579, Hot: 2545, Hot: 1577\n\n\nUrbanicity\n0\n1.00\nFALSE\n3\nUrb: 12395, Rur: 4081, Urb: 2020\n\n\nRegion\n0\n1.00\nFALSE\n4\nSou: 6426, Wes: 4581, Mid: 3832, Nor: 3657\n\n\nDivision\n0\n1.00\nFALSE\n10\nSou: 3256, Pac: 2497, Eas: 2014, Mid: 1977\n\n\nstate_postal\n0\n1.00\nFALSE\n51\nCA: 1152, TX: 1016, NY: 904, FL: 655\n\n\nstate_name\n0\n1.00\nFALSE\n51\nCal: 1152, Tex: 1016, New: 904, Flo: 655\n\n\nHousingUnitType\n0\n1.00\nFALSE\n5\nSin: 12319, Apa: 2439, Sin: 1751, Apa: 1013\n\n\nYearMade\n0\n1.00\nTRUE\n9\n197: 2817, 200: 2748, Bef: 2721, 199: 2451\n\n\nHeatingBehavior\n751\n0.96\nFALSE\n6\nSet: 7806, Man: 4654, Pro: 3310, Tur: 1491\n\n\nACBehavior\n2325\n0.87\nFALSE\n6\nSet: 6738, Man: 3637, Tur: 2746, Pro: 2638\n\n\n\nVariable type: logical\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\n\n\nSpaceHeatingUsed\n0\n1\n0.96\nTRU: 17745, FAL: 751\n\n\nACUsed\n0\n1\n0.87\nTRU: 16171, FAL: 2325\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nDOEID\n0\n1.00\n109248.50\n5339.48\n100001.00\n104624.75\n109248.50\n113872.25\n118496.00\n▇▇▇▇▇\n\n\nHDD65\n0\n1.00\n4271.81\n2329.46\n0.00\n2433.75\n4396.50\n5810.25\n17383.00\n▆▇▂▁▁\n\n\nCDD65\n0\n1.00\n1525.52\n1143.24\n0.00\n814.00\n1179.00\n1805.00\n5534.00\n▇▆▂▁▁\n\n\nHDD30YR\n0\n1.00\n4679.41\n2338.52\n0.00\n2897.75\n4825.00\n6290.00\n16071.00\n▅▇▃▁▁\n\n\nCDD30YR\n0\n1.00\n1309.88\n988.47\n0.00\n601.00\n1020.00\n1703.00\n4905.00\n▇▅▂▁▁\n\n\nTOTSQFT_EN\n0\n1.00\n1959.97\n1164.97\n200.00\n1100.00\n1700.00\n2510.00\n15000.00\n▇▁▁▁▁\n\n\nTOTHSQFT\n0\n1.00\n1744.46\n1106.33\n0.00\n1000.00\n1520.00\n2300.00\n15000.00\n▇▁▁▁▁\n\n\nTOTCSQFT\n0\n1.00\n1393.57\n1168.31\n0.00\n460.00\n1200.00\n2000.00\n14600.00\n▇▁▁▁▁\n\n\nWinterTempDay\n751\n0.96\n69.77\n3.62\n50.00\n68.00\n70.00\n72.00\n90.00\n▁▁▇▁▁\n\n\nWinterTempAway\n751\n0.96\n67.45\n5.02\n50.00\n65.00\n68.00\n70.00\n90.00\n▁▅▇▁▁\n\n\nWinterTempNight\n751\n0.96\n68.01\n4.66\n50.00\n65.00\n68.00\n70.00\n90.00\n▁▃▇▁▁\n\n\nSummerTempDay\n2325\n0.87\n72.01\n4.74\n50.00\n70.00\n72.00\n75.00\n90.00\n▁▁▇▃▁\n\n\nSummerTempAway\n2325\n0.87\n73.45\n5.68\n50.00\n70.00\n74.00\n78.00\n90.00\n▁▁▇▇▁\n\n\nSummerTempNight\n2325\n0.87\n71.22\n4.80\n50.00\n68.00\n72.00\n74.00\n90.00\n▁▂▇▃▁\n\n\nBTUEL\n0\n1.00\n37016.17\n24265.34\n143.32\n20205.76\n31890.03\n48297.98\n628155.47\n▇▁▁▁▁\n\n\nDOLLAREL\n0\n1.00\n1424.81\n862.08\n-889.48\n836.49\n1257.86\n1818.95\n15680.18\n▇▁▁▁▁\n\n\nBTUNG\n0\n1.00\n36960.50\n46538.32\n0.00\n0.00\n22011.97\n62714.20\n1134708.69\n▇▁▁▁▁\n\n\nDOLLARNG\n0\n1.00\n396.05\n483.41\n0.00\n0.00\n313.86\n644.93\n8154.98\n▇▁▁▁▁\n\n\nBTULP\n0\n1.00\n3916.95\n16741.93\n0.00\n0.00\n0.00\n0.00\n364215.39\n▇▁▁▁▁\n\n\nDOLLARLP\n0\n1.00\n80.89\n333.79\n0.00\n0.00\n0.00\n0.00\n6621.44\n▇▁▁▁▁\n\n\nBTUFO\n0\n1.00\n5108.60\n22698.98\n0.00\n0.00\n0.00\n0.00\n426268.50\n▇▁▁▁▁\n\n\nDOLLARFO\n0\n1.00\n88.43\n395.97\n0.00\n0.00\n0.00\n0.00\n7003.69\n▇▁▁▁▁\n\n\nBTUWOOD\n0\n1.00\n3596.49\n17766.37\n0.00\n0.00\n0.00\n0.00\n500000.00\n▇▁▁▁▁\n\n\nTOTALBTU\n0\n1.00\n83002.29\n53206.00\n1182.22\n45565.16\n74180.11\n108535.02\n1367548.13\n▇▁▁▁▁\n\n\nTOTALDOL\n0\n1.00\n1990.17\n1108.99\n-150.51\n1258.32\n1793.20\n2472.00\n20043.41\n▇▁▁▁▁\n\n\n\n\n\nNumeric vars\n\n\nCode\nrecs_2020 %&gt;% select(where(is.numeric)) %&gt;% colnames()\n\n\n [1] \"DOEID\"           \"HDD65\"           \"CDD65\"           \"HDD30YR\"        \n [5] \"CDD30YR\"         \"TOTSQFT_EN\"      \"TOTHSQFT\"        \"TOTCSQFT\"       \n [9] \"WinterTempDay\"   \"WinterTempAway\"  \"WinterTempNight\" \"SummerTempDay\"  \n[13] \"SummerTempAway\"  \"SummerTempNight\" \"NWEIGHT\"         \"NWEIGHT1\"       \n[17] \"NWEIGHT2\"        \"NWEIGHT3\"        \"NWEIGHT4\"        \"NWEIGHT5\"       \n[21] \"NWEIGHT6\"        \"NWEIGHT7\"        \"NWEIGHT8\"        \"NWEIGHT9\"       \n[25] \"NWEIGHT10\"       \"NWEIGHT11\"       \"NWEIGHT12\"       \"NWEIGHT13\"      \n[29] \"NWEIGHT14\"       \"NWEIGHT15\"       \"NWEIGHT16\"       \"NWEIGHT17\"      \n[33] \"NWEIGHT18\"       \"NWEIGHT19\"       \"NWEIGHT20\"       \"NWEIGHT21\"      \n[37] \"NWEIGHT22\"       \"NWEIGHT23\"       \"NWEIGHT24\"       \"NWEIGHT25\"      \n[41] \"NWEIGHT26\"       \"NWEIGHT27\"       \"NWEIGHT28\"       \"NWEIGHT29\"      \n[45] \"NWEIGHT30\"       \"NWEIGHT31\"       \"NWEIGHT32\"       \"NWEIGHT33\"      \n[49] \"NWEIGHT34\"       \"NWEIGHT35\"       \"NWEIGHT36\"       \"NWEIGHT37\"      \n[53] \"NWEIGHT38\"       \"NWEIGHT39\"       \"NWEIGHT40\"       \"NWEIGHT41\"      \n[57] \"NWEIGHT42\"       \"NWEIGHT43\"       \"NWEIGHT44\"       \"NWEIGHT45\"      \n[61] \"NWEIGHT46\"       \"NWEIGHT47\"       \"NWEIGHT48\"       \"NWEIGHT49\"      \n[65] \"NWEIGHT50\"       \"NWEIGHT51\"       \"NWEIGHT52\"       \"NWEIGHT53\"      \n[69] \"NWEIGHT54\"       \"NWEIGHT55\"       \"NWEIGHT56\"       \"NWEIGHT57\"      \n[73] \"NWEIGHT58\"       \"NWEIGHT59\"       \"NWEIGHT60\"       \"BTUEL\"          \n[77] \"DOLLAREL\"        \"BTUNG\"           \"DOLLARNG\"        \"BTULP\"          \n[81] \"DOLLARLP\"        \"BTUFO\"           \"DOLLARFO\"        \"BTUWOOD\"        \n[85] \"TOTALBTU\"        \"TOTALDOL\"       \n\n\nAmerican national election studies design object\n\n\nCode\ncps_state_in &lt;- getCensus(\n  name = \"cps/basic/mar\",\n  vintage = 2020,\n  region = \"state\",\n  vars = c(\n    \"HRMONTH\", \"HRYEAR4\",\n    \"PRTAGE\", \"PRCITSHP\", \"PWSSWGT\"\n  ),\n  key = Sys.getenv(\"CENSUS_KEY\")\n)\n\ncps_state &lt;- cps_state_in %&gt;%\n  as_tibble() %&gt;%\n  mutate(\n    across(\n      .cols = everything(),\n      .fns = as.numeric\n    )\n  )\n\n\n\n\nCode\ncps_narrow_resp &lt;- cps_state %&gt;%\n  filter(\n    PRTAGE &gt;= 18,\n    PRCITSHP %in% c(1:4)\n  )\n\n\nCalculate use population from the narrow data. Weights should add to total pop.\n\n\nCode\ntargetpop &lt;- cps_narrow_resp %&gt;% \n  pull(PWSSWGT) %&gt;%\n  sum()\nscales::comma(targetpop)\n\n\n[1] \"231,034,125\"\n\n\nWe can the weight the us election study appropriately\n\n\nCode\nanes_adjwgt &lt;- anes_2020 %&gt;%\n  mutate(\n    weight = V200010b / sum(V200010b) * targetpop\n  )\n\n\nWe then make it conform to a survey design\n\n\nCode\nanes_des &lt;- anes_adjwgt %&gt;%\n  as_survey_design(\n    weights = weight,\n    strata = V200010d,\n    ids = V200010c,\n    nest = TRUE\n  )\nanes_des\n\n\nStratified 1 - level Cluster Sampling design (with replacement)\nWith (101) clusters.\nCalled via srvyr\nSampling variables:\n  - ids: V200010c \n  - strata: V200010d \n  - weights: weight \nData variables: \n  - V200001 (dbl), CaseID (dbl), V200002 (hvn_lbll), InterviewMode (fct),\n    V200010b (dbl), Weight (dbl), V200010c (dbl), VarUnit (fct), V200010d\n    (dbl), Stratum (fct), V201006 (hvn_lbll), CampaignInterest (fct), V201023\n    (hvn_lbll), EarlyVote2020 (fct), V201024 (hvn_lbll), V201025x (hvn_lbll),\n    V201028 (hvn_lbll), V201029 (hvn_lbll), V201101 (hvn_lbll), V201102\n    (hvn_lbll), VotedPres2016 (fct), V201103 (hvn_lbll),\n    VotedPres2016_selection (fct), V201228 (hvn_lbll), V201229 (hvn_lbll),\n    V201230 (hvn_lbll), V201231x (hvn_lbll), PartyID (fct), V201233 (hvn_lbll),\n    TrustGovernment (fct), V201237 (hvn_lbll), TrustPeople (fct), V201507x\n    (hvn_lbll), Age (dbl), AgeGroup (fct), V201510 (hvn_lbll), Education (fct),\n    V201546 (hvn_lbll), V201547a (hvn_lbll), V201547b (hvn_lbll), V201547c\n    (hvn_lbll), V201547d (hvn_lbll), V201547e (hvn_lbll), V201547z (hvn_lbll),\n    V201549x (hvn_lbll), RaceEth (fct), V201600 (hvn_lbll), Gender (fct),\n    V201607 (hvn_lbll), V201610 (hvn_lbll), V201611 (hvn_lbll), V201613\n    (hvn_lbll), V201615 (hvn_lbll), V201616 (hvn_lbll), V201617x (hvn_lbll),\n    Income (fct), Income7 (fct), V202051 (hvn_lbll), V202066 (hvn_lbll),\n    V202072 (hvn_lbll), VotedPres2020 (fct), V202073 (hvn_lbll), V202109x\n    (hvn_lbll), V202110x (hvn_lbll), VotedPres2020_selection (fct), weight\n    (dbl)\n\n\nUsing replicated weights\n\n\nCode\nrecs_des &lt;- recs_2020 %&gt;%\n  as_survey_rep(\n    weights = NWEIGHT,\n    repweights = NWEIGHT1:NWEIGHT60,\n    type = \"JK1\",\n    scale = 59 / 60,\n    mse = TRUE\n  )\n\nrecs_des\n\n\nCall: Called via srvyr\nUnstratified cluster jacknife (JK1) with 60 replicates and MSE variances.\nSampling variables:\n  - repweights: `NWEIGHT1 + NWEIGHT2 + NWEIGHT3 + NWEIGHT4 + NWEIGHT5 +\n    NWEIGHT6 + NWEIGHT7 + NWEIGHT8 + NWEIGHT9 + NWEIGHT10 + NWEIGHT11 +\n    NWEIGHT12 + NWEIGHT13 + NWEIGHT14 + NWEIGHT15 + NWEIGHT16 + NWEIGHT17 +\n    NWEIGHT18 + NWEIGHT19 + NWEIGHT20 + NWEIGHT21 + NWEIGHT22 + NWEIGHT23 +\n    NWEIGHT24 + NWEIGHT25 + NWEIGHT26 + NWEIGHT27 + NWEIGHT28 + NWEIGHT29 +\n    NWEIGHT30 + NWEIGHT31 + NWEIGHT32 + NWEIGHT33 + NWEIGHT34 + NWEIGHT35 +\n    NWEIGHT36 + NWEIGHT37 + NWEIGHT38 + NWEIGHT39 + NWEIGHT40 + NWEIGHT41 +\n    NWEIGHT42 + NWEIGHT43 + NWEIGHT44 + NWEIGHT45 + NWEIGHT46 + NWEIGHT47 +\n    NWEIGHT48 + NWEIGHT49 + NWEIGHT50 + NWEIGHT51 + NWEIGHT52 + NWEIGHT53 +\n    NWEIGHT54 + NWEIGHT55 + NWEIGHT56 + NWEIGHT57 + NWEIGHT58 + NWEIGHT59 +\n    NWEIGHT60` \n  - weights: NWEIGHT \nData variables: \n  - DOEID (dbl), ClimateRegion_BA (fct), Urbanicity (fct), Region (fct),\n    REGIONC (chr), Division (fct), STATE_FIPS (chr), state_postal (fct),\n    state_name (fct), HDD65 (dbl), CDD65 (dbl), HDD30YR (dbl), CDD30YR (dbl),\n    HousingUnitType (fct), YearMade (ord), TOTSQFT_EN (dbl), TOTHSQFT (dbl),\n    TOTCSQFT (dbl), SpaceHeatingUsed (lgl), ACUsed (lgl), HeatingBehavior\n    (fct), WinterTempDay (dbl), WinterTempAway (dbl), WinterTempNight (dbl),\n    ACBehavior (fct), SummerTempDay (dbl), SummerTempAway (dbl),\n    SummerTempNight (dbl), NWEIGHT (dbl), NWEIGHT1 (dbl), NWEIGHT2 (dbl),\n    NWEIGHT3 (dbl), NWEIGHT4 (dbl), NWEIGHT5 (dbl), NWEIGHT6 (dbl), NWEIGHT7\n    (dbl), NWEIGHT8 (dbl), NWEIGHT9 (dbl), NWEIGHT10 (dbl), NWEIGHT11 (dbl),\n    NWEIGHT12 (dbl), NWEIGHT13 (dbl), NWEIGHT14 (dbl), NWEIGHT15 (dbl),\n    NWEIGHT16 (dbl), NWEIGHT17 (dbl), NWEIGHT18 (dbl), NWEIGHT19 (dbl),\n    NWEIGHT20 (dbl), NWEIGHT21 (dbl), NWEIGHT22 (dbl), NWEIGHT23 (dbl),\n    NWEIGHT24 (dbl), NWEIGHT25 (dbl), NWEIGHT26 (dbl), NWEIGHT27 (dbl),\n    NWEIGHT28 (dbl), NWEIGHT29 (dbl), NWEIGHT30 (dbl), NWEIGHT31 (dbl),\n    NWEIGHT32 (dbl), NWEIGHT33 (dbl), NWEIGHT34 (dbl), NWEIGHT35 (dbl),\n    NWEIGHT36 (dbl), NWEIGHT37 (dbl), NWEIGHT38 (dbl), NWEIGHT39 (dbl),\n    NWEIGHT40 (dbl), NWEIGHT41 (dbl), NWEIGHT42 (dbl), NWEIGHT43 (dbl),\n    NWEIGHT44 (dbl), NWEIGHT45 (dbl), NWEIGHT46 (dbl), NWEIGHT47 (dbl),\n    NWEIGHT48 (dbl), NWEIGHT49 (dbl), NWEIGHT50 (dbl), NWEIGHT51 (dbl),\n    NWEIGHT52 (dbl), NWEIGHT53 (dbl), NWEIGHT54 (dbl), NWEIGHT55 (dbl),\n    NWEIGHT56 (dbl), NWEIGHT57 (dbl), NWEIGHT58 (dbl), NWEIGHT59 (dbl),\n    NWEIGHT60 (dbl), BTUEL (dbl), DOLLAREL (dbl), BTUNG (dbl), DOLLARNG (dbl),\n    BTULP (dbl), DOLLARLP (dbl), BTUFO (dbl), DOLLARFO (dbl), BTUWOOD (dbl),\n    TOTALBTU (dbl), TOTALDOL (dbl)\n\n\n\n\n\n\nCount observations with survey_count() or survey_tally()\nSum variables with survey_total()\nMeans and proportions: survey_mean(), survey_prop()\nQuantiles and medians: survey_quantile(), survey_median()\nCorrelations: survey_cor()\nRatios: survey_ratio()\nVariances and standard deviation: survey_var(), survey_sd()\n\n\n\n\n\nCode\nrecs_des %&gt;% survey_tally()\n\n\n# A tibble: 1 × 2\n           n  n_se\n       &lt;dbl&gt; &lt;dbl&gt;\n1 123529025. 0.148\n\n\nEstimated counts by subgroups\n\n\nCode\nrecs_des %&gt;% survey_count(Region, Division, name = \"N\")\n\n\n# A tibble: 10 × 4\n   Region    Division                   N         N_se\n   &lt;fct&gt;     &lt;fct&gt;                  &lt;dbl&gt;        &lt;dbl&gt;\n 1 Northeast New England         5876166  0.0000000137\n 2 Northeast Middle Atlantic    16043503  0.0000000487\n 3 Midwest   East North Central 18546912  0.000000437 \n 4 Midwest   West North Central  8495815  0.0000000177\n 5 South     South Atlantic     24843261  0.0000000418\n 6 South     East South Central  7380717. 0.114       \n 7 South     West South Central 14619094  0.000488    \n 8 West      Mountain North      4615844  0.119       \n 9 West      Mountain South      4602070  0.0000000492\n10 West      Pacific            18505643. 0.00000295  \n\n\nTo achieve same result by survey_tally(), you first group\n\n\nCode\nrecs_des %&gt;% group_by(Region, Division) %&gt;% survey_tally(name = \"N\")\n\n\n# A tibble: 10 × 4\n# Groups:   Region [4]\n   Region    Division                   N         N_se\n   &lt;fct&gt;     &lt;fct&gt;                  &lt;dbl&gt;        &lt;dbl&gt;\n 1 Northeast New England         5876166  0.0000000137\n 2 Northeast Middle Atlantic    16043503  0.0000000487\n 3 Midwest   East North Central 18546912  0.000000437 \n 4 Midwest   West North Central  8495815  0.0000000177\n 5 South     South Atlantic     24843261  0.0000000418\n 6 South     East South Central  7380717. 0.114       \n 7 South     West South Central 14619094  0.000488    \n 8 West      Mountain North      4615844  0.119       \n 9 West      Mountain South      4602070  0.0000000492\n10 West      Pacific            18505643. 0.00000295  \n\n\n\n\n\nTo get population count estimate, we leave argument x, empty.\n\n\nCode\nrecs_des %&gt;% \n  summarize(\n    tot = survey_total()\n  )\n\n\n# A tibble: 1 × 2\n         tot tot_se\n       &lt;dbl&gt;  &lt;dbl&gt;\n1 123529025.  0.148\n\n\nOverall summation of a continuous variable\n\n\nCode\nrecs_des %&gt;%\n  summarize(elec_bill = survey_total(DOLLAREL))\n\n\n# A tibble: 1 × 2\n      elec_bill elec_bill_se\n          &lt;dbl&gt;        &lt;dbl&gt;\n1 170473527909.   664893504.\n\n\nSummation by groups\n\n\nCode\nrecs_des %&gt;% group_by(Region) %&gt;%\n  summarize(\n  elec_bill = survey_total(DOLLAREL, vartype = \"ci\")\n  \n)\n\n\n# A tibble: 4 × 4\n  Region       elec_bill elec_bill_low elec_bill_upp\n  &lt;fct&gt;            &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n1 Northeast 29430369947.  28788987554.  30071752341.\n2 Midwest   34972544751.  34339576041.  35605513460.\n3 South     72496840204.  71534780902.  73458899506.\n4 West      33573773008.  32909111702.  34238434313.\n\n\nYou can supply .by argument inside summarize\n\n\nCode\nrecs_des  %&gt;%\n  summarize(\n  elec_bill = survey_total(DOLLAREL, vartype = \"ci\"),\n  .by = Region\n)\n\n\n# A tibble: 4 × 4\n  Region       elec_bill elec_bill_low elec_bill_upp\n  &lt;fct&gt;            &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n1 Northeast 29430369947.  28788987554.  30071752341.\n2 Midwest   34972544751.  34339576041.  35605513460.\n3 South     72496840204.  71534780902.  73458899506.\n4 West      33573773008.  32909111702.  34238434313.\n\n\n\n\n\n\n\nCode\nrecs_des %&gt;% group_by(Region, ACUsed) %&gt;%\n  summarise(\n  p = survey_prop()\n)\n\n\nWhen `proportion` is unspecified, `survey_prop()` now defaults to `proportion = TRUE`.\nℹ This should improve confidence interval coverage.\nThis message is displayed once per session.\n\n\n# A tibble: 8 × 4\n# Groups:   Region [4]\n  Region    ACUsed      p    p_se\n  &lt;fct&gt;     &lt;lgl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 Northeast FALSE  0.110  0.00590\n2 Northeast TRUE   0.890  0.00590\n3 Midwest   FALSE  0.0666 0.00508\n4 Midwest   TRUE   0.933  0.00508\n5 South     FALSE  0.0581 0.00278\n6 South     TRUE   0.942  0.00278\n7 West      FALSE  0.255  0.00759\n8 West      TRUE   0.745  0.00759\n\n\nJoint proportions using interact\n\n\nCode\nrecs_des %&gt;% \n  group_by(\n    interact(Region, ACUsed)\n  ) %&gt;% \n  summarize(\n    p = survey_prop() \n  ) %&gt;% mutate(\n    p = scales::percent(p)\n  )\n\n\n# A tibble: 8 × 4\n  Region    ACUsed p         p_se\n  &lt;fct&gt;     &lt;lgl&gt;  &lt;chr&gt;    &lt;dbl&gt;\n1 Northeast FALSE  1.96%  0.00105\n2 Northeast TRUE   15.79% 0.00105\n3 Midwest   FALSE  1.46%  0.00111\n4 Midwest   TRUE   20.43% 0.00111\n5 South     FALSE  2.20%  0.00106\n6 South     TRUE   35.72% 0.00106\n7 West      FALSE  5.73%  0.00170\n8 West      TRUE   16.72% 0.00170\n\n\nOverall mean\n\n\nCode\nrecs_des %&gt;% summarize(\n  elec_bill = survey_mean(DOLLAREL, vartype = c(\"se\", \"ci\"))\n)\n\n\n# A tibble: 1 × 4\n  elec_bill elec_bill_se elec_bill_low elec_bill_upp\n      &lt;dbl&gt;        &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n1     1380.         5.38         1369.         1391.\n\n\n\n\n\nOverall quantiles\n\n\nCode\nrecs_des %&gt;%\n  summarize(\n    elec_bill = survey_quantile(DOLLAREL, quantiles = c(.25, .5, .75))\n  )\n\n\n# A tibble: 1 × 6\n  elec_bill_q25 elec_bill_q50 elec_bill_q75 elec_bill_q25_se elec_bill_q50_se\n          &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n1          795.         1215.         1770.             5.69             6.33\n# ℹ 1 more variable: elec_bill_q75_se &lt;dbl&gt;\n\n\nOverall median\n\n\nCode\nrecs_des %&gt;%\n  summarize(\n    elec_bill = survey_median(DOLLAREL)\n  )\n\n\n# A tibble: 1 × 2\n  elec_bill elec_bill_se\n      &lt;dbl&gt;        &lt;dbl&gt;\n1     1215.         6.33\n\n\nCorrelations\n\n\nCode\nrecs_des %&gt;%\n  summarize(SQFT_Elec_Corr = survey_corr(TOTSQFT_EN, BTUEL))\n\n\nWarning: There was 1 warning in `dplyr::summarise()`.\nℹ In argument: `SQFT_Elec_Corr = survey_corr(TOTSQFT_EN, BTUEL)`.\nCaused by warning in `sweep()`:\n! length(STATS) or dim(STATS) do not match dim(x)[MARGIN]\n\n\n# A tibble: 1 × 2\n  SQFT_Elec_Corr SQFT_Elec_Corr_se\n           &lt;dbl&gt;             &lt;dbl&gt;\n1          0.417           0.00689\n\n\n\n\nDesign effect measures the precision of an estimate under a particular sampling design relative to a simple random sampling design. If &lt;1, the design is statistically more efficient than SRS. It is used in the calculation of effective sample size - the sample size needed if we were to employ SRS\n\\[n_{eff} = \\frac{n}{D_{eff}}\\]\n\n\nCode\nrecs_des %&gt;%\n  summarize(\n    across(\n          c(BTUEL, BTUNG, BTULP, BTUFO, BTUWOOD),\n          ~ survey_mean(.x, deff = TRUE, vartype = NULL)\n    )\n  ) %&gt;% select(ends_with(\"deff\"))\n\n\n# A tibble: 1 × 5\n  BTUEL_deff BTUNG_deff BTULP_deff BTUFO_deff BTUWOOD_deff\n       &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;\n1      0.597      0.938       1.21      0.720         1.10\n\n\n\n\n\n\n\nCode\nrecs_des %&gt;%\n  cascade(DOLLAREL_mn = survey_mean(DOLLAREL))\n\n\n# A tibble: 1 × 2\n  DOLLAREL_mn DOLLAREL_mn_se\n        &lt;dbl&gt;          &lt;dbl&gt;\n1       1380.           5.38\n\n\nGroup\n\n\nCode\nrecs_des %&gt;%\n  group_by(Region) %&gt;%\n  cascade(DOLLAREL_mn = survey_mean(DOLLAREL), .fill = \"National\")\n\n\n# A tibble: 5 × 3\n  Region    DOLLAREL_mn DOLLAREL_mn_se\n  &lt;fct&gt;           &lt;dbl&gt;          &lt;dbl&gt;\n1 Northeast       1343.          14.6 \n2 Midwest         1293.          11.7 \n3 South           1548.          10.3 \n4 West            1211.          12.0 \n5 National        1380.           5.38\n\n\nOr through a long dplyr pipeline\n\n\nCode\nrecs_des %&gt;% \n  summarize(\n    cons_mean = survey_mean(DOLLAREL),\n    .by = Region\n  ) %&gt;% bind_rows(\n    ., recs_des %&gt;%\n      summarize(\n        cons_mean = survey_mean(DOLLAREL)\n      ) %&gt;% mutate(\n        ., Region = \"National\"\n      ) %&gt;% select(Region, everything())\n  )\n\n\n# A tibble: 5 × 3\n  Region    cons_mean cons_mean_se\n  &lt;chr&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1 Northeast     1343.        14.6 \n2 Midwest       1293.        11.7 \n3 South         1548.        10.3 \n4 West          1211.        12.0 \n5 National      1380.         5.38\n\n\n\n\n\n\n\nCode\nrecs_des %&gt;% \n  summarize(\n    across(\n      starts_with(\"BTU\"),\n      list(\n        Total = ~ survey_total(.x, vartype = \"cv\"),\n        Mean = ~ survey_mean(.x, vartype = \"cv\")\n      ),\n      .unpack = \"{outer}.{inner}\"\n    )\n  )\n\n\n# A tibble: 1 × 20\n  BTUEL_Total.coef BTUEL_Total._cv BTUEL_Mean.coef BTUEL_Mean._cv\n             &lt;dbl&gt;           &lt;dbl&gt;           &lt;dbl&gt;          &lt;dbl&gt;\n1    4453284510065         0.00377          36051.        0.00377\n# ℹ 16 more variables: BTUNG_Total.coef &lt;dbl&gt;, BTUNG_Total._cv &lt;dbl&gt;,\n#   BTUNG_Mean.coef &lt;dbl&gt;, BTUNG_Mean._cv &lt;dbl&gt;, BTULP_Total.coef &lt;dbl&gt;,\n#   BTULP_Total._cv &lt;dbl&gt;, BTULP_Mean.coef &lt;dbl&gt;, BTULP_Mean._cv &lt;dbl&gt;,\n#   BTUFO_Total.coef &lt;dbl&gt;, BTUFO_Total._cv &lt;dbl&gt;, BTUFO_Mean.coef &lt;dbl&gt;,\n#   BTUFO_Mean._cv &lt;dbl&gt;, BTUWOOD_Total.coef &lt;dbl&gt;, BTUWOOD_Total._cv &lt;dbl&gt;,\n#   BTUWOOD_Mean.coef &lt;dbl&gt;, BTUWOOD_Mean._cv &lt;dbl&gt;\n\n\n\n\n\n\n\nCode\ncalc &lt;- function(df, var) {\n  df %&gt;%\n    drop_na(!!sym(var)) %&gt;%\n    group_by(!!sym(var)) %&gt;%\n    summarize(p = survey_prop()) %&gt;%\n    mutate(Variable = var) %&gt;%\n    rename(Answer := !!sym(var)) %&gt;%\n    mutate(p = scales::percent(p)) %&gt;%\n    select(Variable, everything())\n}\n\nv &lt;- c(\"TrustGovernment\", \"TrustPeople\")\nmap2(\n  .x = list(anes_des),\n  .y = v,\n  .f = ~calc(df = .x, var = .y)\n) %&gt;% bind_rows()\n\n\n# A tibble: 10 × 4\n   Variable        Answer              p        p_se\n   &lt;chr&gt;           &lt;fct&gt;               &lt;chr&gt;   &lt;dbl&gt;\n 1 TrustGovernment Always              1.6%  0.00204\n 2 TrustGovernment Most of the time    13.2% 0.00553\n 3 TrustGovernment About half the time 30.9% 0.00829\n 4 TrustGovernment Some of the time    43.4% 0.00855\n 5 TrustGovernment Never               11.0% 0.00566\n 6 TrustPeople     Always              0.8%  0.00164\n 7 TrustPeople     Most of the time    41.4% 0.00857\n 8 TrustPeople     About half the time 28.2% 0.00776\n 9 TrustPeople     Some of the time    24.5% 0.00670\n10 TrustPeople     Never               5.0%  0.00422\n\n\n\n\n\n\n\n\nComparison of means and proportions: svyttest()\ngoodness-of-fit test: svygofchisq()\nTest of independence: svychisq()\nTest of homogeneity: svychisq()\n\n\n\n\none-sample t-test: compare to 0 - var ~ 0 or to a different value var - val = 0\ntwo-sample t-test:\n\nunpaired: two level grouping variable - var ~ groupvar or three level grouping variable - var ~ groupvar == level\npaired: var_1 - var_2 = 0\n\n\nExample: one-sample t-test for mean\n\n\nCode\nrecs_des %&gt;%\n  svyttest(\n    formula = SummerTempNight - 68 ~ 0,\n    design =.,\n    na.rm = TRUE\n  )\n\n\n\n    Design-based one-sample t-test\n\ndata:  SummerTempNight - 68 ~ 0\nt = 84.788, df = 58, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 3.287816 3.446810\nsample estimates:\n    mean \n3.367313 \n\n\nget mean summer temps at night\n\n\nCode\nrecs_des %&gt;% \n  summarize(m = survey_mean(SummerTempNight, na.rm = TRUE))\n\n\n# A tibble: 1 × 2\n      m   m_se\n  &lt;dbl&gt;  &lt;dbl&gt;\n1  71.4 0.0397\n\n\nExample: one-sample t-test for proportion\n\n\nCode\nrecs_des %&gt;% summarize(\n  p = survey_prop(),\n  .by = ACUsed\n)\n\n\n# A tibble: 2 × 3\n  ACUsed     p    p_se\n  &lt;lgl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 FALSE  0.113 0.00306\n2 TRUE   0.887 0.00306\n\n\n\n\nCode\nrecs_des %&gt;%\n  svyttest(\n    formula = (ACUsed == TRUE) - 0.90 ~ 0,\n    design = .,\n    na.rm = TRUE\n  ) %&gt;% tidy() %&gt;%\n  mutate(p.value = pretty_p_value(p.value)) %&gt;%\n  gt() %&gt;%\n  fmt_number()\n\n\n\n\nTable 1: One-sample t-test for estimate of US housholds use of A/C in their homes differing from 90%\n\n\n\n\n\n\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n\n\n−0.01\n−4.40\n&lt;0.0001\n58.00\n−0.02\n−0.01\nDesign-based one-sample t-test\ntwo.sided\n\n\n\n\n\n\n\n\n\n\nExample: paired two-sample t-test\n\n\nCode\nrecs_des %&gt;%\n  svyttest(\n    formula = SummerTempNight - WinterTempNight ~ 0,\n    design = .,\n    na.rm = TRUE\n  ) %&gt;% tidy() %&gt;%\n  mutate(\n    p.value = pretty_p_value(p.value)\n  ) %&gt;%\n  gt() %&gt;%\n  fmt_number()\n\n\n\n\nTable 2: Paired two-sample t-test\n\n\n\n\n\n\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n\n\n2.85\n50.83\n&lt;0.0001\n58.00\n2.74\n2.96\nDesign-based one-sample t-test\ntwo.sided\n\n\n\n\n\n\n\n\n\n\n\n\n\nSee https://tidy-survey-r.github.io/tidy-survey-book/c06-statistical-testing.html\nExample: goodness of fit test\n\n\nCode\nanes_des_educ &lt;- anes_des %&gt;%\n  mutate(\n    Education2 = fct_collapse(Education,\n      \"Bachelor or Higher\" = c(\"Bachelor's\", \"Graduate\")\n    )\n  )\n\nanes_des_educ %&gt;%\n  drop_na(Education2) %&gt;%\n  group_by(Education2) %&gt;% \n  summarize(p = survey_mean())\n\n\n# A tibble: 4 × 3\n  Education2              p    p_se\n  &lt;fct&gt;               &lt;dbl&gt;   &lt;dbl&gt;\n1 Less than HS       0.0805 0.00568\n2 High school        0.277  0.0102 \n3 Post HS            0.290  0.00713\n4 Bachelor or Higher 0.352  0.00732\n\n\n\n\nCode\nanes_des_educ %&gt;%\n  svygofchisq(\n    formula = ~Education2,\n    design = .,\n    p = c(0.11, 0.27, 0.28, 0.35),\n    na.rm = TRUE\n  ) %&gt;% tidy()\n\n\nMultiple parameters; naming those columns scale and df.\n\n\n# A tibble: 1 × 5\n    scale    df statistic  p.value method                                       \n    &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;                                        \n1 113213.  2.26  1922280. 0.000292 Design-based chi-squared test for given prob…\n\n\n\n\nCode\nex1_table &lt;- anes_des_educ %&gt;%\n  drop_na(Education2) %&gt;%\n  group_by(Education2) %&gt;%\n  summarize(Observed = survey_mean(vartype = \"ci\")) %&gt;%\n  rename(Education = Education2) %&gt;%\n  mutate(Expected = c(0.11, 0.27, 0.29, 0.33)) %&gt;%\n  select(Education, Expected, everything())\n\nex1_table\n\n\n# A tibble: 4 × 5\n  Education          Expected Observed Observed_low Observed_upp\n  &lt;fct&gt;                 &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1 Less than HS           0.11   0.0805       0.0691       0.0919\n2 High school            0.27   0.277        0.257        0.298 \n3 Post HS                0.29   0.290        0.276        0.305 \n4 Bachelor or Higher     0.33   0.352        0.337        0.367 \n\n\n\n\nCode\nex1_table %&gt;%\n  pivot_longer(\n    cols = c(\"Expected\", \"Observed\"),\n    names_to = \"Names\",\n    values_to = \"Proportion\"\n  ) %&gt;%\n  mutate(\n    Observed_low = if_else(Names == \"Observed\", Observed_low, NA_real_),\n    Observed_upp = if_else(Names == \"Observed\", Observed_upp, NA_real_),\n    Names = if_else(Names == \"Observed\",\n      \"ANES (observed)\", \"ACS (expected)\"\n    )\n  ) %&gt;%\n  ggplot(aes(x = Education, y = Proportion, color = Names)) +\n  geom_point(alpha = 0.75, size = 2) +\n  geom_errorbar(aes(ymin = Observed_low, ymax = Observed_upp),\n    width = 0.25\n  ) +\n  theme_bw() +\n  # scale_color_manual(name = \"Type\", values = book_colors[c(4, 1)]) +\n  theme(legend.position = \"bottom\", legend.title = element_blank())"
  },
  {
    "objectID": "posts/2026-01-15-survey-design/index.html#data",
    "href": "posts/2026-01-15-survey-design/index.html#data",
    "title": "Surveys: Design and Analysis",
    "section": "",
    "text": "Provided by {srvyrexploR} library. The first is America’s elections data.\n\n\nCode\ndata('anes_2020', package = \"srvyrexploR\")\nanes_2020 %&gt;% select(-matches(\"^V\\\\d\")) %&gt;%# starts with V followed by a digit\nglimpse()\n\n\nRows: 7,453\nColumns: 21\n$ CaseID                  &lt;dbl&gt; 200015, 200022, 200039, 200046, 200053, 200060…\n$ InterviewMode           &lt;fct&gt; Web, Web, Web, Web, Web, Web, Web, Web, Web, W…\n$ Weight                  &lt;dbl&gt; 1.0057375, 1.1634731, 0.7686811, 0.5210195, 0.…\n$ VarUnit                 &lt;fct&gt; 2, 2, 1, 2, 1, 2, 1, 2, 2, 2, 1, 1, 2, 2, 2, 2…\n$ Stratum                 &lt;fct&gt; 9, 26, 41, 29, 23, 37, 7, 37, 32, 41, 22, 7, 3…\n$ CampaignInterest        &lt;fct&gt; Somewhat interested, Not much interested, Some…\n$ EarlyVote2020           &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, Yes, NA, NA, N…\n$ VotedPres2016           &lt;fct&gt; Yes, Yes, Yes, Yes, Yes, No, Yes, No, Yes, Yes…\n$ VotedPres2016_selection &lt;fct&gt; Trump, Other, Clinton, Clinton, Trump, NA, Oth…\n$ PartyID                 &lt;fct&gt; Strong republican, Independent, Independent-de…\n$ TrustGovernment         &lt;fct&gt; Never, Never, Some of the time, About half the…\n$ TrustPeople             &lt;fct&gt; About half the time, Some of the time, Some of…\n$ Age                     &lt;dbl&gt; 46, 37, 40, 41, 72, 71, 37, 45, 70, 43, 37, 55…\n$ AgeGroup                &lt;fct&gt; 40-49, 30-39, 40-49, 40-49, 70 or older, 70 or…\n$ Education               &lt;fct&gt; Bachelor's, Post HS, High school, Post HS, Gra…\n$ RaceEth                 &lt;fct&gt; \"Hispanic\", \"Asian, NH/PI\", \"White\", \"Asian, N…\n$ Gender                  &lt;fct&gt; Male, Female, Female, Male, Male, Female, Fema…\n$ Income                  &lt;fct&gt; \"$175,000-249,999\", \"$70,000-74,999\", \"$100,00…\n$ Income7                 &lt;fct&gt; $125k or more, $60k to &lt; 80k, $100k to &lt; 125k,…\n$ VotedPres2020           &lt;fct&gt; NA, Yes, Yes, Yes, Yes, Yes, Yes, NA, Yes, Yes…\n$ VotedPres2020_selection &lt;fct&gt; NA, Other, Biden, Biden, Trump, Biden, Trump, …\n\n\n\n\nCode\nanes_2020 %&gt;% select(-matches(\"^V\\\\d\")) %&gt;% skimr::skim()\n\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n7453\n\n\nNumber of columns\n21\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n18\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nInterviewMode\n0\n1.00\nFALSE\n3\nWeb: 7064, Vid: 274, Tel: 115\n\n\nVarUnit\n0\n1.00\nFALSE\n3\n2: 3750, 1: 3689, 3: 14\n\n\nStratum\n0\n1.00\nFALSE\n50\n12: 179, 6: 172, 27: 172, 21: 170\n\n\nCampaignInterest\n1\n1.00\nFALSE\n3\nVer: 3940, Som: 2569, Not: 943\n\n\nEarlyVote2020\n6963\n0.07\nFALSE\n2\nYes: 375, No: 115\n\n\nVotedPres2016\n21\n1.00\nFALSE\n2\nYes: 5810, No: 1622\n\n\nVotedPres2016_selection\n1686\n0.77\nFALSE\n3\nCli: 2911, Tru: 2466, Oth: 390\n\n\nPartyID\n25\n1.00\nFALSE\n7\nStr: 1796, Str: 1545, Ind: 881, Ind: 876\n\n\nTrustGovernment\n29\n1.00\nFALSE\n5\nSom: 3313, Abo: 2313, Mos: 1016, Nev: 702\n\n\nTrustPeople\n13\n1.00\nFALSE\n5\nMos: 3511, Abo: 2020, Som: 1597, Nev: 264\n\n\nAgeGroup\n294\n0.96\nFALSE\n6\n60-: 1436, 70 : 1330, 30-: 1241, 50-: 1200\n\n\nEducation\n116\n0.98\nFALSE\n5\nPos: 2514, Bac: 1877, Gra: 1474, Hig: 1160\n\n\nRaceEth\n81\n0.99\nFALSE\n6\nWhi: 5420, His: 662, Bla: 650, Asi: 248\n\n\nGender\n51\n0.99\nFALSE\n2\nFem: 4027, Mal: 3375\n\n\nIncome\n517\n0.93\nFALSE\n22\nUnd: 647, $50: 485, $10: 451, $25: 405\n\n\nIncome7\n517\n0.93\nFALSE\n7\n$12: 1468, Und: 1076, $20: 1051, $40: 984\n\n\nVotedPres2020\n1053\n0.86\nFALSE\n2\nYes: 6313, No: 87\n\n\nVotedPres2020_selection\n1219\n0.84\nFALSE\n3\nBid: 3509, Tru: 2567, Oth: 158\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nCaseID\n0\n1.00\n336416.23\n103653.12\n200015.00\n225427.00\n335416.00\n427865.00\n535469.00\n▇▃▃▆▂\n\n\nWeight\n0\n1.00\n1.00\n1.02\n0.01\n0.39\n0.69\n1.21\n6.65\n▇▂▁▁▁\n\n\nAge\n294\n0.96\n51.83\n17.14\n18.00\n37.00\n53.00\n66.00\n80.00\n▅▇▇▇▇\n\n\n\n\n\nResidential energy consumption survey\n\n\nCode\ndata(\"recs_2020\", package = \"srvyrexploR\")\nrecs_2020 %&gt;% \n  select(-matches(\"^NWEIGHT\")) %&gt;% skim()\n\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n18496\n\n\nNumber of columns\n39\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nfactor\n10\n\n\nlogical\n2\n\n\nnumeric\n25\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nREGIONC\n0\n1\n4\n9\n0\n4\n0\n\n\nSTATE_FIPS\n0\n1\n2\n2\n0\n51\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nClimateRegion_BA\n0\n1.00\nFALSE\n8\nCol: 7116, Mix: 5579, Hot: 2545, Hot: 1577\n\n\nUrbanicity\n0\n1.00\nFALSE\n3\nUrb: 12395, Rur: 4081, Urb: 2020\n\n\nRegion\n0\n1.00\nFALSE\n4\nSou: 6426, Wes: 4581, Mid: 3832, Nor: 3657\n\n\nDivision\n0\n1.00\nFALSE\n10\nSou: 3256, Pac: 2497, Eas: 2014, Mid: 1977\n\n\nstate_postal\n0\n1.00\nFALSE\n51\nCA: 1152, TX: 1016, NY: 904, FL: 655\n\n\nstate_name\n0\n1.00\nFALSE\n51\nCal: 1152, Tex: 1016, New: 904, Flo: 655\n\n\nHousingUnitType\n0\n1.00\nFALSE\n5\nSin: 12319, Apa: 2439, Sin: 1751, Apa: 1013\n\n\nYearMade\n0\n1.00\nTRUE\n9\n197: 2817, 200: 2748, Bef: 2721, 199: 2451\n\n\nHeatingBehavior\n751\n0.96\nFALSE\n6\nSet: 7806, Man: 4654, Pro: 3310, Tur: 1491\n\n\nACBehavior\n2325\n0.87\nFALSE\n6\nSet: 6738, Man: 3637, Tur: 2746, Pro: 2638\n\n\n\nVariable type: logical\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\n\n\nSpaceHeatingUsed\n0\n1\n0.96\nTRU: 17745, FAL: 751\n\n\nACUsed\n0\n1\n0.87\nTRU: 16171, FAL: 2325\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nDOEID\n0\n1.00\n109248.50\n5339.48\n100001.00\n104624.75\n109248.50\n113872.25\n118496.00\n▇▇▇▇▇\n\n\nHDD65\n0\n1.00\n4271.81\n2329.46\n0.00\n2433.75\n4396.50\n5810.25\n17383.00\n▆▇▂▁▁\n\n\nCDD65\n0\n1.00\n1525.52\n1143.24\n0.00\n814.00\n1179.00\n1805.00\n5534.00\n▇▆▂▁▁\n\n\nHDD30YR\n0\n1.00\n4679.41\n2338.52\n0.00\n2897.75\n4825.00\n6290.00\n16071.00\n▅▇▃▁▁\n\n\nCDD30YR\n0\n1.00\n1309.88\n988.47\n0.00\n601.00\n1020.00\n1703.00\n4905.00\n▇▅▂▁▁\n\n\nTOTSQFT_EN\n0\n1.00\n1959.97\n1164.97\n200.00\n1100.00\n1700.00\n2510.00\n15000.00\n▇▁▁▁▁\n\n\nTOTHSQFT\n0\n1.00\n1744.46\n1106.33\n0.00\n1000.00\n1520.00\n2300.00\n15000.00\n▇▁▁▁▁\n\n\nTOTCSQFT\n0\n1.00\n1393.57\n1168.31\n0.00\n460.00\n1200.00\n2000.00\n14600.00\n▇▁▁▁▁\n\n\nWinterTempDay\n751\n0.96\n69.77\n3.62\n50.00\n68.00\n70.00\n72.00\n90.00\n▁▁▇▁▁\n\n\nWinterTempAway\n751\n0.96\n67.45\n5.02\n50.00\n65.00\n68.00\n70.00\n90.00\n▁▅▇▁▁\n\n\nWinterTempNight\n751\n0.96\n68.01\n4.66\n50.00\n65.00\n68.00\n70.00\n90.00\n▁▃▇▁▁\n\n\nSummerTempDay\n2325\n0.87\n72.01\n4.74\n50.00\n70.00\n72.00\n75.00\n90.00\n▁▁▇▃▁\n\n\nSummerTempAway\n2325\n0.87\n73.45\n5.68\n50.00\n70.00\n74.00\n78.00\n90.00\n▁▁▇▇▁\n\n\nSummerTempNight\n2325\n0.87\n71.22\n4.80\n50.00\n68.00\n72.00\n74.00\n90.00\n▁▂▇▃▁\n\n\nBTUEL\n0\n1.00\n37016.17\n24265.34\n143.32\n20205.76\n31890.03\n48297.98\n628155.47\n▇▁▁▁▁\n\n\nDOLLAREL\n0\n1.00\n1424.81\n862.08\n-889.48\n836.49\n1257.86\n1818.95\n15680.18\n▇▁▁▁▁\n\n\nBTUNG\n0\n1.00\n36960.50\n46538.32\n0.00\n0.00\n22011.97\n62714.20\n1134708.69\n▇▁▁▁▁\n\n\nDOLLARNG\n0\n1.00\n396.05\n483.41\n0.00\n0.00\n313.86\n644.93\n8154.98\n▇▁▁▁▁\n\n\nBTULP\n0\n1.00\n3916.95\n16741.93\n0.00\n0.00\n0.00\n0.00\n364215.39\n▇▁▁▁▁\n\n\nDOLLARLP\n0\n1.00\n80.89\n333.79\n0.00\n0.00\n0.00\n0.00\n6621.44\n▇▁▁▁▁\n\n\nBTUFO\n0\n1.00\n5108.60\n22698.98\n0.00\n0.00\n0.00\n0.00\n426268.50\n▇▁▁▁▁\n\n\nDOLLARFO\n0\n1.00\n88.43\n395.97\n0.00\n0.00\n0.00\n0.00\n7003.69\n▇▁▁▁▁\n\n\nBTUWOOD\n0\n1.00\n3596.49\n17766.37\n0.00\n0.00\n0.00\n0.00\n500000.00\n▇▁▁▁▁\n\n\nTOTALBTU\n0\n1.00\n83002.29\n53206.00\n1182.22\n45565.16\n74180.11\n108535.02\n1367548.13\n▇▁▁▁▁\n\n\nTOTALDOL\n0\n1.00\n1990.17\n1108.99\n-150.51\n1258.32\n1793.20\n2472.00\n20043.41\n▇▁▁▁▁\n\n\n\n\n\nNumeric vars\n\n\nCode\nrecs_2020 %&gt;% select(where(is.numeric)) %&gt;% colnames()\n\n\n [1] \"DOEID\"           \"HDD65\"           \"CDD65\"           \"HDD30YR\"        \n [5] \"CDD30YR\"         \"TOTSQFT_EN\"      \"TOTHSQFT\"        \"TOTCSQFT\"       \n [9] \"WinterTempDay\"   \"WinterTempAway\"  \"WinterTempNight\" \"SummerTempDay\"  \n[13] \"SummerTempAway\"  \"SummerTempNight\" \"NWEIGHT\"         \"NWEIGHT1\"       \n[17] \"NWEIGHT2\"        \"NWEIGHT3\"        \"NWEIGHT4\"        \"NWEIGHT5\"       \n[21] \"NWEIGHT6\"        \"NWEIGHT7\"        \"NWEIGHT8\"        \"NWEIGHT9\"       \n[25] \"NWEIGHT10\"       \"NWEIGHT11\"       \"NWEIGHT12\"       \"NWEIGHT13\"      \n[29] \"NWEIGHT14\"       \"NWEIGHT15\"       \"NWEIGHT16\"       \"NWEIGHT17\"      \n[33] \"NWEIGHT18\"       \"NWEIGHT19\"       \"NWEIGHT20\"       \"NWEIGHT21\"      \n[37] \"NWEIGHT22\"       \"NWEIGHT23\"       \"NWEIGHT24\"       \"NWEIGHT25\"      \n[41] \"NWEIGHT26\"       \"NWEIGHT27\"       \"NWEIGHT28\"       \"NWEIGHT29\"      \n[45] \"NWEIGHT30\"       \"NWEIGHT31\"       \"NWEIGHT32\"       \"NWEIGHT33\"      \n[49] \"NWEIGHT34\"       \"NWEIGHT35\"       \"NWEIGHT36\"       \"NWEIGHT37\"      \n[53] \"NWEIGHT38\"       \"NWEIGHT39\"       \"NWEIGHT40\"       \"NWEIGHT41\"      \n[57] \"NWEIGHT42\"       \"NWEIGHT43\"       \"NWEIGHT44\"       \"NWEIGHT45\"      \n[61] \"NWEIGHT46\"       \"NWEIGHT47\"       \"NWEIGHT48\"       \"NWEIGHT49\"      \n[65] \"NWEIGHT50\"       \"NWEIGHT51\"       \"NWEIGHT52\"       \"NWEIGHT53\"      \n[69] \"NWEIGHT54\"       \"NWEIGHT55\"       \"NWEIGHT56\"       \"NWEIGHT57\"      \n[73] \"NWEIGHT58\"       \"NWEIGHT59\"       \"NWEIGHT60\"       \"BTUEL\"          \n[77] \"DOLLAREL\"        \"BTUNG\"           \"DOLLARNG\"        \"BTULP\"          \n[81] \"DOLLARLP\"        \"BTUFO\"           \"DOLLARFO\"        \"BTUWOOD\"        \n[85] \"TOTALBTU\"        \"TOTALDOL\"       \n\n\nAmerican national election studies design object\n\n\nCode\ncps_state_in &lt;- getCensus(\n  name = \"cps/basic/mar\",\n  vintage = 2020,\n  region = \"state\",\n  vars = c(\n    \"HRMONTH\", \"HRYEAR4\",\n    \"PRTAGE\", \"PRCITSHP\", \"PWSSWGT\"\n  ),\n  key = Sys.getenv(\"CENSUS_KEY\")\n)\n\ncps_state &lt;- cps_state_in %&gt;%\n  as_tibble() %&gt;%\n  mutate(\n    across(\n      .cols = everything(),\n      .fns = as.numeric\n    )\n  )\n\n\n\n\nCode\ncps_narrow_resp &lt;- cps_state %&gt;%\n  filter(\n    PRTAGE &gt;= 18,\n    PRCITSHP %in% c(1:4)\n  )\n\n\nCalculate use population from the narrow data. Weights should add to total pop.\n\n\nCode\ntargetpop &lt;- cps_narrow_resp %&gt;% \n  pull(PWSSWGT) %&gt;%\n  sum()\nscales::comma(targetpop)\n\n\n[1] \"231,034,125\"\n\n\nWe can the weight the us election study appropriately\n\n\nCode\nanes_adjwgt &lt;- anes_2020 %&gt;%\n  mutate(\n    weight = V200010b / sum(V200010b) * targetpop\n  )\n\n\nWe then make it conform to a survey design\n\n\nCode\nanes_des &lt;- anes_adjwgt %&gt;%\n  as_survey_design(\n    weights = weight,\n    strata = V200010d,\n    ids = V200010c,\n    nest = TRUE\n  )\nanes_des\n\n\nStratified 1 - level Cluster Sampling design (with replacement)\nWith (101) clusters.\nCalled via srvyr\nSampling variables:\n  - ids: V200010c \n  - strata: V200010d \n  - weights: weight \nData variables: \n  - V200001 (dbl), CaseID (dbl), V200002 (hvn_lbll), InterviewMode (fct),\n    V200010b (dbl), Weight (dbl), V200010c (dbl), VarUnit (fct), V200010d\n    (dbl), Stratum (fct), V201006 (hvn_lbll), CampaignInterest (fct), V201023\n    (hvn_lbll), EarlyVote2020 (fct), V201024 (hvn_lbll), V201025x (hvn_lbll),\n    V201028 (hvn_lbll), V201029 (hvn_lbll), V201101 (hvn_lbll), V201102\n    (hvn_lbll), VotedPres2016 (fct), V201103 (hvn_lbll),\n    VotedPres2016_selection (fct), V201228 (hvn_lbll), V201229 (hvn_lbll),\n    V201230 (hvn_lbll), V201231x (hvn_lbll), PartyID (fct), V201233 (hvn_lbll),\n    TrustGovernment (fct), V201237 (hvn_lbll), TrustPeople (fct), V201507x\n    (hvn_lbll), Age (dbl), AgeGroup (fct), V201510 (hvn_lbll), Education (fct),\n    V201546 (hvn_lbll), V201547a (hvn_lbll), V201547b (hvn_lbll), V201547c\n    (hvn_lbll), V201547d (hvn_lbll), V201547e (hvn_lbll), V201547z (hvn_lbll),\n    V201549x (hvn_lbll), RaceEth (fct), V201600 (hvn_lbll), Gender (fct),\n    V201607 (hvn_lbll), V201610 (hvn_lbll), V201611 (hvn_lbll), V201613\n    (hvn_lbll), V201615 (hvn_lbll), V201616 (hvn_lbll), V201617x (hvn_lbll),\n    Income (fct), Income7 (fct), V202051 (hvn_lbll), V202066 (hvn_lbll),\n    V202072 (hvn_lbll), VotedPres2020 (fct), V202073 (hvn_lbll), V202109x\n    (hvn_lbll), V202110x (hvn_lbll), VotedPres2020_selection (fct), weight\n    (dbl)\n\n\nUsing replicated weights\n\n\nCode\nrecs_des &lt;- recs_2020 %&gt;%\n  as_survey_rep(\n    weights = NWEIGHT,\n    repweights = NWEIGHT1:NWEIGHT60,\n    type = \"JK1\",\n    scale = 59 / 60,\n    mse = TRUE\n  )\n\nrecs_des\n\n\nCall: Called via srvyr\nUnstratified cluster jacknife (JK1) with 60 replicates and MSE variances.\nSampling variables:\n  - repweights: `NWEIGHT1 + NWEIGHT2 + NWEIGHT3 + NWEIGHT4 + NWEIGHT5 +\n    NWEIGHT6 + NWEIGHT7 + NWEIGHT8 + NWEIGHT9 + NWEIGHT10 + NWEIGHT11 +\n    NWEIGHT12 + NWEIGHT13 + NWEIGHT14 + NWEIGHT15 + NWEIGHT16 + NWEIGHT17 +\n    NWEIGHT18 + NWEIGHT19 + NWEIGHT20 + NWEIGHT21 + NWEIGHT22 + NWEIGHT23 +\n    NWEIGHT24 + NWEIGHT25 + NWEIGHT26 + NWEIGHT27 + NWEIGHT28 + NWEIGHT29 +\n    NWEIGHT30 + NWEIGHT31 + NWEIGHT32 + NWEIGHT33 + NWEIGHT34 + NWEIGHT35 +\n    NWEIGHT36 + NWEIGHT37 + NWEIGHT38 + NWEIGHT39 + NWEIGHT40 + NWEIGHT41 +\n    NWEIGHT42 + NWEIGHT43 + NWEIGHT44 + NWEIGHT45 + NWEIGHT46 + NWEIGHT47 +\n    NWEIGHT48 + NWEIGHT49 + NWEIGHT50 + NWEIGHT51 + NWEIGHT52 + NWEIGHT53 +\n    NWEIGHT54 + NWEIGHT55 + NWEIGHT56 + NWEIGHT57 + NWEIGHT58 + NWEIGHT59 +\n    NWEIGHT60` \n  - weights: NWEIGHT \nData variables: \n  - DOEID (dbl), ClimateRegion_BA (fct), Urbanicity (fct), Region (fct),\n    REGIONC (chr), Division (fct), STATE_FIPS (chr), state_postal (fct),\n    state_name (fct), HDD65 (dbl), CDD65 (dbl), HDD30YR (dbl), CDD30YR (dbl),\n    HousingUnitType (fct), YearMade (ord), TOTSQFT_EN (dbl), TOTHSQFT (dbl),\n    TOTCSQFT (dbl), SpaceHeatingUsed (lgl), ACUsed (lgl), HeatingBehavior\n    (fct), WinterTempDay (dbl), WinterTempAway (dbl), WinterTempNight (dbl),\n    ACBehavior (fct), SummerTempDay (dbl), SummerTempAway (dbl),\n    SummerTempNight (dbl), NWEIGHT (dbl), NWEIGHT1 (dbl), NWEIGHT2 (dbl),\n    NWEIGHT3 (dbl), NWEIGHT4 (dbl), NWEIGHT5 (dbl), NWEIGHT6 (dbl), NWEIGHT7\n    (dbl), NWEIGHT8 (dbl), NWEIGHT9 (dbl), NWEIGHT10 (dbl), NWEIGHT11 (dbl),\n    NWEIGHT12 (dbl), NWEIGHT13 (dbl), NWEIGHT14 (dbl), NWEIGHT15 (dbl),\n    NWEIGHT16 (dbl), NWEIGHT17 (dbl), NWEIGHT18 (dbl), NWEIGHT19 (dbl),\n    NWEIGHT20 (dbl), NWEIGHT21 (dbl), NWEIGHT22 (dbl), NWEIGHT23 (dbl),\n    NWEIGHT24 (dbl), NWEIGHT25 (dbl), NWEIGHT26 (dbl), NWEIGHT27 (dbl),\n    NWEIGHT28 (dbl), NWEIGHT29 (dbl), NWEIGHT30 (dbl), NWEIGHT31 (dbl),\n    NWEIGHT32 (dbl), NWEIGHT33 (dbl), NWEIGHT34 (dbl), NWEIGHT35 (dbl),\n    NWEIGHT36 (dbl), NWEIGHT37 (dbl), NWEIGHT38 (dbl), NWEIGHT39 (dbl),\n    NWEIGHT40 (dbl), NWEIGHT41 (dbl), NWEIGHT42 (dbl), NWEIGHT43 (dbl),\n    NWEIGHT44 (dbl), NWEIGHT45 (dbl), NWEIGHT46 (dbl), NWEIGHT47 (dbl),\n    NWEIGHT48 (dbl), NWEIGHT49 (dbl), NWEIGHT50 (dbl), NWEIGHT51 (dbl),\n    NWEIGHT52 (dbl), NWEIGHT53 (dbl), NWEIGHT54 (dbl), NWEIGHT55 (dbl),\n    NWEIGHT56 (dbl), NWEIGHT57 (dbl), NWEIGHT58 (dbl), NWEIGHT59 (dbl),\n    NWEIGHT60 (dbl), BTUEL (dbl), DOLLAREL (dbl), BTUNG (dbl), DOLLARNG (dbl),\n    BTULP (dbl), DOLLARLP (dbl), BTUFO (dbl), DOLLARFO (dbl), BTUWOOD (dbl),\n    TOTALBTU (dbl), TOTALDOL (dbl)"
  },
  {
    "objectID": "posts/2026-01-15-survey-design/index.html#descriptive-statisitcs",
    "href": "posts/2026-01-15-survey-design/index.html#descriptive-statisitcs",
    "title": "Surveys: Design and Analysis",
    "section": "",
    "text": "Count observations with survey_count() or survey_tally()\nSum variables with survey_total()\nMeans and proportions: survey_mean(), survey_prop()\nQuantiles and medians: survey_quantile(), survey_median()\nCorrelations: survey_cor()\nRatios: survey_ratio()\nVariances and standard deviation: survey_var(), survey_sd()\n\n\n\n\n\nCode\nrecs_des %&gt;% survey_tally()\n\n\n# A tibble: 1 × 2\n           n  n_se\n       &lt;dbl&gt; &lt;dbl&gt;\n1 123529025. 0.148\n\n\nEstimated counts by subgroups\n\n\nCode\nrecs_des %&gt;% survey_count(Region, Division, name = \"N\")\n\n\n# A tibble: 10 × 4\n   Region    Division                   N         N_se\n   &lt;fct&gt;     &lt;fct&gt;                  &lt;dbl&gt;        &lt;dbl&gt;\n 1 Northeast New England         5876166  0.0000000137\n 2 Northeast Middle Atlantic    16043503  0.0000000487\n 3 Midwest   East North Central 18546912  0.000000437 \n 4 Midwest   West North Central  8495815  0.0000000177\n 5 South     South Atlantic     24843261  0.0000000418\n 6 South     East South Central  7380717. 0.114       \n 7 South     West South Central 14619094  0.000488    \n 8 West      Mountain North      4615844  0.119       \n 9 West      Mountain South      4602070  0.0000000492\n10 West      Pacific            18505643. 0.00000295  \n\n\nTo achieve same result by survey_tally(), you first group\n\n\nCode\nrecs_des %&gt;% group_by(Region, Division) %&gt;% survey_tally(name = \"N\")\n\n\n# A tibble: 10 × 4\n# Groups:   Region [4]\n   Region    Division                   N         N_se\n   &lt;fct&gt;     &lt;fct&gt;                  &lt;dbl&gt;        &lt;dbl&gt;\n 1 Northeast New England         5876166  0.0000000137\n 2 Northeast Middle Atlantic    16043503  0.0000000487\n 3 Midwest   East North Central 18546912  0.000000437 \n 4 Midwest   West North Central  8495815  0.0000000177\n 5 South     South Atlantic     24843261  0.0000000418\n 6 South     East South Central  7380717. 0.114       \n 7 South     West South Central 14619094  0.000488    \n 8 West      Mountain North      4615844  0.119       \n 9 West      Mountain South      4602070  0.0000000492\n10 West      Pacific            18505643. 0.00000295  \n\n\n\n\n\nTo get population count estimate, we leave argument x, empty.\n\n\nCode\nrecs_des %&gt;% \n  summarize(\n    tot = survey_total()\n  )\n\n\n# A tibble: 1 × 2\n         tot tot_se\n       &lt;dbl&gt;  &lt;dbl&gt;\n1 123529025.  0.148\n\n\nOverall summation of a continuous variable\n\n\nCode\nrecs_des %&gt;%\n  summarize(elec_bill = survey_total(DOLLAREL))\n\n\n# A tibble: 1 × 2\n      elec_bill elec_bill_se\n          &lt;dbl&gt;        &lt;dbl&gt;\n1 170473527909.   664893504.\n\n\nSummation by groups\n\n\nCode\nrecs_des %&gt;% group_by(Region) %&gt;%\n  summarize(\n  elec_bill = survey_total(DOLLAREL, vartype = \"ci\")\n  \n)\n\n\n# A tibble: 4 × 4\n  Region       elec_bill elec_bill_low elec_bill_upp\n  &lt;fct&gt;            &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n1 Northeast 29430369947.  28788987554.  30071752341.\n2 Midwest   34972544751.  34339576041.  35605513460.\n3 South     72496840204.  71534780902.  73458899506.\n4 West      33573773008.  32909111702.  34238434313.\n\n\nYou can supply .by argument inside summarize\n\n\nCode\nrecs_des  %&gt;%\n  summarize(\n  elec_bill = survey_total(DOLLAREL, vartype = \"ci\"),\n  .by = Region\n)\n\n\n# A tibble: 4 × 4\n  Region       elec_bill elec_bill_low elec_bill_upp\n  &lt;fct&gt;            &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n1 Northeast 29430369947.  28788987554.  30071752341.\n2 Midwest   34972544751.  34339576041.  35605513460.\n3 South     72496840204.  71534780902.  73458899506.\n4 West      33573773008.  32909111702.  34238434313.\n\n\n\n\n\n\n\nCode\nrecs_des %&gt;% group_by(Region, ACUsed) %&gt;%\n  summarise(\n  p = survey_prop()\n)\n\n\nWhen `proportion` is unspecified, `survey_prop()` now defaults to `proportion = TRUE`.\nℹ This should improve confidence interval coverage.\nThis message is displayed once per session.\n\n\n# A tibble: 8 × 4\n# Groups:   Region [4]\n  Region    ACUsed      p    p_se\n  &lt;fct&gt;     &lt;lgl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 Northeast FALSE  0.110  0.00590\n2 Northeast TRUE   0.890  0.00590\n3 Midwest   FALSE  0.0666 0.00508\n4 Midwest   TRUE   0.933  0.00508\n5 South     FALSE  0.0581 0.00278\n6 South     TRUE   0.942  0.00278\n7 West      FALSE  0.255  0.00759\n8 West      TRUE   0.745  0.00759\n\n\nJoint proportions using interact\n\n\nCode\nrecs_des %&gt;% \n  group_by(\n    interact(Region, ACUsed)\n  ) %&gt;% \n  summarize(\n    p = survey_prop() \n  ) %&gt;% mutate(\n    p = scales::percent(p)\n  )\n\n\n# A tibble: 8 × 4\n  Region    ACUsed p         p_se\n  &lt;fct&gt;     &lt;lgl&gt;  &lt;chr&gt;    &lt;dbl&gt;\n1 Northeast FALSE  1.96%  0.00105\n2 Northeast TRUE   15.79% 0.00105\n3 Midwest   FALSE  1.46%  0.00111\n4 Midwest   TRUE   20.43% 0.00111\n5 South     FALSE  2.20%  0.00106\n6 South     TRUE   35.72% 0.00106\n7 West      FALSE  5.73%  0.00170\n8 West      TRUE   16.72% 0.00170\n\n\nOverall mean\n\n\nCode\nrecs_des %&gt;% summarize(\n  elec_bill = survey_mean(DOLLAREL, vartype = c(\"se\", \"ci\"))\n)\n\n\n# A tibble: 1 × 4\n  elec_bill elec_bill_se elec_bill_low elec_bill_upp\n      &lt;dbl&gt;        &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n1     1380.         5.38         1369.         1391.\n\n\n\n\n\nOverall quantiles\n\n\nCode\nrecs_des %&gt;%\n  summarize(\n    elec_bill = survey_quantile(DOLLAREL, quantiles = c(.25, .5, .75))\n  )\n\n\n# A tibble: 1 × 6\n  elec_bill_q25 elec_bill_q50 elec_bill_q75 elec_bill_q25_se elec_bill_q50_se\n          &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n1          795.         1215.         1770.             5.69             6.33\n# ℹ 1 more variable: elec_bill_q75_se &lt;dbl&gt;\n\n\nOverall median\n\n\nCode\nrecs_des %&gt;%\n  summarize(\n    elec_bill = survey_median(DOLLAREL)\n  )\n\n\n# A tibble: 1 × 2\n  elec_bill elec_bill_se\n      &lt;dbl&gt;        &lt;dbl&gt;\n1     1215.         6.33\n\n\nCorrelations\n\n\nCode\nrecs_des %&gt;%\n  summarize(SQFT_Elec_Corr = survey_corr(TOTSQFT_EN, BTUEL))\n\n\nWarning: There was 1 warning in `dplyr::summarise()`.\nℹ In argument: `SQFT_Elec_Corr = survey_corr(TOTSQFT_EN, BTUEL)`.\nCaused by warning in `sweep()`:\n! length(STATS) or dim(STATS) do not match dim(x)[MARGIN]\n\n\n# A tibble: 1 × 2\n  SQFT_Elec_Corr SQFT_Elec_Corr_se\n           &lt;dbl&gt;             &lt;dbl&gt;\n1          0.417           0.00689\n\n\n\n\nDesign effect measures the precision of an estimate under a particular sampling design relative to a simple random sampling design. If &lt;1, the design is statistically more efficient than SRS. It is used in the calculation of effective sample size - the sample size needed if we were to employ SRS\n\\[n_{eff} = \\frac{n}{D_{eff}}\\]\n\n\nCode\nrecs_des %&gt;%\n  summarize(\n    across(\n          c(BTUEL, BTUNG, BTULP, BTUFO, BTUWOOD),\n          ~ survey_mean(.x, deff = TRUE, vartype = NULL)\n    )\n  ) %&gt;% select(ends_with(\"deff\"))\n\n\n# A tibble: 1 × 5\n  BTUEL_deff BTUNG_deff BTULP_deff BTUFO_deff BTUWOOD_deff\n       &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;\n1      0.597      0.938       1.21      0.720         1.10\n\n\n\n\n\n\n\nCode\nrecs_des %&gt;%\n  cascade(DOLLAREL_mn = survey_mean(DOLLAREL))\n\n\n# A tibble: 1 × 2\n  DOLLAREL_mn DOLLAREL_mn_se\n        &lt;dbl&gt;          &lt;dbl&gt;\n1       1380.           5.38\n\n\nGroup\n\n\nCode\nrecs_des %&gt;%\n  group_by(Region) %&gt;%\n  cascade(DOLLAREL_mn = survey_mean(DOLLAREL), .fill = \"National\")\n\n\n# A tibble: 5 × 3\n  Region    DOLLAREL_mn DOLLAREL_mn_se\n  &lt;fct&gt;           &lt;dbl&gt;          &lt;dbl&gt;\n1 Northeast       1343.          14.6 \n2 Midwest         1293.          11.7 \n3 South           1548.          10.3 \n4 West            1211.          12.0 \n5 National        1380.           5.38\n\n\nOr through a long dplyr pipeline\n\n\nCode\nrecs_des %&gt;% \n  summarize(\n    cons_mean = survey_mean(DOLLAREL),\n    .by = Region\n  ) %&gt;% bind_rows(\n    ., recs_des %&gt;%\n      summarize(\n        cons_mean = survey_mean(DOLLAREL)\n      ) %&gt;% mutate(\n        ., Region = \"National\"\n      ) %&gt;% select(Region, everything())\n  )\n\n\n# A tibble: 5 × 3\n  Region    cons_mean cons_mean_se\n  &lt;chr&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1 Northeast     1343.        14.6 \n2 Midwest       1293.        11.7 \n3 South         1548.        10.3 \n4 West          1211.        12.0 \n5 National      1380.         5.38\n\n\n\n\n\n\n\nCode\nrecs_des %&gt;% \n  summarize(\n    across(\n      starts_with(\"BTU\"),\n      list(\n        Total = ~ survey_total(.x, vartype = \"cv\"),\n        Mean = ~ survey_mean(.x, vartype = \"cv\")\n      ),\n      .unpack = \"{outer}.{inner}\"\n    )\n  )\n\n\n# A tibble: 1 × 20\n  BTUEL_Total.coef BTUEL_Total._cv BTUEL_Mean.coef BTUEL_Mean._cv\n             &lt;dbl&gt;           &lt;dbl&gt;           &lt;dbl&gt;          &lt;dbl&gt;\n1    4453284510065         0.00377          36051.        0.00377\n# ℹ 16 more variables: BTUNG_Total.coef &lt;dbl&gt;, BTUNG_Total._cv &lt;dbl&gt;,\n#   BTUNG_Mean.coef &lt;dbl&gt;, BTUNG_Mean._cv &lt;dbl&gt;, BTULP_Total.coef &lt;dbl&gt;,\n#   BTULP_Total._cv &lt;dbl&gt;, BTULP_Mean.coef &lt;dbl&gt;, BTULP_Mean._cv &lt;dbl&gt;,\n#   BTUFO_Total.coef &lt;dbl&gt;, BTUFO_Total._cv &lt;dbl&gt;, BTUFO_Mean.coef &lt;dbl&gt;,\n#   BTUFO_Mean._cv &lt;dbl&gt;, BTUWOOD_Total.coef &lt;dbl&gt;, BTUWOOD_Total._cv &lt;dbl&gt;,\n#   BTUWOOD_Mean.coef &lt;dbl&gt;, BTUWOOD_Mean._cv &lt;dbl&gt;\n\n\n\n\n\n\n\nCode\ncalc &lt;- function(df, var) {\n  df %&gt;%\n    drop_na(!!sym(var)) %&gt;%\n    group_by(!!sym(var)) %&gt;%\n    summarize(p = survey_prop()) %&gt;%\n    mutate(Variable = var) %&gt;%\n    rename(Answer := !!sym(var)) %&gt;%\n    mutate(p = scales::percent(p)) %&gt;%\n    select(Variable, everything())\n}\n\nv &lt;- c(\"TrustGovernment\", \"TrustPeople\")\nmap2(\n  .x = list(anes_des),\n  .y = v,\n  .f = ~calc(df = .x, var = .y)\n) %&gt;% bind_rows()\n\n\n# A tibble: 10 × 4\n   Variable        Answer              p        p_se\n   &lt;chr&gt;           &lt;fct&gt;               &lt;chr&gt;   &lt;dbl&gt;\n 1 TrustGovernment Always              1.6%  0.00204\n 2 TrustGovernment Most of the time    13.2% 0.00553\n 3 TrustGovernment About half the time 30.9% 0.00829\n 4 TrustGovernment Some of the time    43.4% 0.00855\n 5 TrustGovernment Never               11.0% 0.00566\n 6 TrustPeople     Always              0.8%  0.00164\n 7 TrustPeople     Most of the time    41.4% 0.00857\n 8 TrustPeople     About half the time 28.2% 0.00776\n 9 TrustPeople     Some of the time    24.5% 0.00670\n10 TrustPeople     Never               5.0%  0.00422"
  },
  {
    "objectID": "posts/2026-01-15-survey-design/index.html#statistical-testing",
    "href": "posts/2026-01-15-survey-design/index.html#statistical-testing",
    "title": "Surveys: Design and Analysis",
    "section": "",
    "text": "Comparison of means and proportions: svyttest()\ngoodness-of-fit test: svygofchisq()\nTest of independence: svychisq()\nTest of homogeneity: svychisq()\n\n\n\n\none-sample t-test: compare to 0 - var ~ 0 or to a different value var - val = 0\ntwo-sample t-test:\n\nunpaired: two level grouping variable - var ~ groupvar or three level grouping variable - var ~ groupvar == level\npaired: var_1 - var_2 = 0\n\n\nExample: one-sample t-test for mean\n\n\nCode\nrecs_des %&gt;%\n  svyttest(\n    formula = SummerTempNight - 68 ~ 0,\n    design =.,\n    na.rm = TRUE\n  )\n\n\n\n    Design-based one-sample t-test\n\ndata:  SummerTempNight - 68 ~ 0\nt = 84.788, df = 58, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 3.287816 3.446810\nsample estimates:\n    mean \n3.367313 \n\n\nget mean summer temps at night\n\n\nCode\nrecs_des %&gt;% \n  summarize(m = survey_mean(SummerTempNight, na.rm = TRUE))\n\n\n# A tibble: 1 × 2\n      m   m_se\n  &lt;dbl&gt;  &lt;dbl&gt;\n1  71.4 0.0397\n\n\nExample: one-sample t-test for proportion\n\n\nCode\nrecs_des %&gt;% summarize(\n  p = survey_prop(),\n  .by = ACUsed\n)\n\n\n# A tibble: 2 × 3\n  ACUsed     p    p_se\n  &lt;lgl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 FALSE  0.113 0.00306\n2 TRUE   0.887 0.00306\n\n\n\n\nCode\nrecs_des %&gt;%\n  svyttest(\n    formula = (ACUsed == TRUE) - 0.90 ~ 0,\n    design = .,\n    na.rm = TRUE\n  ) %&gt;% tidy() %&gt;%\n  mutate(p.value = pretty_p_value(p.value)) %&gt;%\n  gt() %&gt;%\n  fmt_number()\n\n\n\n\nTable 1: One-sample t-test for estimate of US housholds use of A/C in their homes differing from 90%\n\n\n\n\n\n\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n\n\n−0.01\n−4.40\n&lt;0.0001\n58.00\n−0.02\n−0.01\nDesign-based one-sample t-test\ntwo.sided\n\n\n\n\n\n\n\n\n\n\nExample: paired two-sample t-test\n\n\nCode\nrecs_des %&gt;%\n  svyttest(\n    formula = SummerTempNight - WinterTempNight ~ 0,\n    design = .,\n    na.rm = TRUE\n  ) %&gt;% tidy() %&gt;%\n  mutate(\n    p.value = pretty_p_value(p.value)\n  ) %&gt;%\n  gt() %&gt;%\n  fmt_number()\n\n\n\n\nTable 2: Paired two-sample t-test\n\n\n\n\n\n\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n\n\n2.85\n50.83\n&lt;0.0001\n58.00\n2.74\n2.96\nDesign-based one-sample t-test\ntwo.sided\n\n\n\n\n\n\n\n\n\n\n\n\n\nSee https://tidy-survey-r.github.io/tidy-survey-book/c06-statistical-testing.html\nExample: goodness of fit test\n\n\nCode\nanes_des_educ &lt;- anes_des %&gt;%\n  mutate(\n    Education2 = fct_collapse(Education,\n      \"Bachelor or Higher\" = c(\"Bachelor's\", \"Graduate\")\n    )\n  )\n\nanes_des_educ %&gt;%\n  drop_na(Education2) %&gt;%\n  group_by(Education2) %&gt;% \n  summarize(p = survey_mean())\n\n\n# A tibble: 4 × 3\n  Education2              p    p_se\n  &lt;fct&gt;               &lt;dbl&gt;   &lt;dbl&gt;\n1 Less than HS       0.0805 0.00568\n2 High school        0.277  0.0102 \n3 Post HS            0.290  0.00713\n4 Bachelor or Higher 0.352  0.00732\n\n\n\n\nCode\nanes_des_educ %&gt;%\n  svygofchisq(\n    formula = ~Education2,\n    design = .,\n    p = c(0.11, 0.27, 0.28, 0.35),\n    na.rm = TRUE\n  ) %&gt;% tidy()\n\n\nMultiple parameters; naming those columns scale and df.\n\n\n# A tibble: 1 × 5\n    scale    df statistic  p.value method                                       \n    &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;                                        \n1 113213.  2.26  1922280. 0.000292 Design-based chi-squared test for given prob…\n\n\n\n\nCode\nex1_table &lt;- anes_des_educ %&gt;%\n  drop_na(Education2) %&gt;%\n  group_by(Education2) %&gt;%\n  summarize(Observed = survey_mean(vartype = \"ci\")) %&gt;%\n  rename(Education = Education2) %&gt;%\n  mutate(Expected = c(0.11, 0.27, 0.29, 0.33)) %&gt;%\n  select(Education, Expected, everything())\n\nex1_table\n\n\n# A tibble: 4 × 5\n  Education          Expected Observed Observed_low Observed_upp\n  &lt;fct&gt;                 &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1 Less than HS           0.11   0.0805       0.0691       0.0919\n2 High school            0.27   0.277        0.257        0.298 \n3 Post HS                0.29   0.290        0.276        0.305 \n4 Bachelor or Higher     0.33   0.352        0.337        0.367 \n\n\n\n\nCode\nex1_table %&gt;%\n  pivot_longer(\n    cols = c(\"Expected\", \"Observed\"),\n    names_to = \"Names\",\n    values_to = \"Proportion\"\n  ) %&gt;%\n  mutate(\n    Observed_low = if_else(Names == \"Observed\", Observed_low, NA_real_),\n    Observed_upp = if_else(Names == \"Observed\", Observed_upp, NA_real_),\n    Names = if_else(Names == \"Observed\",\n      \"ANES (observed)\", \"ACS (expected)\"\n    )\n  ) %&gt;%\n  ggplot(aes(x = Education, y = Proportion, color = Names)) +\n  geom_point(alpha = 0.75, size = 2) +\n  geom_errorbar(aes(ymin = Observed_low, ymax = Observed_upp),\n    width = 0.25\n  ) +\n  theme_bw() +\n  # scale_color_manual(name = \"Type\", values = book_colors[c(4, 1)]) +\n  theme(legend.position = \"bottom\", legend.title = element_blank())"
  },
  {
    "objectID": "posts/2026-01-15-survey-design/index.html#statified-sampling",
    "href": "posts/2026-01-15-survey-design/index.html#statified-sampling",
    "title": "Surveys: Design and Analysis",
    "section": "Statified Sampling",
    "text": "Statified Sampling\nPopulation can be grouped into homogeneous units (strata). Random samples are then drawn from each of the units.\nIf \\(\\hat{y}_h\\) is the sample mean for stratum \\(h\\), \\(N_h\\) the population size of stratum \\(h\\), and \\(H\\) the total number of strata, then estimates of the mean\n\\[\\hat{y} = \\frac{1}{N}\\sum_{h=1}^H N_h \\hat{y}_h\\] and\n\\[se(\\hat{y}) = \\sqrt{\\frac{1}{N^2}\\sum_{h=1}^H N^2_h \\frac{s^2_h}{n_h}(1 - \\frac{n_h}{N_h})}\\] where\n\\[s^2_h = \\frac{1}{n_h - 1} \\sum_{i=1}^{n_h}(y_{i,h} - \\hat{y_h})^2\\] and proportion\n\\[\\hat{p} = \\frac{1}{N} \\sum_{h=1}^H N_h \\hat{p}_h\\]\n\\[se(\\hat{p}) = \\frac{1}{N}\\sqrt{\\sum_{h=1}^H N^2_h\\frac{\\hat{p_h}(1- \\hat{p_h})}{n_h - 1}(1 - \\frac{n_h}{N_h})}\\]"
  },
  {
    "objectID": "posts/2026-01-15-survey-design/index.html#clustered-sampling",
    "href": "posts/2026-01-15-survey-design/index.html#clustered-sampling",
    "title": "Surveys: Design and Analysis",
    "section": "Clustered sampling",
    "text": "Clustered sampling\nApplies if a population can be divided into mutually exclusive subgroups (clusters or primary sampling units (PSU)).A random selection of PSUs is sampled followed by another level of sampling within the chosen clusters. Suppose \\(a\\) clusters are sampled from a population of A clusters via SRS. Within each sampled cluster \\(i\\) there are \\(B_i\\) units in the population of which \\(b_i\\) units are sampled using SRS. if \\(\\hat{y}_i\\) is the \\(i\\)th cluster mean, then the population mean is estimated by\n\\[\\hat{y} = \\frac{\\sum_{i=1}^aB_i\\hat{y_i}}{\\sum_{i=1}^a B_i}\\]\n\\[se(\\hat{y}) = \\frac{1}{\\hat{N}}\\sqrt{(1 - \\frac{a}{A})\\frac{s^2_a}{a} + \\frac{A}{a}\\sum_{i=1}^a(1 - \\frac{b_i}{B_i}) \\frac{s_i^2}{b_i}}\\] where \\(\\hat{N}\\) is the estimated population size, \\(s^2_a\\) is the between-cluster variance, and \\(s^2_i\\) is the within-cluster variance.\nBetween-cluster variance \\(s^2_a\\) is:\n\\[s_a^2 = \\frac{1}{a - 1} \\sum_{i = 1}^a (\\hat{y_i} - \\frac{\\sum_{i=1}^a \\hat{y_i}}{a})^2\\] where \\(\\frac{\\sum_{i=1}^a \\hat{y_i}}{a} = \\bar{y}\\) is the grand mean.\nThe within-cluster variance (\\(s^2_i\\)) is estimated through\n\\[s^2_i = \\frac{1}{a(b_i - 1)} \\sum_{j = 1}^{b_i}(y_{ij} - \\hat{y_i})\\] where \\(y_{ij}\\) is the outcome for sampled unit \\(j\\) from cluster \\(i\\)"
  },
  {
    "objectID": "posts/2026-01-21-econometrics/index.html",
    "href": "posts/2026-01-21-econometrics/index.html",
    "title": "Econometrics: R introduction",
    "section": "",
    "text": "A number of assumptions underlying ordinary least square (OLS) regression come into close scrutiny under multiple linear regressions. Key assumptions include linearity, independence of residuals, homoscedasticity, normality of residuals.\n\n\nCode\npacman::p_load(\n  AER, tidyverse, mvtnorm, flextable, modelsummary, sandwich, stargazer,\n  MASS, rddtools, scales, broom, prettyunits\n)\ndata(\"CASchools\", package = \"AER\")\n\n#' Render model coefficients as a flextable\n#'\n#' @param x model object\n#'\n#' @returns\n#' @export\n#'\n#' @examples\nrender_summary &lt;- function(x){\n  x %&gt;% broom::tidy() %&gt;%\n    mutate(p.value = prettyunits::pretty_p_value(p.value)) %&gt;%\n    mutate(across(where(is.numeric), ~ round(., 3))) %&gt;%\n    flextable::flextable()\n}\n\n#' Render a list of models as flextable\n#'\n#' @param models list object of models\n#' @param robust_se list object of robust standard errors\n#'\n#' @returns\n#' @export\n#'\n#' @examples\nmodel_tables &lt;- function(models, robust_se) {\n  modelsummary(\n    models,\n    vcov = robust_se,\n    # Applies your robust SEs\n    output = \"flextable\",\n    # Outputs as a flextable object\n    fmt = 3,\n    # 3 decimal places\n    stars = TRUE,\n    # Significance stars (optional)\n    # gof_map = gm1 # Clean up GOF statistics\n    coef_omit = \"^\\\\(Intercept\\\\)$\"\n    ,\n    gof_omit = NULL\n  ) |&gt;\n    autofit() |&gt;\n    theme_vanilla()   %&gt;%         # Clean academic style\n    # This command removes any remaining horizontal lines inside the body\n    border_inner_h(part = \"body\", border = officer::fp_border(width = 0))\n}\n\n\n\n\nCode\n#|fig-height: 18\nm &lt;- lm(math ~ expenditure, data = CASchools)\n\npar(mfrow = c(2, 2))\nplot(m, col = \"steelblue\", pch = 20)\npar(mfrow = c(1, 1))\n\n\n\n\n\n\n\n\nFigure 1: OLS assumptions visual diagnostic\n\n\n\n\n\n\nA plot of residuals vs. fitted values assesses linearity and homoscedasticity assumption. A random (white noise) scatter around 0 suggests the assumptions are met.\nA normal quantile-quantile (Q-Q) plot assesses the normality assumption of the residuals with departure from the straight line suggesting departure from normality.\nA plot of standardized residuals against fitted values assesses homoscedasticity assumption. Look for white noise around the horizontal line in the scale location plot.\nThe residual vs. leverage plot assesses for influential outliers. Points far away from the center are considered influential.\n\nA number of factors may make multiple OLS regression biased due to a violation of any of the assumptions underlying OLS. Sources of these biases include:\nomitted variables, misspecification of the functional form, measurement errors, missing data and sample selection, simultaneous causality bias.\n\n\nThere is no easy fix for addressing omitted variables as a source of bias. Available guidelines stress\n\nspecifying coefficient(s) of interest\nIdentifying important sources of omitted variable bias using available domain knowledge prior to model fitting so you end up with a a baseline model specification and a questionable list of other regressors\nUsing different model specification to test whether questionable regressors have effects different from zero\nProviding full disclosure of your results such as results of different model specifications in support of your argument\n\nThe other sources of bias can be assessed via simulations.\n\n\n\nAssume the underlying function is such that \\(Y_i = X_i^2\\), but the model is specified in the form \\(Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i\\).\n\n\nCode\nset.seed(as.numeric(Sys.time()))\n\nX &lt;- runif(100, -5, 5)\nY &lt;- X^2 + rnorm(100)\n\nml &lt;- lm(Y ~ X)\n\nplot(\n  X,\n  Y,\n  main = \"Model misspecification\",\n  pch = 20,\n  col = \"steelblue\"\n  \n)\n\n# add the regression line\nabline(\n  ml,\n  col = \"grey\",\n  lwd = 2, \n  lty = 2\n)\n\nlegend(\n  \"bottomright\",\n  bg = \"transparent\",\n  cex = 0.8,\n  lwd = 2,\n  lty = 2,\n  col = \"grey\",\n  legend = \"OLS line\"\n  \n)\n\n\n\n\n\n\n\n\nFigure 2: Model misspecification\n\n\n\n\n\n\n\n\nSet in due to imprecise measurement of the independent variable and do not disappear not even with a large sample size. Due to imprecise measurements we observe \\(\\overset{\\sim}{X}_i\\) instead of \\(X_i\\) the model under consideration becomes\n\\[\\begin{align*}\n  Y_i =& \\, \\beta_0 + \\beta_1 \\overset{\\sim}{X}_i + \\underbrace{\\beta_1 (X_i - \\overset{\\sim}{X}_i) + u_i}_{=v_i}. \\\\\n  Y_i =& \\, \\beta_0 + \\beta_1 \\overset{\\sim}{X}_i + v_i.\n\\end{align*}\\]\nwhere \\(\\overset{\\sim}{X}_i\\) and the error term \\(v_i\\) are correlated. The OLS would thus be biased and inconsistent for \\(B_1\\) with the strength of bias dependent on the correlation between the observed regressor \\(\\overset{\\sim}{X}_i\\) and the measurement error \\(w_i = X_i - \\overset{\\sim}{X}_i\\). The classical measurement error model assumes that the measurement error, \\(w_i\\), has zero mean and that it is uncorrelated with the variable, \\(X_i\\), and the error term of the population regression model, \\(u_i\\):\n\\[\\begin{equation}\n  \\overset{\\sim}{X}_i = X_i + w_i, \\ \\ \\rho_{w_i,u_i}=0, \\ \\ \\rho_{w_i,X_i}=0.\n\\end{equation}\\] Then it holds that \\[\\begin{equation}\n  \\widehat{\\beta}_1 \\xrightarrow{p}{\\frac{\\sigma_{X}^2}{\\sigma_{X}^2 + \\sigma_{w}^2}} \\beta_1,\n\\end{equation}\n\\tag{1}\\] which implies inconsistency as \\(\\sigma_{X}^2, \\sigma_{w}^2 &gt; 0\\) such that the fraction in (Equation 1) is smaller than \\(1\\). Note that there are two extreme cases:\n\nIf there is no measurement error, \\(\\sigma_{w}^2=0\\) such that \\(\\widehat{\\beta}_1 \\xrightarrow{p}{\\beta_1}\\).\nIf \\(\\sigma_{w}^2 \\gg \\sigma_{X}^2\\) we have \\(\\widehat{\\beta}_1 \\xrightarrow{p}{0}\\). This is the case if the measurement error is so large that there essentially is no information on \\(X\\) in the data that can be used to estimate \\(\\beta_1\\).\n\nThe most obvious way to deal with errors-in-variables bias is to use an accurately measured \\(X\\). If this is not possible, instrumental variables regression is an option. One might also deal with the issue by using a mathematical model of the measurement error and adjust the estimates appropriately: if it is plausible that the classical measurement error model applies and if there is information that can be used to estimate the ratio in equation (Equation 1), one could compute an estimate that corrects for the downward bias.\n\n\nCode\ndat &lt;- data.frame(\n  rmvnorm(1000, c(50, 100), \n          sigma = cbind(c(10, 5), c(5, 10))))\n\n# set columns names\ncolnames(dat) &lt;- c(\"X\", \"Y\")\n\n# estimate the model (without measurement error)\nnoerror_mod &lt;- lm(Y ~ X, data = dat)\n\n# estimate the model (with measurement error in X)\ndat$X &lt;- dat$X + rnorm(n = 1000, sd = sqrt(10))\nerror_mod &lt;- lm(Y ~ X, data = dat)\n\n# print estimated coefficients to console\nnoerror_mod$coefficients\n\n\n(Intercept)           X \n 74.7383283   0.5046925 \n\n\nCode\n#&gt; (Intercept)           X \n#&gt;  76.3002047   0.4755264\nerror_mod$coefficients\n\n\n(Intercept)           X \n 88.8165365   0.2228252 \n\n\nCode\n#&gt; (Intercept)           X \n#&gt;   87.276004    0.255212\n#&gt;   \n# plot sample data\nplot(dat$X, dat$Y, \n     pch = 20, \n     col = \"steelblue\",\n     xlab = \"X\",\n     ylab = \"Y\")\n\n# add population regression function\nabline(coef = c(75, 0.5), \n       col = \"darkgreen\",\n       lwd  = 1.5)\n\n# add estimated regression functions\nabline(noerror_mod, \n       col = \"purple\",\n       lwd  = 1.5)\n\nabline(error_mod, \n       col = \"darkred\",\n       lwd  = 1.5)\n\n# add legend\nlegend(\"topleft\",\n       bg = \"transparent\",\n       cex = 0.8,\n       lty = 1,\n       col = c(\"darkgreen\", \"purple\", \"darkred\"), \n       legend = c(\"Population\", \"No Errors\", \"Errors\"))\n\n\n\n\n\n\n\n\nFigure 3: Measurement error\n\n\n\n\n\n\n\n\nIf data is not missing at random, we are faced with biased estimates.\n\n\nCode\n# randmly select 500 obs\nid &lt;- sample(1:1000, 500)\ndat1 &lt;- dat[-id,]\ndat2 &lt;- dat[id,]\n\nci_mod &lt;- lm(Y ~ X, data = dat1)\n\nplot(\n  dat1$X,\n  dat1$Y,\n  col = \"steelblue\",\n  pch = 20,\n  cex = 0.8,\n  xlab = \"X\",\n  ylab = \"Y\"\n)\n\npoints(\n  dat2$X,\n  dat2$Y,\n  cex = 0.8,\n  col = \"grey\",\n  pch = 20\n)\n\n# add pop reg function\nabline(\n  coef = c(75, 100),\n  col = \"darkgreen\",\n  lwd = 1.5\n)\n\n# add full sample model\nabline(noerror_mod, col = \"black\")\n\n# add model without a sample of 500\nabline(ci_mod, col = \"purple\")\n\nlegend(\n  \"bottomright\",\n  lty = 2,\n  bg = \"transparent\",\n  cex = 0.8,\n  col = c(\"darkgreen\", \"black\", \"purple\"),\n  legend = c(\"population\", \"full sample\", \"random sample of 500\")\n)\n\n\n\n\n\n\n\n\nFigure 4: Missing data bias\n\n\n\n\n\n\n\n\nArises when Y influences X. Let’s assume that we are interested in estimating the effect of 20% increase in price on cigarette. We get the log of price and use it to model consumption, giving a price elasticity of consumption interpretation. Since it is not demand on the Y-axis, this not an estimate of the demand curve, and is therefore a classic example of reverse (simultaneous) causality.\n\n\nCode\ndata(\"CigarettesSW\")\nc1995 &lt;- subset(CigarettesSW, year == \"1995\")\n\n# estimate the model\ncigcon_mod &lt;- lm(log(packs) ~ log(price), data = c1995)\ncigcon_mod\n\n\n\nCall:\nlm(formula = log(packs) ~ log(price), data = c1995)\n\nCoefficients:\n(Intercept)   log(price)  \n     10.850       -1.213  \n\n\nCode\n# plot the estimated regression line and the data\nplot(log(c1995$price), log(c1995$packs),\n     xlab = \"ln(Price)\",\n     ylab = \"ln(Consumption)\",\n     main = \"Demand for Cigarettes\",\n     pch = 20,\n     col = \"steelblue\")\n\nabline(cigcon_mod, \n       col = \"darkred\", \n       lwd = 1.5)\n# add legend\nlegend(\"topright\",lty=1,col= \"darkred\", \"Estimated Regression Line\")\n\n\n\n\n\n\n\n\nFigure 5: Reverse causlaity bias"
  },
  {
    "objectID": "posts/2026-01-21-econometrics/index.html#omitted-variables",
    "href": "posts/2026-01-21-econometrics/index.html#omitted-variables",
    "title": "Econometrics: R introduction",
    "section": "",
    "text": "There is no easy fix for addressing omitted variables as a source of bias. Available guidelines stress\n\nspecifying coefficient(s) of interest\nIdentifying important sources of omitted variable bias using available domain knowledge prior to model fitting so you end up with a a baseline model specification and a questionable list of other regressors\nUsing different model specification to test whether questionable regressors have effects different from zero\nProviding full disclosure of your results such as results of different model specifications in support of your argument\n\nThe other sources of bias can be assessed via simulations."
  },
  {
    "objectID": "posts/2026-01-21-econometrics/index.html#misspecification-of-the-functional-form-of-a-regression-function",
    "href": "posts/2026-01-21-econometrics/index.html#misspecification-of-the-functional-form-of-a-regression-function",
    "title": "Econometrics: R introduction",
    "section": "",
    "text": "Assume the underlying function is such that \\(Y_i = X_i^2\\), but the model is specified in the form \\(Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i\\).\n\n\nCode\nset.seed(as.numeric(Sys.time()))\n\nX &lt;- runif(100, -5, 5)\nY &lt;- X^2 + rnorm(100)\n\nml &lt;- lm(Y ~ X)\n\nplot(\n  X,\n  Y,\n  main = \"Model misspecification\",\n  pch = 20,\n  col = \"steelblue\"\n  \n)\n\n# add the regression line\nabline(\n  ml,\n  col = \"grey\",\n  lwd = 2, \n  lty = 2\n)\n\nlegend(\n  \"bottomright\",\n  bg = \"transparent\",\n  cex = 0.8,\n  lwd = 2,\n  lty = 2,\n  col = \"grey\",\n  legend = \"OLS line\"\n  \n)\n\n\n\n\n\n\n\n\nFigure 2: Model misspecification"
  },
  {
    "objectID": "posts/2026-01-21-econometrics/index.html#measurement-error-and-errors-in-variable-bias",
    "href": "posts/2026-01-21-econometrics/index.html#measurement-error-and-errors-in-variable-bias",
    "title": "Econometrics: R introduction",
    "section": "",
    "text": "Set in due to imprecise measurement of the independent variable and do not disappear not even with a large sample size. Due to imprecise measurements we observe \\(\\overset{\\sim}{X}_i\\) instead of \\(X_i\\) the model under consideration becomes\n\\[\\begin{align*}\n  Y_i =& \\, \\beta_0 + \\beta_1 \\overset{\\sim}{X}_i + \\underbrace{\\beta_1 (X_i - \\overset{\\sim}{X}_i) + u_i}_{=v_i}. \\\\\n  Y_i =& \\, \\beta_0 + \\beta_1 \\overset{\\sim}{X}_i + v_i.\n\\end{align*}\\]\nwhere \\(\\overset{\\sim}{X}_i\\) and the error term \\(v_i\\) are correlated. The OLS would thus be biased and inconsistent for \\(B_1\\) with the strength of bias dependent on the correlation between the observed regressor \\(\\overset{\\sim}{X}_i\\) and the measurement error \\(w_i = X_i - \\overset{\\sim}{X}_i\\). The classical measurement error model assumes that the measurement error, \\(w_i\\), has zero mean and that it is uncorrelated with the variable, \\(X_i\\), and the error term of the population regression model, \\(u_i\\):\n\\[\\begin{equation}\n  \\overset{\\sim}{X}_i = X_i + w_i, \\ \\ \\rho_{w_i,u_i}=0, \\ \\ \\rho_{w_i,X_i}=0.\n\\end{equation}\\] Then it holds that \\[\\begin{equation}\n  \\widehat{\\beta}_1 \\xrightarrow{p}{\\frac{\\sigma_{X}^2}{\\sigma_{X}^2 + \\sigma_{w}^2}} \\beta_1,\n\\end{equation}\n\\tag{1}\\] which implies inconsistency as \\(\\sigma_{X}^2, \\sigma_{w}^2 &gt; 0\\) such that the fraction in (Equation 1) is smaller than \\(1\\). Note that there are two extreme cases:\n\nIf there is no measurement error, \\(\\sigma_{w}^2=0\\) such that \\(\\widehat{\\beta}_1 \\xrightarrow{p}{\\beta_1}\\).\nIf \\(\\sigma_{w}^2 \\gg \\sigma_{X}^2\\) we have \\(\\widehat{\\beta}_1 \\xrightarrow{p}{0}\\). This is the case if the measurement error is so large that there essentially is no information on \\(X\\) in the data that can be used to estimate \\(\\beta_1\\).\n\nThe most obvious way to deal with errors-in-variables bias is to use an accurately measured \\(X\\). If this is not possible, instrumental variables regression is an option. One might also deal with the issue by using a mathematical model of the measurement error and adjust the estimates appropriately: if it is plausible that the classical measurement error model applies and if there is information that can be used to estimate the ratio in equation (Equation 1), one could compute an estimate that corrects for the downward bias.\n\n\nCode\ndat &lt;- data.frame(\n  rmvnorm(1000, c(50, 100), \n          sigma = cbind(c(10, 5), c(5, 10))))\n\n# set columns names\ncolnames(dat) &lt;- c(\"X\", \"Y\")\n\n# estimate the model (without measurement error)\nnoerror_mod &lt;- lm(Y ~ X, data = dat)\n\n# estimate the model (with measurement error in X)\ndat$X &lt;- dat$X + rnorm(n = 1000, sd = sqrt(10))\nerror_mod &lt;- lm(Y ~ X, data = dat)\n\n# print estimated coefficients to console\nnoerror_mod$coefficients\n\n\n(Intercept)           X \n 74.7383283   0.5046925 \n\n\nCode\n#&gt; (Intercept)           X \n#&gt;  76.3002047   0.4755264\nerror_mod$coefficients\n\n\n(Intercept)           X \n 88.8165365   0.2228252 \n\n\nCode\n#&gt; (Intercept)           X \n#&gt;   87.276004    0.255212\n#&gt;   \n# plot sample data\nplot(dat$X, dat$Y, \n     pch = 20, \n     col = \"steelblue\",\n     xlab = \"X\",\n     ylab = \"Y\")\n\n# add population regression function\nabline(coef = c(75, 0.5), \n       col = \"darkgreen\",\n       lwd  = 1.5)\n\n# add estimated regression functions\nabline(noerror_mod, \n       col = \"purple\",\n       lwd  = 1.5)\n\nabline(error_mod, \n       col = \"darkred\",\n       lwd  = 1.5)\n\n# add legend\nlegend(\"topleft\",\n       bg = \"transparent\",\n       cex = 0.8,\n       lty = 1,\n       col = c(\"darkgreen\", \"purple\", \"darkred\"), \n       legend = c(\"Population\", \"No Errors\", \"Errors\"))\n\n\n\n\n\n\n\n\nFigure 3: Measurement error"
  },
  {
    "objectID": "posts/2026-01-21-econometrics/index.html#missing-data-and-sample-selection",
    "href": "posts/2026-01-21-econometrics/index.html#missing-data-and-sample-selection",
    "title": "Econometrics: R introduction",
    "section": "",
    "text": "If data is not missing at random, we are faced with biased estimates.\n\n\nCode\n# randmly select 500 obs\nid &lt;- sample(1:1000, 500)\ndat1 &lt;- dat[-id,]\ndat2 &lt;- dat[id,]\n\nci_mod &lt;- lm(Y ~ X, data = dat1)\n\nplot(\n  dat1$X,\n  dat1$Y,\n  col = \"steelblue\",\n  pch = 20,\n  cex = 0.8,\n  xlab = \"X\",\n  ylab = \"Y\"\n)\n\npoints(\n  dat2$X,\n  dat2$Y,\n  cex = 0.8,\n  col = \"grey\",\n  pch = 20\n)\n\n# add pop reg function\nabline(\n  coef = c(75, 100),\n  col = \"darkgreen\",\n  lwd = 1.5\n)\n\n# add full sample model\nabline(noerror_mod, col = \"black\")\n\n# add model without a sample of 500\nabline(ci_mod, col = \"purple\")\n\nlegend(\n  \"bottomright\",\n  lty = 2,\n  bg = \"transparent\",\n  cex = 0.8,\n  col = c(\"darkgreen\", \"black\", \"purple\"),\n  legend = c(\"population\", \"full sample\", \"random sample of 500\")\n)\n\n\n\n\n\n\n\n\nFigure 4: Missing data bias"
  },
  {
    "objectID": "posts/2026-01-21-econometrics/index.html#simultaneous-causality-bias",
    "href": "posts/2026-01-21-econometrics/index.html#simultaneous-causality-bias",
    "title": "Econometrics: R introduction",
    "section": "",
    "text": "Arises when Y influences X. Let’s assume that we are interested in estimating the effect of 20% increase in price on cigarette. We get the log of price and use it to model consumption, giving a price elasticity of consumption interpretation. Since it is not demand on the Y-axis, this not an estimate of the demand curve, and is therefore a classic example of reverse (simultaneous) causality.\n\n\nCode\ndata(\"CigarettesSW\")\nc1995 &lt;- subset(CigarettesSW, year == \"1995\")\n\n# estimate the model\ncigcon_mod &lt;- lm(log(packs) ~ log(price), data = c1995)\ncigcon_mod\n\n\n\nCall:\nlm(formula = log(packs) ~ log(price), data = c1995)\n\nCoefficients:\n(Intercept)   log(price)  \n     10.850       -1.213  \n\n\nCode\n# plot the estimated regression line and the data\nplot(log(c1995$price), log(c1995$packs),\n     xlab = \"ln(Price)\",\n     ylab = \"ln(Consumption)\",\n     main = \"Demand for Cigarettes\",\n     pch = 20,\n     col = \"steelblue\")\n\nabline(cigcon_mod, \n       col = \"darkred\", \n       lwd = 1.5)\n# add legend\nlegend(\"topright\",lty=1,col= \"darkred\", \"Estimated Regression Line\")\n\n\n\n\n\n\n\n\nFigure 5: Reverse causlaity bias"
  },
  {
    "objectID": "posts/2026-01-21-econometrics/index.html#simple-instrumental-variable-regression",
    "href": "posts/2026-01-21-econometrics/index.html#simple-instrumental-variable-regression",
    "title": "Econometrics: R introduction",
    "section": "Simple instrumental variable regression",
    "text": "Simple instrumental variable regression\nInstrumental variables (IV) can be used to address possible omitted variable bias. Considering a simple regression model\n\\[\n\\begin{equation}\nY_i = \\beta_0 + \\beta_1X_i + \\epsilon_i, i = 1, \\ldots, n\n\\end{equation}\n\\tag{2}\\]\nin which the error term is correlated with the endogenous regressor \\(X\\) making the OLS estimate inconsistent for \\(\\beta_1\\), we can use a single IV variable \\(Z\\) to obtain a consistent estimator for \\(\\beta_1\\). The IV variable has to meet the\n\nrelevance condition: \\(X\\) and its instrument \\(Z\\) must be correlated \\(\\rho_{Z_i, X_i} \\ne 0\\) or multicollinear for multiple IV regression.\ninstrument exogeneity condition: The instrument \\(Z\\) must not be correlated with the error term. \\(\\rho_{Z_i, \\epsilon_i} = 0\\)\n\nEstimation is through a two-stage least square (TSLS) estimation. The first stage condenses the variation in the endogenous variable \\(X\\) into a problem-free part that is explained by the IV variable \\(Z\\), and a problematic component that is correlated to the error term \\(\\epsilon_i\\). In the second stage the problem-free component of the variation in \\(X\\) is used to estimate \\(\\beta_1\\).\nThe first stage uses the regression model \\(X_i = \\pi_0 + \\pi_1Z_i + v_i\\), where \\(\\pi_0 + \\pi_1Z_i\\) is the component of \\(X_i\\) explained by \\(Z_i\\), and \\(v_i\\) is the component that can not be explained by \\(Z_i\\) and is correlated with \\(\\epsilon_i\\). We can use OLS estimates \\(\\hat{\\pi_0}\\) and \\(\\hat{\\pi_1}\\) to predict \\(X_i\\) giving problem-free \\(\\hat{X_i}\\) (\\(\\hat{X_i}\\) exogenous in the regression of Y on \\(\\hat{X}\\) done on second stage ) if \\(Z\\) is indeed a valid instrument. The second stage produces \\(\\hat{\\beta}_0^{TSLS}\\), and \\(\\hat{\\beta}_1^{TSLS}\\), which are the TSLS estimates of \\(\\beta_0\\) and \\(\\beta_1\\) respectively. For a single IV it can be shown that the TSLS estimator is but the ratio of sample covariance between \\(Z\\) and \\(Y\\)\n\\[\n\\begin{equation}\n\\hat{\\beta}^{TSLS} = \\frac{s_{Z, Y}}{s_{Z, X}} = \\frac{\\frac{1}{n-1} \\sum_{i=1}^n (Y_i - \\bar{Y})(Z_i - \\bar{Z})}{\\frac{1}{n-1}\\sum_{i=1}^n (X_i - \\bar{X}) (Z_i - \\bar{Z})}\n\\end{equation}\n\\tag{3}\\]\n\nExample: Demand for cigarettes\n\n\nCode\nskimr::skim(CigarettesSW)\n\n\n\nData summary\n\n\nName\nCigarettesSW\n\n\nNumber of rows\n96\n\n\nNumber of columns\n9\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nstate\n0\n1\nFALSE\n48\nAL: 2, AR: 2, AZ: 2, CA: 2\n\n\nyear\n0\n1\nFALSE\n2\n198: 48, 199: 48\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ncpi\n0\n1\n1.30\n0.23\n1.08\n1.08\n1.30\n1.52000e+00\n1.52\n▇▁▁▁▇\n\n\npopulation\n0\n1\n5168866.32\n5442344.66\n478447.00\n1622606.50\n3697471.50\n5.90150e+06\n31493524.00\n▇▂▁▁▁\n\n\npacks\n0\n1\n109.18\n25.87\n49.27\n92.45\n110.16\n1.23520e+02\n197.99\n▂▇▇▁▁\n\n\nincome\n0\n1\n99878735.74\n120541138.18\n6887097.00\n25520384.00\n61661644.00\n1.27314e+08\n771470144.00\n▇▁▁▁▁\n\n\ntax\n0\n1\n42.68\n16.14\n18.00\n31.00\n37.00\n5.08800e+01\n99.00\n▇▆▃▂▁\n\n\nprice\n0\n1\n143.45\n43.89\n84.97\n102.71\n137.72\n1.76150e+02\n240.85\n▇▁▅▂▂\n\n\ntaxs\n0\n1\n48.33\n19.33\n21.27\n34.77\n41.05\n5.94800e+01\n112.63\n▇▅▂▁▁\n\n\n\n\n\nWe wish to estimate \\(\\beta_1\\) in\n\\[\n\\begin{equation}\nlog(Q_i) = \\beta_0 + \\beta_1 log(P_i) + \\epsilon_i\n\\end{equation}\n\\tag{4}\\]\nwhere \\(Q_i\\) is the number of cigarette packs per capita sold and \\(P_i\\) is the after tax average real price per pack of cigarette in state \\(i\\). We use sales tax (dollars per pack) to instrument the endogenous regressor \\(log(P_i)\\). Salestax is considered a relevant instrument as it is included in the after-tax average price per pack.\n\n\nCode\n# compute real per capita prices\nCigarettesSW$rprice &lt;- with(CigarettesSW, price / cpi)\n\n#  compute the sales tax\nCigarettesSW$salestax &lt;- with(CigarettesSW, (taxs - tax) / cpi)\n\n# check the correlation between sales tax and price\ncor(CigarettesSW$salestax, CigarettesSW$price)\n\n\n[1] 0.6141228\n\n\nCode\n#&gt; [1] 0.6141228\n\n# generate a subset for the year 1995\nc1995 &lt;- subset(CigarettesSW, year == \"1995\")\n\n\nThe first stage regression is \\(log(P_i) = \\pi_0 + \\pi_1SalesTax_i + v_i\\). In the second stage we regress \\(log(Q_i)\\) on \\(log(P_i)\\) to obtain \\(\\hat{\\beta_0}^{TSLS}\\) \\(\\hat{\\beta_1}^{TSLS}\\).\n\n\nCode\n# perform the first stage regression\ncig_s1 &lt;- lm(log(rprice) ~ salestax, data = c1995)\n\ncoeftest(cig_s1, vcov = vcovHC, type = \"HC1\") %&gt;% broom::tidy() %&gt;%\n  mutate(p.value = prettyunits::pretty_p_value(p.value)) %&gt;% \n  flextable::flextable()\n\n\ntermestimatestd.errorstatisticp.value(Intercept)4.616546320.028917686159.644388&lt;0.0001salestax0.030728860.0048354326.354935&lt;0.0001\n\n\nCode\n# store the fitted values from first stage\nlcigp_pred &lt;- cig_s1$fitted.values\n\n\nRun the second stage:\n\n\nCode\ncig_s2 &lt;- lm(log(c1995$packs) ~ lcigp_pred)\ncoeftest(cig_s2, vcov. = vcovHC) %&gt;% broom::tidy() %&gt;%\n  mutate(p.value = prettyunits::pretty_p_value(p.value)) %&gt;%\n  flextable::flextable()\n\n\ntermestimatestd.errorstatisticp.value(Intercept)9.7198771.70303575.707383&lt;0.0001lcigp_pred-1.0835870.3556347-3.0469100.0038\n\n\nThus estimating the model (Equation 4) using TSLS yields\n\\[\n\\begin{equation}\n  \\widehat{\\log(Q_i)} = \\underset{(1.70)}{9.72} - \\underset{(0.36)}{1.08} \\log(P_i)\n\\end{equation}\n\\tag{5}\\]\nwhere we write \\(\\log(P_i)\\) instead of \\(\\widehat{\\log(P_i)}\\) for consistency. See AER::ivreg() for automatic IV regression."
  },
  {
    "objectID": "posts/2026-01-21-econometrics/index.html#the-general-iv-regression-model",
    "href": "posts/2026-01-21-econometrics/index.html#the-general-iv-regression-model",
    "title": "Econometrics: R introduction",
    "section": "The general IV regression model",
    "text": "The general IV regression model\nAn extension of simple IV regression to multiple predictors:\n\\[\n\\begin{equation}\n  Y_i = \\beta_0 + \\beta_1 X_{1i} + \\dots + \\beta_k X_{ki} + \\beta_{k+1} W_{1i} + \\dots + \\beta_{k+r} W_{ri} + \\epsilon_i,\n\\end{equation}\n\\tag{6}\\]\nwith \\(i=1,\\dots,n\\) is the general instrumental variables regression model where\n\n\\(Y_i\\) is the dependent variable,\n\\(\\\\beta_0,\\dots,\\beta_{k+1}\\) are \\(1+k+r\\) unknown regression coefficients,\n\\(X_{1i},\\dots,X_{ki}\\) are \\(k\\) endogenous regressors ,\n\\(W_{1i},\\dots,W_{ri}\\) are \\(r\\) exogenous regressors which are uncorrelated with \\(u_i\\),\n\\(\\epsilon_i\\) is the error term,\n\\(Z_{1i},\\dots,Z_{mi}\\) are \\(m\\) instrumental variables.\n\nThe coefficients are overidentified if \\(m&gt;k\\). If \\(m&lt;k\\), the coefficients are underidentified and when \\(m=k\\) they are exactly identified. For estimation of the IV regression model we require exact identification or overidentification. The computations can be autometed with AER::ivreg() function.\nSuppose we wish to estimate the model \\(Y_i = \\beta_0 + \\beta_1X_{1i} + \\beta_2X_{2i} + W_{1i} + \\epsilon_i\\), where \\(X_{1i}\\) and \\(X_{2i}\\) are endogenous variables instrumented by \\(Z_{1i}\\) and \\(Z_{2i}\\) respectively, and \\(W_{1i}\\) is an exogenous variable, a correct model specification involves listing all exogenous variables as instruments too by joining them with +s on the right of the vertical bar |:\ny ~ X1 + X2 + W1 | W1 + Z1 + Z2. A convenient way of specifying a model is updating a formula using . (which includes all variables except for the outcome). For instance, if we have one exogenous regressor \\(w1\\) and one endogenous regrossor \\(x1\\) with instrument \\(z1\\), the appropriate formula would be y ~ w1 + x1 | w1 + z1 \\(=\\) y ~ w1 + x1 | . ~ x1 + z1.\nSome assumptions underlying the IV regression are:\n\n\\(E(\\epsilon_i|W_{1i}, \\dots, W_{ri}) = 0\\)\n\\((X_{1i}, \\dots, X_{ki}, W_{1i}, \\dots, W_{ri}, Z_{1i}, \\dots, Z_{mi})\\) are i.i.d draws from their joint distribution\nAll variables have nonzero finite fourth moment, i.e., outliers are unlikely\nThe \\(Z_s\\) are valid instruments.\n\n\nExample: Demands for cigarettes\nWe update (Equation 4) to include income:\n\\[\n\\begin{equation}\nlog(Q_i) = \\beta_0 + \\beta_1 log(P_i) + \\beta_2 log(income_i) +  \\epsilon_i\n\\end{equation}\n\\tag{7}\\]\nwhere income is defined as real per capita income.\n\n\nCode\n# add rincome to the dataset\nCigarettesSW$rincome &lt;- with(CigarettesSW, income / population / cpi)\n\nc1995 &lt;- subset(CigarettesSW, year == \"1995\")\n\n# estimate the model\ncig_ivreg2 &lt;- ivreg(log(packs) ~ log(rprice) + log(rincome) | log(rincome) + \n                    salestax, data = c1995)\n\ncoeftest(cig_ivreg2, vcov = vcovHC, type = \"HC1\") %&gt;%\n  broom::tidy() %&gt;%\n  mutate(p.value = prettyunits::pretty_p_value(p.value)) %&gt;%\n  flextable::flextable()\n\n\ntermestimatestd.errorstatisticp.value(Intercept)9.43065831.25939267.4882595&lt;0.0001log(rprice)-1.14337510.3723027-3.07109020.0036log(rincome)0.21451530.31174690.68810710.4949\n\n\nWe obtain\n\\[\n\\begin{equation}\n  \\widehat{\\log(Q_i)} = \\underset{(1.26)}{9.43} - \\underset{(0.37)}{1.14} \\log(P_i) + \\underset{(0.31)}{0.21} \\log(income_i)\n\\end{equation}\n\\tag{8}\\]\nAdding salestax as an IV:\n\n\nCode\n# add cigtax to the data set\nCigarettesSW$cigtax &lt;- with(CigarettesSW, tax/cpi)\n\nc1995 &lt;- subset(CigarettesSW, year == \"1995\")\n\n# estimate the model\ncig_ivreg3 &lt;- ivreg(log(packs) ~ log(rprice) + log(rincome) | \n                    log(rincome) + salestax + cigtax, data = c1995)\n\ncoeftest(cig_ivreg3, vcov = vcovHC, type = \"HC1\") %&gt;% \n  broom::tidy() %&gt;%\n  mutate(p.value = prettyunits::pretty_p_value(p.value)) %&gt;%\n  flextable::flextable()\n\n\ntermestimatestd.errorstatisticp.value(Intercept)9.89495550.959216910.315660&lt;0.0001log(rprice)-1.27742410.2496100-5.117680&lt;0.0001log(rincome)0.28040480.25388971.1044360.2753\n\n\n\\[\n\\begin{equation}\n  \\widehat{\\log(Q_i)} = \\underset{(0.96)}{9.89} - \\underset{(0.25)}{1.28} \\log(P_i) + \\underset{(0.25)}{0.28} \\log(income_i)\n\\end{equation}\n\\tag{9}\\]\nA question then arises on which estimates to trust. One could argue that the estimates from (Equation 9) are more trustworthy due to the lower standard errors compared to (Equation 8). This is the subject considered under instrument validity."
  },
  {
    "objectID": "posts/2026-01-21-econometrics/index.html#checking-instrument-validity",
    "href": "posts/2026-01-21-econometrics/index.html#checking-instrument-validity",
    "title": "Econometrics: R introduction",
    "section": "Checking instrument validity",
    "text": "Checking instrument validity\n\nWeak instrument: To check for a weak instrument, compute the \\(F\\)-statistic corresponding to the hypothesis \\(H_0: Z_1 = \\dots Z_m = 0\\). A rule of thumb is that an \\(F\\)-statistic less than 10 imply weak instruments. If your instruments are weak, you can discard them and find better instruments or stick with them but find methods that improve TSLS.\nInstrument exogeneity assumption is not met: When we observe a correlation between an instrument and the error term, the IV regression is not consistent. Overidentifying restriction test (J-test) is used to test that additional instruments are exogenous. It follows \\(\\chi^2_{m -k}\\) (under homoscedasticity), where \\(m\\) is the number of instruments and \\(k\\) the number of endogenous regressors. It involves regressing the error term on all instruments and endogenous variables:\n\n\\[\n\\begin{equation}\n\\hat{\\epsilon}^{TSLS} = \\delta_0 + \\delta_1Z_{1i} + \\dots + \\delta_mZ_{mi} +\n\\delta_{m+1}W_{1i} + \\dots + \\delta_{m+r}W_{ri} + e_i\n\\end{equation}\n\\tag{10}\\]\nand testing whether all instruments are exogenous through the joint hypothesis \\(H_0: \\delta_1 = 0, \\dots, \\delta_m = 0\\), and computing the statistic \\(J = mF\\). (see https://www.econometrics-with-r.org/12.4-attdfc.html)\n\nExample: Demand for cigarettes\nWe assess whether general sales tax and the cigarette-specific tax are valid instruments. It could be argued that tobacco growing states find economic importance in lower tobacco taxes and may lobby for them staying low. It may also be possible that tobacco growing states have higher rates of smoking than others, which would lead to endogeneity of cigarette-specific taxes. This can only be solved by including data on the size of the tobacco and cigarette industry into the regression, which is not available in this case.\nIt can be assumed that the role of tobacco and cigarette industry differs across states, but not across time. We can then exploit the panel structure of the data at two different time periods (1985, 1995) to estimate the long-run elasticity of the demand for cigarettes:\n\\[\n\\begin{equation}\nlog(Q_{i, 1995}) - log(Q_{i, 1985}) = \\beta_0 + \\beta_1[log(P_{i, 1995}) -\nlog(P_{i, 1985})] + \\beta_2[log(income_{i, 1995}) - log(income_{i, 1985})] + \\epsilon_i\n\\end{equation}\n\\tag{11}\\]\n\n\nCode\n# subset data for year 1985\nc1985 &lt;- subset(CigarettesSW, year == \"1985\")\n\n# define differences in variables\npacksdiff &lt;- log(c1995$packs) - log(c1985$packs)\n\npricediff &lt;- log(c1995$price/c1995$cpi) - log(c1985$price/c1985$cpi)\n\nincomediff &lt;- log(c1995$income/c1995$population/c1995$cpi) -\nlog(c1985$income/c1985$population/c1985$cpi)\n\nsalestaxdiff &lt;- (c1995$taxs - c1995$tax)/c1995$cpi - \n                                  (c1985$taxs - c1985$tax)/c1985$cpi\n\ncigtaxdiff &lt;- c1995$tax/c1995$cpi - c1985$tax/c1985$cpi\n\n\nWe then perform 3 different IV estimations using ivreg()for differences between 1985, 1995:\n\nTSLS using only the difference in sales taxes as the IV\nTSLS using only difference in cigarette-specific taxes\nTSLS using both differences in sales taxes and cigarette-specific taxes\n\n\n\nCode\n# estimating the 3 models\ncig_ivreg_diff1 &lt;- ivreg(\n  packsdiff ~ pricediff + incomediff | incomediff + salestaxdiff\n)\n\ncig_ivreg_diff2 &lt;- cig_ivreg_diff1 &lt;- ivreg(\n  packsdiff ~ pricediff + incomediff | incomediff + cigtaxdiff\n)\n\ncig_ivreg_diff3 &lt;- cig_ivreg_diff1 &lt;- ivreg(\n  packsdiff ~ pricediff + incomediff | incomediff + salestaxdiff + cigtaxdiff\n)\n\n\nSales tax as IV\n\n\nCode\ncoeftest(cig_ivreg_diff1, vcov. = vcovHC, type = \"HC1\") %&gt;%\n  render_summary()\n\n\n\n\nTable 1: Difference in sales tax as IV\n\n\n\ntermestimatestd.errorstatisticp.value(Intercept)-0.0520.062-0.8320.4097pricediff-1.2020.197-6.105&lt;0.0001incomediff0.4620.3091.4940.1423\n\n\n\n\n\nCigarette-specific tax as IV\n\n\nCode\ncoeftest(cig_ivreg_diff2, vcov. = vcovHC, type = \"HC1\") %&gt;% \n  render_summary()\n\n\n\n\nTable 2: Cigarette-specific tax as IV\n\n\n\ntermestimatestd.errorstatisticp.value(Intercept)-0.0170.067-0.2540.8009pricediff-1.3430.229-5.871&lt;0.0001incomediff0.4280.2991.4330.1587\n\n\n\n\n\nBoth sales tax and cigarette-specific tax as IV\n\n\nCode\ncoeftest(cig_ivreg_diff3, vcov. = vcovHC, type = \"HC1\") %&gt;%\n  render_summary()\n\n\n\n\nTable 3: Both sales tax and cigarette-specific tax as IV\n\n\n\ntermestimatestd.errorstatisticp.value(Intercept)-0.0520.062-0.8320.4097pricediff-1.2020.197-6.105&lt;0.0001incomediff0.4620.3091.4940.1423\n\n\n\n\n\nA tabulated summary\n\n\nCode\n# 1. Define the models in a named list (this replaces column.labels)\nmodels &lt;- list(\n  \"IV: salestax\" = cig_ivreg_diff1,\n  \"IV: cigtax\"   = cig_ivreg_diff2,\n  \"IVs: salestax, cigtax\" = cig_ivreg_diff3\n)\n\n# Define which Goodness-of-Fit stats to show and what to call them\ngm &lt;- list(\n  list(\"raw\" = \"nobs\", \"clean\" = \"Observations\", \"fmt\" = 0),\n  list(\"raw\" = \"r.squared\", \"clean\" = \"R2\", \"fmt\" = 3),\n  list(\"raw\" = \"adj.r.squared\", \"clean\" = \"Adj. R2\", \"fmt\" = 3)\n)\n\n# 2. Define standard errors\n# Note: modelsummary can calculate HC1 automatically, \n# but since you already have 'rob_se', we can pass it directly.\nrob_se_list &lt;- list(\n  sqrt(diag(vcovHC(cig_ivreg_diff1, type = \"HC1\"))),\n  sqrt(diag(vcovHC(cig_ivreg_diff2, type = \"HC1\"))),\n  sqrt(diag(vcovHC(cig_ivreg_diff3, type = \"HC1\")))\n)\n\n# 3. Generate the table\nmodelsummary(\n  models,\n  vcov = rob_se_list,        # Applies your robust SEs\n  output = \"flextable\",      # Outputs as a flextable object\n  fmt = 3,                   # 3 decimal places\n  stars = TRUE,              # Significance stars (optional)\n  gof_map = gm # Clean up GOF statistics\n) |&gt; \n  autofit() |&gt; \n  theme_vanilla()   %&gt;%         # Clean academic style\n# This command removes any remaining horizontal lines inside the body\n  border_inner_h(part = \"body\", border = officer::fp_border(width = 0))\n\n\n\n\nTable 4: Dependent Variable: 1985-1995 Difference in Log per Pack Price\n\n\n\n IV: salestaxIV: cigtaxIVs: salestax, cigtax(Intercept)-0.052-0.017-0.052(0.062)(0.067)(0.062)pricediff-1.202***-1.343***-1.202***(0.197)(0.229)(0.197)incomediff0.4620.4280.462(0.309)(0.299)(0.309)Observations484848R20.5470.5200.547Adj. R20.5260.4980.526+ p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\nWe observe negative estimates for pricediff that differ when cigtax is used as IV. We proceed to test the validity of the instruments by computing F-statistics for the first-stage regressions.\n\n\nCode\n# first-stage regressions\nmod_relevance1 &lt;- lm(pricediff ~ salestaxdiff + incomediff)\nmod_relevance2 &lt;- lm(pricediff ~ cigtaxdiff + incomediff)\nmod_relevance3 &lt;- lm(pricediff ~ incomediff + salestaxdiff + cigtaxdiff)\n\n# check instrument relevance\n\nmods &lt;- list(mod_relevance1, mod_relevance2, mod_relevance3)\nnames(mods) &lt;- c(\"salestax\", \"cigartax\", \"both\")\nhyp &lt;- list(\"salestaxdiff = 0\", \"cigtaxdiff = 0\", c(\"salestaxdiff = 0\", \"cigtaxdiff = 0\"))\n\niv_tests &lt;- map2(\n  mods, hyp, \\(m, h){\n    linearHypothesis(m, h, vcov. = vcovHC(m, type = \"HC1\"))\n  }\n) \n\n iv_tests |&gt;\n  map_dfr(broom::tidy, .id = \"model\") %&gt;% \n   mutate(p.value = prettyunits::pretty_p_value(p.value)) %&gt;%\n   mutate(\n     across(where(is.numeric), ~ round(., 3))\n   ) %&gt;% \n   flextable()\n\n\n\n\nTable 5: Test IV relevance by an F-test. All the F-statistics are significantly different from zero\n\n\n\nmodeltermnull.valueestimatestd.errorstatisticp.valuedf.residualdfsalestaxsalestaxdiff00.0250.00433.674&lt;0.0001451cigartaxcigtaxdiff00.0100.001107.183&lt;0.0001451bothsalestaxdiff00.0130.00388.616&lt;0.0001442bothcigtaxdiff00.0080.00188.616&lt;0.0001442\n\n\n\n\n\nNext we conduct overidentifying restriction test for model with both sales tax and cigatax as IV which is the only model where the coefficient on the difference in log prices is overidentified (\\(m = 2, k=1\\)) making it possible to compute the \\(J\\) statistic. To achive this, we regress the residuals from cig_ivreg_diff3 on the IVs and the presumed exogenous regressor incomediff. We again test if the coefficients of the IV variables is zero, a condition necessary for exogenous variables.\n\n\nCode\n# compute the J-statistic\ncig_iv_OR &lt;- lm(residuals(cig_ivreg_diff3) ~ incomediff + salestaxdiff + cigtaxdiff)\n\ncig_OR_test &lt;- linearHypothesis(cig_iv_OR, \n                               c(\"salestaxdiff = 0\", \"cigtaxdiff = 0\"), \n                               test = \"Chisq\")\ncig_OR_test %&gt;% render_summary()\n\n\n\n\nTable 6: J-test statitic testing for exogeneity condition of IVs\n\n\n\ntermnull.valueestimatestd.errorstatisticp.valuedf.residualrssdfsumsqsalestaxdiff00.0130.0064.9320.0849440.33720.038cigtaxdiff0-0.0040.0024.9320.0849440.33720.038\n\n\n\n\n\nNote: The reported degrees of freedom is wrong, (2 instead of \\(m-k = 2-1=1\\)). The J-statistic is \\(\\chi^2_1\\) instead of \\(\\chi^2_2\\) as reported by the model. We calculate the correct p-value:\n\n\nCode\npchisq(cig_OR_test[2, 5], df = 1, lower.tail = FALSE)\n\n\n[1] 0.02636406\n\n\nWe reject the hypothesis that both instruments are exogenous at \\(\\alpha = 0.05\\) and conclude that\n\nsales tax is an invalid instrument for the per-pack price,\nand so is the cigarette-specific tax,\nand that both instruments are invalid."
  },
  {
    "objectID": "posts/2026-01-21-econometrics/index.html#potential-outcomes-causal-effects-and-idealized-experiments",
    "href": "posts/2026-01-21-econometrics/index.html#potential-outcomes-causal-effects-and-idealized-experiments",
    "title": "Econometrics: R introduction",
    "section": "Potential outcomes, causal effects and idealized experiments",
    "text": "Potential outcomes, causal effects and idealized experiments\nInterest is in the average causal effect estimated by the difference estimator. A potential outcome is the outcome for an individual under a potential treatment. For this individual the causal effect of the treatment is the difference between the potential outcome if the individual receives the treatment and the potential outcome if they do not. We are interested in studying the average causal effect since this difference may vary from individual to individual. In a randomized controlled experiment, it is expected that 1. Subjects are randomly selected from the population(ensures average causal effect in sample equals average causal effect in population), 2. Subjects are randomly assigned to treatment and control group (ensures receiving a treatment is independent of a subject’s potential outcome).\nIf above conditions are met, then the average causal effect is the expected outcome in treatment group less expected outcome in the control:\n\\[\n\\text{Average causal effect}  = E(Y_i|X_i = 1) - E(Y_i|X_i = 0)\n\\]\nwhere \\(X_i\\) is a binary treatment indicator. Estimation follows differences estimator which is an OLS regression model\n\\[\n\\begin{equation}\nY_i = \\beta_0 + \\beta_1X_i + \\epsilon_i, i = 1, \\dots, n\n\\end{equation}\n\\tag{12}\\]\nwhere random assignment ensures \\(E(\\epsilon_i|X_i) = 0\\). The difference estimator in (Equation 12) can take additional regressors \\(W_1, \\dots, W_r\\) and is called a differences estimator with additional regressors:\n\\[\n\\begin{equation}\nY_i = \\beta_0 + \\beta_1X_i + \\beta_2W_{1i} + \\dots + \\beta_{1+r}W_{ri} +\n\\epsilon_i, i = 1, \\dots, n\n\\end{equation}\n\\tag{13}\\]\nIt utilizes a conditional mean independence assumption that states that treatment assignment \\(X_i\\) is random and independent of any pretreatment variable \\(W_i\\). This ensures that\n\\[\n\\begin{equation}\nE(\\epsilon_i|X_i, W_i) = E(\\epsilon_i|W_i) = 0\n\\end{equation}\n\\]\nimplying that the conditional expectation of the error term \\(\\epsilon_i\\), given the treatment indicator \\(X_i\\) and the the pretreatment characteristic \\(W_i\\) does not depend on \\(X_i\\).\n### Analysis of experimental data\nSTAR data set from {AER} is a large student-teacher achievement ratio randomized controlled experiment that was assessing whether class size reduction is effective in improving education outcomes. 6400 students were randomly assigned into one of the three interventions: small class (13 to 17 students per teacher), regular class (22 to 25 students per teacher), and regular-with-aide class (22 to 25 students with a full-time teacher’s aide). Teachers were also randomly assigned to classes. The subjects were followed from kindergarten to grade 3 (K,1, 2, 3 suffixes in variable names).\n\n\nCode\ndata(\"STAR\", package = \"AER\")\nskimr::skim(STAR)\n\n\nWarning: Couldn't find skimmers for class: yearqtr; No user-defined `sfl`\nprovided. Falling back to `character`.\n\n\n\nData summary\n\n\nName\nSTAR\n\n\nNumber of rows\n11598\n\n\nNumber of columns\n47\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n34\n\n\nnumeric\n12\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nbirth\n70\n0.99\n4\n7\n0\n21\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\ngender\n20\n1.00\nFALSE\n2\nmal: 6122, fem: 5456\n\n\nethnicity\n145\n0.99\nFALSE\n6\ncau: 7193, afa: 4173, asi: 32, his: 21\n\n\nstark\n5273\n0.55\nFALSE\n3\nreg: 2231, reg: 2194, sma: 1900\n\n\nstar1\n4769\n0.59\nFALSE\n3\nreg: 2584, reg: 2320, sma: 1925\n\n\nstar2\n4758\n0.59\nFALSE\n3\nreg: 2495, reg: 2329, sma: 2016\n\n\nstar3\n4796\n0.59\nFALSE\n3\nreg: 2543, sma: 2174, reg: 2085\n\n\nlunchk\n5296\n0.54\nFALSE\n2\nnon: 3250, fre: 3052\n\n\nlunch1\n4947\n0.57\nFALSE\n2\nfre: 3430, non: 3221\n\n\nlunch2\n5102\n0.56\nFALSE\n2\nfre: 3336, non: 3160\n\n\nlunch3\n5078\n0.56\nFALSE\n2\nfre: 3293, non: 3227\n\n\nschoolk\n5273\n0.55\nFALSE\n4\nrur: 2917, inn: 1428, sub: 1412, urb: 568\n\n\nschool1\n4769\n0.59\nFALSE\n4\nrur: 3237, sub: 1586, inn: 1380, urb: 626\n\n\nschool2\n4758\n0.59\nFALSE\n4\nrur: 3167, sub: 1710, inn: 1481, urb: 482\n\n\nschool3\n4796\n0.59\nFALSE\n4\nrur: 3240, sub: 1720, inn: 1335, urb: 507\n\n\ndegreek\n5294\n0.54\nFALSE\n4\nbac: 4119, mas: 1981, mas: 161, spe: 43\n\n\ndegree1\n4788\n0.59\nFALSE\n4\nbac: 4456, mas: 2294, spe: 38, phd: 22\n\n\ndegree2\n4819\n0.58\nFALSE\n4\nbac: 4249, mas: 2427, spe: 67, phd: 36\n\n\ndegree3\n4862\n0.58\nFALSE\n3\nbac: 3762, mas: 2885, spe: 89, phd: 0\n\n\nladderk\n5869\n0.49\nFALSE\n6\nlev: 4671, app: 514, pro: 334, lev: 119\n\n\nladder1\n4811\n0.59\nFALSE\n6\nlev: 4492, app: 718, pro: 666, not: 506\n\n\nladder2\n4878\n0.58\nFALSE\n6\nlev: 4703, not: 754, app: 482, pro: 411\n\n\nladder3\n4847\n0.58\nFALSE\n6\nlev: 4437, pro: 550, not: 497, lev: 484\n\n\ntethnicityk\n5335\n0.54\nFALSE\n2\ncau: 5232, afa: 1031\n\n\ntethnicity1\n4822\n0.58\nFALSE\n2\ncau: 5592, afa: 1184\n\n\ntethnicity2\n4819\n0.58\nFALSE\n2\ncau: 5398, afa: 1381\n\n\ntethnicity3\n4847\n0.58\nFALSE\n3\ncau: 5328, afa: 1409, asi: 14\n\n\nsystemk\n5273\n0.55\nFALSE\n42\n11: 1788, 18: 360, 22: 319, 6: 228\n\n\nsystem1\n4769\n0.59\nFALSE\n42\n11: 1757, 22: 508, 18: 269, 12: 240\n\n\nsystem2\n4758\n0.59\nFALSE\n41\n11: 1908, 22: 495, 18: 312, 12: 263\n\n\nsystem3\n4796\n0.59\nFALSE\n41\n11: 1792, 22: 473, 18: 302, 12: 246\n\n\nschoolidk\n5273\n0.55\nFALSE\n79\n27: 156, 28: 154, 51: 146, 22: 137\n\n\nschoolid1\n4769\n0.59\nFALSE\n76\n51: 238, 63: 149, 27: 142, 9: 134\n\n\nschoolid2\n4758\n0.59\nFALSE\n75\n51: 235, 27: 146, 22: 141, 9: 140\n\n\nschoolid3\n4796\n0.59\nFALSE\n75\n51: 207, 58: 147, 9: 146, 66: 139\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nreadk\n5809\n0.50\n436.73\n31.71\n315\n414\n433\n453\n627\n▁▇▅▁▁\n\n\nread1\n5202\n0.55\n520.78\n55.19\n404\n478\n514\n558\n651\n▂▇▇▅▃\n\n\nread2\n5521\n0.52\n583.93\n46.04\n468\n552\n582\n614\n732\n▂▇▇▃▁\n\n\nread3\n5598\n0.52\n615.41\n38.57\n499\n588\n614\n641\n775\n▁▇▇▂▁\n\n\nmathk\n5727\n0.51\n485.38\n47.70\n288\n454\n484\n513\n626\n▁▁▇▅▂\n\n\nmath1\n4998\n0.57\n530.53\n43.10\n404\n500\n529\n557\n676\n▁▆▇▃▁\n\n\nmath2\n5533\n0.52\n580.61\n44.57\n441\n550\n579\n611\n721\n▁▃▇▃▁\n\n\nmath3\n5521\n0.52\n617.97\n39.84\n487\n591\n616\n645\n774\n▁▅▇▂▁\n\n\nexperiencek\n5294\n0.54\n9.26\n5.81\n0\n5\n9\n13\n27\n▇▇▇▂▁\n\n\nexperience1\n4788\n0.59\n11.63\n8.94\n0\n4\n10\n17\n42\n▇▆▃▁▁\n\n\nexperience2\n4860\n0.58\n13.15\n8.65\n0\n7\n12\n18\n40\n▇▇▅▂▁\n\n\nexperience3\n4847\n0.58\n13.93\n8.61\n0\n7\n14\n19\n38\n▆▇▆▂▁\n\n\n\n\n\nIn essence, there were 2 treatments in each grade (small class with 13 - 17 students, and a regular class with 22 - 25 students and a teaching aide). We introduce two binary variables, each being an indicator for the respective treatment group for the difference estimator. This is to capture the treatment effect for each treatment group separately yielding the population regression model for the test score as\n\\[\n\\begin{equation}\nY_i = \\beta_0 + \\beta_1 smallClass_i + \\beta2RegAide_i + \\epsilon_i\n\\end{equation}\n  \\tag{14}\\]\n\n\nCode\ngrades &lt;- c(\"k\",1:3)\n\n# map over grades to create a list of models\n\nstar_models &lt;- grades |&gt;\n  set_names(paste0(\"grade_\", grades)) |&gt;\n  map(\n    \\(y){\n#       construct the formula dynamically: I(ready + mathy) ~ starY\n      form &lt;- as.formula(paste0(\"I(read\", y, \"+ math\", y, \") ~ star\", y))\n      lm(form, data = STAR)\n    }\n  )\n\n# compute robust standard error\nrob_se &lt;- star_models |&gt;\n map(\\(m) sqrt(diag(vcovHC(m, type = \"HC1\"))))\n\n# Define which Goodness-of-Fit stats to show and what to call them\ngm1 &lt;- list(\n  list(\"raw\" = \"nobs\", \"clean\" = \"Observations\", \"fmt\" = 0),\n  list(\"raw\" = \"r.squared\", \"clean\" = \"R2\", \"fmt\" = 3),\n  list(\"raw\" = \"adj.r.squared\", \"clean\" = \"Adj. R2\", \"fmt\" = 3),\n  # Added Residual Standard Error\n  list(\"raw\" = \"sigma\", \"clean\" = \"Residual Std. Error\", \"fmt\" = 3),\n  # Added F-statistic\n  list(\"raw\" = \"statistic\", \"clean\" = \"F-statistic\", \"fmt\" = 3)\n)\n\n# render table\n\nmodel_tables(models = star_models, robust_se = rob_se)\n\n\n\n\n\n grade_kgrade_1grade_2grade_3starksmall13.899***(2.454)starkregular+aide0.314(2.271)star1small29.781***(2.831)star1regular+aide11.959***(2.652)star2small19.394***(2.712)star2regular+aide3.479(2.545)star3small15.587***(2.396)star3regular+aide-0.291(2.273)Num.Obs.5786637960495967R20.0070.0170.0090.010R2 Adj.0.0070.0170.0090.010AIC66151.575587.270731.268126.3BIC66178.275614.270758.168153.1Log.Lik.-33071.755-37789.595-35361.613-34059.138RMSE73.4790.4883.6772.89Std.Errorsgrade_kgrade_1grade_2grade_3+ p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\nTable 7: Project STAR: Differences estimate. Class size reduction improves study performance excpt for grade 1\n\n\n\n\nWe can augment the model in (Equation 14) with extra variables under 2 circumstances:\n\nAdding the regressors explain some of the observed variations in the outcome making the estimates more efficient\nIf treatment assignment is not random, the estimates obtained in (Equation 14) are possibly biased. The additional regressors could mitigate this challenge.\n\nFor our setting we could consider 6 additional variables (in a staggered manner):\n\nexperience - Teacher’s years of experience\nboy - A dummy (student is a boy)\nlunch - Free lunch eligibility (dummy)\nblack - Student is black (dummy)\nrace - Student’s race is other than black or white (dummy)\nschoolid - School indicator variables\n\nNote: For randomized controlled experiments, we can make causal claims only for variables that subjects were randomly assigned into."
  },
  {
    "objectID": "posts/2026-01-21-econometrics/index.html#quasi-experiments",
    "href": "posts/2026-01-21-econometrics/index.html#quasi-experiments",
    "title": "Econometrics: R introduction",
    "section": "Quasi experiments",
    "text": "Quasi experiments\nQuasi experiments exploit “as if” randomness to use methods considered under randomized controlled experiments. They come in two forms:\n\nRandom variations in individual circumstances allow to view the treatment “as if” it was randomly determined.\nThe treatment is only partly determined by “as if” random variation.\n\nThe former allows the use of either model (Equation 13), i.e., the difference estimator with additional regressors, or, using difference-in-difference (DID) if the “as if” randomness does not guarantee the non-existence of systematic differences in the control and treatment groups. The latter case invites the use of an IV approach for a case like model (Equation 13), utilizing the source of “as if” randomness in treatment assignment as the instrument. Other methods include sharp regression discontinuity design (RDD) and fuzzy regression discontinuity design (FRDD). We use simulated data to explore these methods\n\nThe difference-in-difference (DID) estimator\nThe source of “as if” randomness in treatment assignment in quasi-experiments does not always rule out systematic differences between control and treatment groups. DID involves data in organised in the before and after treatment.\n\\[\n\\begin{align}\n  \\widehat{\\beta}_1^{\\text{diffs-in-diffs}} =& \\, (\\overline{Y}^{\\text{treatment,after}} - \\overline{Y}^{\\text{treatment,before}}) - (\\overline{Y}^{\\text{control,after}} - \\overline{Y}^{\\text{control,before}}) \\\\\n  =& \\Delta \\overline{Y}^{\\text{treatment}} - \\Delta \\overline{Y}^{\\text{control}},\n\\end{align}\n\\tag{15}\\]\nwith\n\n\\(\\overline{Y}^{\\text{treatment,before}}\\) - the sample average in the treatment group before the treatment\n\\(\\overline{Y}^{\\text{treatment,after}}\\) - the sample average in the treatment group after the treatment\n\\(\\overline{Y}^{\\text{treatment,before}}\\) - the sample average in the control group before the treatment\n\\(\\overline{Y}^{\\text{treatment,after}}\\) - the sample average in the control group after the treatment.\n\nWe now use R to plot a DID setting.\n\n\nCode\n# initialize plot and add control group\nplot(c(0, 1), c(6, 8), \n     type = \"p\",\n     ylim = c(5, 12),\n     xlim = c(-0.3, 1.3),\n     # main = \"The Differences-in-Differences Estimator\",\n     xlab = \"Period\",\n     ylab = \"Y\",\n     col = \"steelblue\",\n     pch = 20,\n     xaxt = \"n\",\n     yaxt = \"n\")\n\naxis(1, at = c(0, 1), labels = c(\"before\", \"after\"))\naxis(2, at = c(0, 13))\n\n# add treatment group\npoints(c(0, 1, 1), c(7, 9, 11), \n       col = \"darkred\",\n       pch = 20)\n\n# add line segments\nlines(c(0, 1), c(7, 11), col = \"darkred\")\nlines(c(0, 1), c(6, 8), col = \"steelblue\")\nlines(c(0, 1), c(7, 9), col = \"darkred\", lty = 2)\nlines(c(1, 1), c(9, 11), col = \"black\", lty = 2, lwd = 2)\n\n# add annotations\ntext(1, 10, expression(hat(beta)[1]^{DID}), cex = 0.8, pos = 4)\ntext(0, 5.5, \"s. mean control\", cex = 0.8 , pos = 4)\ntext(0, 6.8, \"s. mean treatment\", cex = 0.8 , pos = 4)\ntext(1, 7.9, \"s. mean control\", cex = 0.8 , pos = 4)\ntext(1, 11.1, \"s. mean treatment\", cex = 0.8 , pos = 4)\n\n\n\n\n\n\n\n\nFigure 6: Difference-in-differnce plot\n\n\n\n\n\nThe DID estimator (Equation 15) can take a regression notation: \\(\\hat{\\beta_1}^{DID}\\) is the OLS estimator of \\(\\beta_1\\) in\n\\[\n\\begin{equation}\n\\Delta Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i\n\\end{equation}\n\\tag{16}\\]\nwith \\(\\Delta Y_i\\) denoting the difference in pre and post-treatment outcomes of subject \\(i\\), \\(X_i\\) is the treatment indicator. We can add additional regressors measuring pretreatment characteristics to (Equation 16) to get the difference-in-difference estimator with additional regressors.\n\\[\n\\begin{equation}\n\\Delta Y_i = \\beta_0 + \\beta_1X_i + \\beta_2W_{1i} + \\dots + \\beta_{1+r}W_{ri} +  \\epsilon_i\n\\end{equation}\n\\tag{17}\\]\nSimulate data:\n\n\nCode\nset.seed(as.numeric(Sys.Date()))\n# set sample size\nn &lt;- 200\n\n# define treatment effect\nTEffect &lt;- 4\n\n# generate treatment dummy\nTDummy &lt;- c(rep(0, n/2), rep(1, n/2))\n\n# simulate pre- and post-treatment values of the dependent variable\ny_pre &lt;- 7 + rnorm(n)\ny_pre[1:n/2] &lt;- y_pre[1:n/2] - 1\ny_post &lt;- 7 + 2 + TEffect * TDummy + rnorm(n)\ny_post[1:n/2] &lt;- y_post[1:n/2] - 1 \n\n\nPlot data:\n\n\nCode\npre &lt;- rep(0, length(y_pre[TDummy==0]))\npost &lt;- rep(1, length(y_pre[TDummy==0]))\n\n# plot control group in t=1\nplot(jitter(pre, 0.6), \n     y_pre[TDummy == 0], \n     ylim = c(0, 16), \n     col = alpha(\"steelblue\", 0.3),\n     pch = 20, \n     xlim = c(-0.5, 1.5),\n     ylab = \"Y\",\n     xlab = \"Period\",\n     xaxt = \"n\"\n     # ,main = \"Artificial Data for DID Estimation\"\n     )\n\naxis(1, at = c(0, 1), labels = c(\"before\", \"after\"))\n\n# add treatment group in t=1\npoints(jitter(pre, 0.6), \n       y_pre[TDummy == 1], \n       col = alpha(\"darkred\", 0.3), \n       pch = 20)\n\n# add control group in t=2\npoints(jitter(post, 0.6),\n       y_post[TDummy == 0], \n       col = alpha(\"steelblue\", 0.5),\n       pch = 20)\n\n# add treatment group in t=2\npoints(jitter(post, 0.6), \n       y_post[TDummy == 1], \n       col = alpha(\"darkred\", 0.5),\n       pch = 20)\n\n\n\n\n\n\n\n\nFigure 7: Artificial data for DID estimation\n\n\n\n\n\nCompute DID estimate by hand:\n\n\nCode\n# compute the DID estimator for the treatment effect 'by hand'\nmean(y_post[TDummy == 1]) - mean(y_pre[TDummy == 1]) - \n(mean(y_post[TDummy == 0]) - mean(y_pre[TDummy == 0]))\n\n\n[1] 4.085678\n\n\nCompute DID using a linear model:\n\n\nCode\n# compute the DID estimator using a linear model\nlm(I(y_post - y_pre) ~ TDummy) %&gt;% render_summary()\n\n\ntermestimatestd.errorstatisticp.value(Intercept)1.9640.12915.220&lt;0.0001TDummy4.0860.18222.392&lt;0.0001\n\n\n\n\nRegression discontinuity estimators\nConsider the model\n\\[\n\\begin{align}\n  Y_i =& \\beta_0 + \\beta_1 X_i + \\beta_2 W_i + u_i\n\\end{align}\n\\tag{18}\\]\nand let\n\\[\n\\begin{align*}\nX_i =&\n  \\begin{cases}\n    1, & W_i \\geq c \\\\\n    0, & W_i &lt; c,\n  \\end{cases}\n\\end{align*}\n\\] so that the receipt of treatment, \\(X_i\\), is determined by some threshold \\(c\\) of a continuous variable \\(W_i\\), the so called running variable. The idea of regression discontinuity design is to use observations with a \\(W_i\\) close to \\(c\\) for estimation of \\(\\beta_1\\). \\(\\beta_1\\) is the average treatment effect for individuals with \\(W_i = c\\) which is assumed to be a good approximation to the treatment effect in the population. (Equation 18) is called a sharp regression discontinuity design because treatment assignment is deterministic and discontinuous at the cutoff: all observations with \\(W_i &lt; c\\) do not receive treatment and all observations where \\(W_i \\geq c\\) are treated.\n\n\nCode\nset.seed(as.numeric(Sys.Date()))\n\n# generate some sample data\nW &lt;- runif(1000, -1, 1)\ny &lt;- 3 + 2 * W + 10 * (W&gt;=0) + rnorm(1000)\n\n# construct rdd_data \ndata &lt;- rdd_data(y, W, cutpoint = 0)\n\n# plot the sample data\nplot(data,\n     col = \"steelblue\",\n     cex = 0.35, \n     xlab = \"W\", \n     ylab = \"Y\")\n\n\n\n\n\n\n\n\nFigure 8: Regression discontinuity plot\n\n\n\n\n\nThe argument nbins sets the number of bins the running variable is divided into for aggregation. The dots represent bin averages of the outcome variable.\nWe use the function rdd_reg_lm() to estimate the treatment effect using model (Equation 18). slope = \"same\" restricts the slopes of the estimated regression function to be the same on both sides of the jump at cutpoint \\(W = 0\\).\n\n\nCode\nrdd_mod &lt;- rdd_reg_lm(rdd_object = data, \n                      slope = \"same\")\nrender_summary(rdd_mod)\n\n\nWarning: The `tidy()` method for objects of class `rdd_reg_lm` is not maintained by the broom team, and is only supported through the `lm` tidier method. Please be cautious in interpreting and reporting broom output.\n\nThis warning is displayed once per session.\n\n\ntermestimatestd.errorstatisticp.value(Intercept)3.0650.07342.135&lt;0.0001D9.8730.12976.751&lt;0.0001x2.1720.11319.244&lt;0.0001\n\n\nYou can visualize by calling plot\n\n\nCode\n# plot the RDD model along with binned observations\nplot(rdd_mod,\n     cex = 0.35, \n     col = \"steelblue\", \n     xlab = \"W\", \n     ylab = \"Y\")\n\n\n\n\n\n\n\n\n\nSo far we assumed that crossing of the threshold determines receipt of treatment so that the jump of the population regression functions at the threshold can be regarded as the causal effect of the treatment.\nWhen crossing the threshold \\(c\\) is not the only cause for receipt of the treatment, treatment is no a deterministic function of \\(W_i\\). Instead, it is useful to think of \\(c\\) as a threshold where the probability of receiving the treatment jumps.\nThis jump may be due to unobservable variables that have impact on the probability of being treated. Thus, \\(X_i\\) in (Equation 18) will be correlated with the error \\(u_i\\) and it becomes more difficult to consistently estimate the treatment effect. In this setting, using a fuzzy regression discontinuity design which is based an IV approach may be a remedy: take the binary variable \\(Z_i\\) as an indicator for crossing of the threshold,\n\\[\n\\begin{align*}\n  Z_i = \\begin{cases}\n    1, & W_i \\geq c \\\\\n    0, & W_i &lt; c,\n  \\end{cases}\n\\end{align*}\n\\] and assume that \\(Z_i\\) relates to \\(Y_i\\) only through the treatment indicator \\(X_i\\). Then \\(Z_i\\) and \\(u_i\\) are uncorrelated but \\(Z_i\\) influences receipt of treatment so it is correlated with \\(X_i\\). Thus, \\(Z_i\\) is a valid instrument for \\(X_i\\) and (Equation 18) can be estimated using TSLS.\nThe following code chunk generates sample data where observations with a value of the running variable \\(W_i\\) below the cutoff \\(c=0\\) do not receive treatment and observations with \\(W_i \\geq 0\\) do receive treatment with a probability of \\(80\\%\\) so that treatment status is only partially determined by the running variable and the cutoff. Treatment leads to an increase in \\(Y\\) by \\(2\\) units. Observations with \\(W_i \\geq 0\\) that do not receive treatment are called no-shows: think of an individual that was assigned to receive the treatment but somehow manages to avoid it.\n\n\nCode\n# generate sample data\nset.seed(as.numeric(Sys.Date()))\nmu &lt;- c(0, 0)\nsigma &lt;- matrix(c(1, 0.7, 0.7, 1), ncol = 2)\n\nset.seed(1234)\nd &lt;- as.data.frame(mvrnorm(2000, mu, sigma))\ncolnames(d) &lt;- c(\"W\", \"Y\")\n\n# introduce fuzziness\nd$treatProb &lt;- ifelse(d$W &lt; 0, 0, 0.8)\n\nfuzz &lt;- sapply(X = d$treatProb, FUN = function(x) rbinom(1, 1, prob = x))\n\n# treatment effect\nd$Y &lt;- d$Y + fuzz * 2\n\n\nPlot:\n\n\nCode\n# generate a colored plot of treatment and control group\nplot(d$W, d$Y,\n     col = c(\"steelblue\", \"darkred\")[factor(fuzz)], \n     pch= 20, \n     cex = 0.5,\n     xlim = c(-3, 3),\n     ylim = c(-3.5, 5),\n     xlab = \"W\",\n     ylab = \"Y\")\n\n# add a dashed vertical line at cutoff\nabline(v = 0, lty = 2)\n#add legend\nlegend(\"topleft\",pch=20,col=c(\"steelblue\",\"darkred\"),\n       legend=c(\"Do not receive treatment\",\"Receive treatment\"))\n\n\n\n\n\n\n\n\n\nObviously, receipt of treatment is no longer a deterministic function of the running variable \\(W\\). Some observations with \\(W\\geq0\\) did not receive the treatment. We may estimate a FRDD by additionally setting treatProb as the assignment variable z in rdd_data. Then rdd_reg_lm() applies the following TSLS procedure: treatment is predicted using \\(W_i\\) and the cutoff dummy \\(Z_i\\), the instrumental variable, in the first stage regression. The fitted values from the first stage regression are used to obtain a consistent estimate of the treatment effect using the second stage where the outcome \\(Y\\) is regressed on the fitted values and the running variable \\(W\\).\n\n\nCode\n# estimate the Fuzzy RDD\ndata &lt;- rdd_data(d$Y, d$W, \n                 cutpoint = 0, \n                 z = d$treatProb)\n\nfrdd_mod &lt;- rdd_reg_lm(rdd_object = data, \n                       slope = \"same\")\nfrdd_mod %&gt;% render_summary()\n\n\ntermestimatestd.errorstatisticp.value(Intercept)0.0140.0400.3620.7171D1.9810.08523.393&lt;0.0001x0.7090.03420.685&lt;0.0001\n\n\nThe estimate is close to \\(2\\), the population treatment effect. We may call plot() on the model object to obtain a figure consisting of binned data and the estimated regression function.\n\n\nCode\n# plot estimated FRDD function\nplot(frdd_mod, \n     cex = 0.5, \n     lwd = 0.4,\n     xlim = c(-4, 4),\n     ylim = c(-3.5, 5),\n     xlab = \"W\",\n     ylab = \"Y\")\n\n\n\n\n\n\n\n\n\nWhat if we used a SRDD instead, thereby ignoring the fact that treatment is not perfectly determined by the cutoff in \\(W\\)? We may get an impression of the consequences by estimating an SRDD using the previously simulated data.\n\n\nCode\n# estimate SRDD\ndata &lt;- rdd_data(d$Y, \n                 d$W, \n                 cutpoint = 0)\n\nsrdd_mod &lt;- rdd_reg_lm(rdd_object = data, \n                       slope = \"same\")\nsrdd_mod %&gt;% render_summary()\n\n\ntermestimatestd.errorstatisticp.value(Intercept)0.0140.0400.3620.7171D1.5850.06823.393&lt;0.0001x0.7090.03420.685&lt;0.0001\n\n\nThe estimate obtained using a SRDD is suggestive of a substantial downward bias. In fact, this procedure is inconsistent for the true causal effect so increasing the sample would not alleviate the bias."
  }
]