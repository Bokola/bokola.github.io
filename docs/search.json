[
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "resources",
    "section": "",
    "text": "Rodrigues’ Reproducible Analytical Pipelines in R\nSamantha’s quarto website tutorial\nSamantha’s quarto blog posts tutorial\nMastering Shiny, by Hadley Wickham\nHappy Git and GitHub for the useR, by Jenny Bryan\nR for Data Science, by Hadley Wickham\nHitchhiker’s Guide to Python, by Kenneth Reitz & Tanya Schlusser\nData wrangling essentials: comparisons in JavaScript, Python, SQL, R, and Excel, by Allison Horst & Paul Buffa\nW3Schools, particularly for their HTML & CSS tutorials\nJayde’s parameterized reports with quarto\nMine’s quarto manuscripts"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Portfolio",
    "section": "",
    "text": "R package development\nTaking on package development inculcates in one best practices in programming: using a secluded development environment made possible by {renv} package and Rstudio’s project initialization functionality, documenting code, shared data and publishing user guides to get people started with the software. Package development workflow included {fussen}, {roxygen2}, and {devtools}\nThe packaged is named pbwrangler, it’s goal to document functions used in reading, processing and writing field experimental data in potato breeding.\n\n\nShiny web app\n{shiny} is undoubtedly a go to tool for building a web app that runs in production or just for presenting a proof of concept. I developed a small app as proof that I understand the underlying framework to be able to build an app that can be used in production.\n\n\nReproducible analytical pipelines in R\nThis was a ‘do it yourself too’ as I was reading an online version of Rodrigues’ reproducible piplines text . It reinforced my understanding of package development powered by {fusen}, reproducibility of package versions using {renv}, reproducible pipelines with {targets}, building and sharing docker containers in dockerhub and github, and continuous integration/development (CI/CD) using github actions. A branch with CI/CD running a docker container can be found here.\n\n\nEnd-to-end Machine Learning (MLflow + Docker + Google cloud)\nThis is an account of my learning journey aided by this tutorial to grasp the nitty-gritties of building, logging, saving and serving machine learning models. The code available at this repo and a write-up is here.\n\n\nWeb scraping: getting data from the internet\nI set out to understand how to scrape data using Python. I explored beautifulsoup, requests, scrapingBee API, and scrapy to scrape data from Google news and a product listing page. I wrote a piece about it too. I have also scheduled this to run daily using github actions.\n\n\nProject-based Data Engineering\nThis is an accompanying “do-it-yourself” as I go through data engineering material from DuckDb. It is my initiative to learn how to to build data pipelines with Python, SQL & DuckDB. The first part is about ingestion, involving reading public data from Google Cloud, writing it locally as .csv or to MotherDuck database. Github for project materials and a post\n\n\nELT pipeline with dbt, snowflake, and dagster\nCreated an ELT pipeline that uses a dbt project to read and write tables to a snowflake warehouse database, then orchestrated the workflow with dagster. Github repo."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "Hello!\nYou’ve made it to my landing page! I am a student enrolled in a Master of Science degree in Statistics and data science, specializing in biostatistics. I use R, SAS, and Python for statistical analysis and visualizations. My interest is statistical computing applied to spatial statistics, infectious diseases modelling and Bayesian data analysis.\n\neducation\n\n\nMS in Biostatistics, 2020 - Ongoing|Hasselt University\n\n\n\nBS in Applied Statistics, 2015|Maseno University\n\n\n\n\n\nexperience\n\n\nBiometrician, 2024-present|International Potato Center (CIP)\n\n\nResearch Assistant, 2022-2023|Karolinska Institutet\n\n\n\nData Manager, 2022-2022|Kenya Medical Research Institute\n\n\n\nData Manager, 2018-2020|KEMRI - Wellcome Trust Research Programme"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "posts",
    "section": "",
    "text": "Interactive maps (R)\n\n\n\nggplot2\n\nplotly\n\necharts4r\n\nggiraph\n\nwidgetframe\n\n\n\n\n\n\n\n\n\nOct 29, 2025\n\n\nBasil Okola\n\n\n\n\n\n\n\n\n\n\n\n\nPublication ready visualization in R\n\n\n\nggplot2\n\ncowplot\n\n\n\n\n\n\n\n\n\nOct 29, 2025\n\n\nBasil Okola\n\n\n\n\n\n\n\n\n\n\n\n\nEnd-to-end Data Engineering with Python DuckDB, Google Cloud, dbt\n\n\n\nQuarto\n\nPython\n\nbeautifulsoup\n\nscrapy\n\nscrapingBee\n\n\n\nPart 1: Data Ingestion\n\n\n\n\n\nAug 8, 2025\n\n\nBasil Okola\n\n\n\n\n\n\n\n\n\n\n\n\nWeb Scraping with Python\n\n\n\nQuarto\n\nPython\n\nbeautifulsoup\n\nscrapy\n\nscrapingBee\n\n\n\nScraping data from the web using {beautifulsoup4}, {requests}, {ScrapingBee}, and {Scrapy}\n\n\n\n\n\nAug 3, 2025\n\n\nBasil Okola\n\n\n\n\n\n\n\n\n\n\n\n\nEnd-to-end Machine Learning (MLflow + Docker + Google Cloud)\n\n\n\nQuarto\n\nPython\n\nMLflow\n\n\n\nMachine learning Workflow using MLflow locally and pushing image to google cloud\n\n\n\n\n\nAug 2, 2025\n\n\nBasil Okola\n\n\n\n\n\n\n\n\n\n\n\n\nR package development guide\n\n\n\nQuarto\n\nR\n\npackages\n\n\n\nWriting R packages using devtools, roxygen2 and usethis packages\n\n\n\n\n\nDec 16, 2023\n\n\nBasil Okola\n\n\n\n\n\n\n\n\n\n\n\n\nInteractive maps (R)\n\n\n\n\n\n\n\n\nInvalid Date\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-12-16-R-package dev/index.html",
    "href": "posts/2023-12-16-R-package dev/index.html",
    "title": "R package development guide",
    "section": "",
    "text": "These are quick step-by-step guides I wrote when going through Andy’s fundamentas of package development notes.\n\nInitialize package dev files with devtools::create_package() which creates mandatory file for the package\nEnable git tracking:\n\nconfigure: usethis::use_git_configure()\ncommit: usethis::use_git()\n\nFill in sections of the DESCRIPTION file. {roxygen2} is used to actualize this.\nEdit.Rprofile() to set options you’ll use in the documentation such as author details. This uses usethis::edit_r_profile()\nCreate a separate R file for each function you’ll be including in the package. Use use_r(\"/path-to-file/R/function-name.R\")\nLoad the functions with devtools::load_all()\nCheck the loaded files with check() which runs R CMD checks to catch errors/warnings/notes that need addressing.\nAdd files you don’t wish to include in the package build-in .Rbuildignore file.\nEnable roxygen2 to be used for package documentation: project options -&gt; Build Tools -&gt; check to generate documentation with roxygen or better devtools::document() which generates NAMESPACE automatically\nAutomate external function imports with usethis::use_import_from(): example usethis::use_import_from(“utils”, “install.packages”)\nDocument functions: put the cursor inside the R function definition and ctrl+shift+alt+R to insert the roxygen skeleton. The workflow here is after documenting -&gt; load_all() -&gt; document() -&gt; check()\nData files go to /data dir. It should be of .rda format\nExternal (non .rda format) data go to /inst/extdata/. Document them in the R file e.g. data.R and store it in /R. load_all() then document()\nCall use_package_doc() to add a dummy .R file that will prompt roxygen to generate basic package-level documentation. I noticed doing this erased imports in {package-name}-package.R file. Add recommended imports (see 10) and check()\nInstall your package with devtools::install()\nAttach your package as with other packages by calling library()\nTesting: Using the edit code -&gt; load_all() -&gt; experiment iteration can be unsustainable if you come back to your code months after development. You should write formal tests supported by {testthat} package.\nSet up formal testing of your package with usethis::use_testthat(). Creates a folder /tests. Don’t edit tests/testhat.R\nCall usethis::use_test() e.g., use_test(\"install_load_packages.R\") to edit tests for functions living in a particular R file in R/.\ntest() or ctr + shift + T runs all tests in your test/ directory. The workflow updates to load_all() -&gt; test() -&gt; document() -&gt; check(). Tests should be small and run quickly.\nDependencies, add imports in DESCRIPTION with use_package().\nAdd README with use_readme_rmd()\nRender readme.rmd with build_readme()\nUse continuous integration with use_github_action() then build_readme() again 25 Build a website for your package with use_pkgdown_github_pages() then document().\n\n\n\n\nCitationBibTeX citation:@online{okola2023,\n  author = {Okola, Basil},\n  title = {R Package Development Guide},\n  date = {2023-12-16},\n  url = {https://bokola.github.io/posts/2023-12-16-R-package dev/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nOkola, Basil. 2023. “R Package Development Guide.” December\n16, 2023. https://bokola.github.io/posts/2023-12-16-R-package\ndev/."
  },
  {
    "objectID": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html",
    "href": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html",
    "title": "End-to-end Machine Learning (MLflow + Docker + Google Cloud)",
    "section": "",
    "text": "BibTeX citation:@online{okola2025,\n  author = {Okola, Basil},\n  title = {End-to-End {Machine} {Learning} {(MLflow} + {Docker} +\n    {Google} {Cloud)}},\n  date = {2025-08-02},\n  url = {https://bokola.github.io/posts/2025-08-02-End-to-end-ML-with-MLflow/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nOkola, Basil. 2025. “End-to-End Machine Learning (MLflow + Docker\n+ Google Cloud).” August 2, 2025. https://bokola.github.io/posts/2025-08-02-End-to-end-ML-with-MLflow/.\nThis is an account of my learning journey aided by this tutorial to grasp the nitty-gritties of buliding, logging, saving and serving machine learning models. First is a description of the development environment used."
  },
  {
    "objectID": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#development-environment",
    "href": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#development-environment",
    "title": "End-to-end Machine Learning (MLflow + Docker + Google Cloud)",
    "section": "Development Environment",
    "text": "Development Environment\nI am running Debian 24.04 LTS, and Pycharm IDE calling Python 3.12 within a .venv virtual environment. Since the model used is a Tensorflow neural network, I had to follow cuda documentation in setting up necessary drives. You also need to start mlflow ui local server by running mlflow ui --port 5000 in the terminal, install dependenices pip install mlflow[extras] hyperopt tensorflow scikit-learn pandas numpy, and set environment variable export MLFLOW_TRACKING_URI=http://localhost:5000."
  },
  {
    "objectID": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-1-data-preparation",
    "href": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-1-data-preparation",
    "title": "End-to-end Machine Learning (MLflow + Docker + Google Cloud)",
    "section": "Step 1 : Data Preparation",
    "text": "Step 1 : Data Preparation\nThe tutorial uses wine quality classification data.\n#prepare data\n\nimport os\nfrom dotenv import load_dotenv\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport tensorflow as tf\nfrom tensorflow import keras\nimport mlflow\nfrom mlflow.models import infer_signature\nfrom hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n\n#test prediction\nimport requests\nimport json\n\n#environment variables\nload_dotenv(\".env\")\n\nMLFLOW_TRACKING_URI=os.getenv(\"MLFLOW_TRACKING_URI\")\nXLA_FLAGS=os.getenv('XLA_FLAGS')\n\n#load data\ndata = pd.read_csv(\n    \"https://raw.githubusercontent.com/mlflow/mlflow/master/tests/datasets/winequality-white.csv\",\n    sep=\";\",\n)\n\ntrain, test = train_test_split(data, test_size=0.2, random_state=12)\ntrain_x = train.drop([\"quality\"], axis=1).values\ntrain_y = train[[\"quality\"]].values.ravel()\ntest_x = test.drop([\"quality\"], axis=1).values\ntest_y = test[[\"quality\"]].values.ravel()\n\n#further split training data for validation\ntrain_x, valid_x, train_y, valid_y = train_test_split(train_x, train_y, test_size=0.2, random_state=12)\n\n#Create model signature for deployment\nsignature = infer_signature(train_x, train_y)"
  },
  {
    "objectID": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#define-model-architecture",
    "href": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#define-model-architecture",
    "title": "End-to-end Machine Learning (MLflow + Docker + Google Cloud)",
    "section": "Define Model architecture",
    "text": "Define Model architecture\ndef create_and_train_model(learning_rate, momentum, epochs=10):\n    \"\"\"\n    Create and train a neural network with specified hyperparameters.\n\n    Returns:\n        dict: Training results including model and metrics\n    \"\"\"\n    # Normalize input features for better training stability\n    mean = np.mean(train_x, axis=0)\n    var = np.var(train_x, axis=0)\n\n    # Define model architecture\n    model = keras.Sequential(\n        [\n            keras.Input([train_x.shape[1]]),\n            keras.layers.Normalization(mean=mean, variance=var),\n            keras.layers.Dense(64, activation=\"relu\"),\n            keras.layers.Dropout(0.2),  # Add regularization\n            keras.layers.Dense(32, activation=\"relu\"),\n            keras.layers.Dense(1),\n        ]\n    )\n\n    # Compile with specified hyperparameters\n    model.compile(\n        optimizer=keras.optimizers.SGD(learning_rate=learning_rate, momentum=momentum),\n        loss=\"mean_squared_error\",\n        metrics=[keras.metrics.RootMeanSquaredError()],\n    )\n\n    # Train with early stopping for efficiency\n    early_stopping = keras.callbacks.EarlyStopping(\n        patience=3, restore_best_weights=True\n    )\n\n    # Train the model\n    history = model.fit(\n        train_x,\n        train_y,\n        validation_data=(valid_x, valid_y),\n        epochs=epochs,\n        batch_size=64,\n        callbacks=[early_stopping],\n        verbose=0,  # Reduce output for cleaner logs\n    )\n\n    # Evaluate on validation set\n    val_loss, val_rmse = model.evaluate(valid_x, valid_y, verbose=0)\n\n    return {\n        \"model\": model,\n        \"val_rmse\": val_rmse,\n        \"val_loss\": val_loss,\n        \"history\": history,\n        \"epochs_trained\": len(history.history[\"loss\"]),\n    }\n    ```\n\n## Step 3: Set up parameter optimization\n\ndef objective(params): ““” Objective function for hyperparameter optimization. This function will be called by Hyperopt for each trial. ““” with mlflow.start_run(nested=True): # Log hyperparameters being tested mlflow.log_params( { “learning_rate”: params[“learning_rate”], “momentum”: params[“momentum”], “optimizer”: “SGD”, “architecture”: “64-32-1”, } )\n    # Train model with current hyperparameters\n    result = create_and_train_model(\n        learning_rate=params[\"learning_rate\"],\n        momentum=params[\"momentum\"],\n        epochs=15,\n    )\n\n    # Log training results\n    mlflow.log_metrics(\n        {\n            \"val_rmse\": result[\"val_rmse\"],\n            \"val_loss\": result[\"val_loss\"],\n            \"epochs_trained\": result[\"epochs_trained\"],\n        }\n    )\n\n    # Log the trained model\n    mlflow.tensorflow.log_model(result[\"model\"], name=\"model\", signature=signature)\n\n    # Log training curves as artifacts\n    import matplotlib.pyplot as plt\n\n    plt.figure(figsize=(12, 4))\n\n    plt.subplot(1, 2, 1)\n    plt.plot(result[\"history\"].history[\"loss\"], label=\"Training Loss\")\n    plt.plot(result[\"history\"].history[\"val_loss\"], label=\"Validation Loss\")\n    plt.title(\"Model Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n\n    plt.subplot(1, 2, 2)\n    plt.plot(\n        result[\"history\"].history[\"root_mean_squared_error\"], label=\"Training RMSE\"\n    )\n    plt.plot(\n        result[\"history\"].history[\"val_root_mean_squared_error\"],\n        label=\"Validation RMSE\",\n    )\n    plt.title(\"Model RMSE\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"RMSE\")\n    plt.legend()\n\n    plt.tight_layout()\n    plt.savefig(\"training_curves.png\")\n    mlflow.log_artifact(\"training_curves.png\")\n    plt.close()\n\n    # Return loss for Hyperopt (it minimizes)\n    return {\"loss\": result[\"val_rmse\"], \"status\": STATUS_OK}"
  },
  {
    "objectID": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-5-analyze-results-in-the-mlflow-ui",
    "href": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-5-analyze-results-in-the-mlflow-ui",
    "title": "End-to-end Machine Learning (MLflow + Docker + Google Cloud)",
    "section": "Step 5: Analyze Results in the MLflow UI",
    "text": "Step 5: Analyze Results in the MLflow UI\n\nNavigate to your experiment → click on “wine-quality-optimization\nAdd key columns: click “columns and add:\n\nMetrics | val_rmse\nParameters | learning_rate\nParameters | momentum\n\nInterprete the visualization: blue lines - better performing runs; red lines - worse performing runs\nAlso take a look at the training curves:\n\n\nfrom PIL import Image\nfrom IPython.display import display\n# Specify the path to your image file\nimage_path = \"val_rmse.png\"\n\n# Read the image\nimg = Image.open(image_path)\n\n# Display the image\ndisplay(img)\n\n\n\n\n\n\n\n\n\nfrom PIL import Image\nfrom IPython.display import display\n\n# Specify the path to your image file\nimage_path = \"training_curves.png\"\n\n# Read the image\nimg = Image.open(image_path)\n\n# Display the image\ndisplay(img)"
  },
  {
    "objectID": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-6-register-your-best-model",
    "href": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-6-register-your-best-model",
    "title": "End-to-end Machine Learning (MLflow + Docker + Google Cloud)",
    "section": "Step 6: Register your best model",
    "text": "Step 6: Register your best model\nTo find the best run: in the table view, click on the run with the lowest val_rmse then navigate to model artifacts and scroll to the “Artifacts” section. then register the model:\n- Go to \"Models\" tab in MLflow UI\n\n- Click on your registered model\n\n- Transition to \"Staging\" stage for testing\n\n- Add tags and descriptions as needed"
  },
  {
    "objectID": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-7-deploy-the-best-model",
    "href": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-7-deploy-the-best-model",
    "title": "End-to-end Machine Learning (MLflow + Docker + Google Cloud)",
    "section": "Step 7: Deploy the best model",
    "text": "Step 7: Deploy the best model\nTest your model with a REST API\n# Serve the model (choose the version number you registered)\nmlflow models serve -m \"models:/wine-quality-predictor/1\" --port 5002\nTest your deployment\n# Test with a sample wine\ncurl -X POST http://localhost:5002/invocations \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"dataframe_split\": {\n      \"columns\": [\n        \"fixed acidity\", \"volatile acidity\", \"citric acid\", \"residual sugar\",\n        \"chlorides\", \"free sulfur dioxide\", \"total sulfur dioxide\", \"density\",\n        \"pH\", \"sulphates\", \"alcohol\"\n      ],\n      \"data\": [[7.0, 0.27, 0.36, 20.7, 0.045, 45, 170, 1.001, 3.0, 0.45, 8.8]]\n    }\n  }'\n\n\nYou could also test with Python\n\nimport requests\nimport json\n\n# Prepare test data\ntest_wine = {\n    \"dataframe_split\": {\n        \"columns\": [\n            \"fixed acidity\",\n            \"volatile acidity\",\n            \"citric acid\",\n            \"residual sugar\",\n            \"chlorides\",\n            \"free sulfur dioxide\",\n            \"total sulfur dioxide\",\n            \"density\",\n            \"pH\",\n            \"sulphates\",\n            \"alcohol\",\n        ],\n        \"data\": [[7.0, 0.27, 0.36, 20.7, 0.045, 45, 170, 1.001, 3.0, 0.45, 8.8]],\n    }\n}\n\n# Make prediction request\nresponse = requests.post(\n    \"http://localhost:5002/invocations\",\n    headers={\"Content-Type\": \"application/json\"},\n    data=json.dumps(test_wine),\n)\n\nprediction = response.json()\nprint(f\"Predicted wine quality: {prediction['predictions'][0]:.2f}\")"
  },
  {
    "objectID": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-8-build-a-production-docker-container",
    "href": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-8-build-a-production-docker-container",
    "title": "End-to-end Machine Learning (MLflow + Docker + Google Cloud)",
    "section": "Step 8: Build a production Docker container",
    "text": "Step 8: Build a production Docker container\n# Build Docker image\nmlflow models build-docker \\\n  --model-uri \"models:/wine-quality-predictor/1\" \\\n  --name \"wine-quality-api\"\nTest your container:\n# Run the container\ndocker run -p 5003:8080 wine-quality-api\n\n# Test in another terminal\ncurl -X POST http://localhost:5003/invocations \\\n-H \"Content-Type: application/json\" \\\n-d '{\n  \"dataframe_split\": {\n    \"columns\": [\"fixed acidity\",\"volatile acidity\",\"citric acid\",\"residual sugar\",\"chlorides\",\"free sulfur dioxide\",\"total sulfur dioxide\",\"density\",\"pH\",\"sulphates\",\"alcohol\"],\n    \"data\": [[7.0, 0.27, 0.36, 20.7, 0.045, 45, 170, 1.001, 3.0, 0.45, 8.8]]\n  }\n}'"
  },
  {
    "objectID": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-9-deploy-to-google-cloud",
    "href": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-9-deploy-to-google-cloud",
    "title": "End-to-end Machine Learning (MLflow + Docker + Google Cloud)",
    "section": "Step 9: Deploy to Google cloud",
    "text": "Step 9: Deploy to Google cloud\n\nAuthentication and project set up\n$ gcloud auth login\nConfigure Docker for gcp $ gcloud auth configure-docker\nset project $ gcloud config set project PROJECT_ID\nIAM roles\n\nArtifacr registry Administrator\nroles/artifactregistry.createOnPushRepoAdmin\nStorage Administrator\n\nExport the credentials export GOOGLE_APPLICATION_CREDENTIALS=\"/path/to/your-service-account-file.json\"\nTag the docker image\n$ docker tag IMAGE_NAME gcr.io/PROJECT_ID/IMAGE_NAME:TAG\nPush the docker image to Google Cloud Container Registry $ docker push gcr.io/PROJECT_ID/IMAGE_NAME:TAG\n\n\nfrom PIL import Image\nfrom IPython.display import display\n# Specify the path to your image file\nimage_path = \"gc-artifact-registry.png\"\n\n# Read the image\nimg = Image.open(image_path)\n\n# Display the image\ndisplay(img)\n\n\n\n\n\n\n\n\nsee"
  },
  {
    "objectID": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-2-define-model-architecture",
    "href": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-2-define-model-architecture",
    "title": "End-to-end Machine Learning (MLflow + Docker + Google Cloud)",
    "section": "Step 2: Define Model architecture",
    "text": "Step 2: Define Model architecture\n\ndef create_and_train_model(learning_rate, momentum, epochs=10):\n    \"\"\"\n    Create and train a neural network with specified hyperparameters.\n\n    Returns:\n        dict: Training results including model and metrics\n    \"\"\"\n    #Normalize input features for better training stability\n    mean = np.mean(train_x, axis=0)\n    var = np.var(train_x, axis=0)\n\n    #Define model architecture\n    model = keras.Sequential(\n        [\n            keras.Input([train_x.shape[1]]),\n            keras.layers.Normalization(mean=mean, variance=var),\n            keras.layers.Dense(64, activation=\"relu\"),\n            keras.layers.Dropout(0.2),  # Add regularization\n            keras.layers.Dense(32, activation=\"relu\"),\n            keras.layers.Dense(1),\n        ]\n    )\n\n    #Compile with specified hyperparameters\n    model.compile(\n        optimizer=keras.optimizers.SGD(learning_rate=learning_rate, momentum=momentum),\n        loss=\"mean_squared_error\",\n        metrics=[keras.metrics.RootMeanSquaredError()],\n    )\n\n    #Train with early stopping for efficiency\n    early_stopping = keras.callbacks.EarlyStopping(\n        patience=3, restore_best_weights=True\n    )\n\n    #Train the model\n    history = model.fit(\n        train_x,\n        train_y,\n        validation_data=(valid_x, valid_y),\n        epochs=epochs,\n        batch_size=64,\n        callbacks=[early_stopping],\n        verbose=0,  # Reduce output for cleaner logs\n    )\n\n    #Evaluate on validation set\n    val_loss, val_rmse = model.evaluate(valid_x, valid_y, verbose=0)\n\n    return {\n        \"model\": model,\n        \"val_rmse\": val_rmse,\n        \"val_loss\": val_loss,\n        \"history\": history,\n        \"epochs_trained\": len(history.history[\"loss\"]),\n    }"
  },
  {
    "objectID": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-3-set-up-parameter-optimization",
    "href": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-3-set-up-parameter-optimization",
    "title": "End-to-end Machine Learning (MLflow + Docker + Google Cloud)",
    "section": "Step 3: Set up parameter optimization",
    "text": "Step 3: Set up parameter optimization\n\ndef objective(params):\n    \"\"\"\n    Objective function for hyperparameter optimization.\n    This function will be called by Hyperopt for each trial.\n    \"\"\"\n    with mlflow.start_run(nested=True):\n        #Log hyperparameters being tested\n        mlflow.log_params(\n            {\n                \"learning_rate\": params[\"learning_rate\"],\n                \"momentum\": params[\"momentum\"],\n                \"optimizer\": \"SGD\",\n                \"architecture\": \"64-32-1\",\n            }\n        )\n\n        #Train model with current hyperparameters\n        result = create_and_train_model(\n            learning_rate=params[\"learning_rate\"],\n            momentum=params[\"momentum\"],\n            epochs=15,\n        )\n\n        #Log training results\n        mlflow.log_metrics(\n            {\n                \"val_rmse\": result[\"val_rmse\"],\n                \"val_loss\": result[\"val_loss\"],\n                \"epochs_trained\": result[\"epochs_trained\"],\n            }\n        )\n\n        #Log the trained model\n        mlflow.tensorflow.log_model(result[\"model\"], name=\"model\", signature=signature)\n\n        #Log training curves as artifacts\n        import matplotlib.pyplot as plt\n\n        plt.figure(figsize=(12, 4))\n\n        plt.subplot(1, 2, 1)\n        plt.plot(result[\"history\"].history[\"loss\"], label=\"Training Loss\")\n        plt.plot(result[\"history\"].history[\"val_loss\"], label=\"Validation Loss\")\n        plt.title(\"Model Loss\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.legend()\n\n        plt.subplot(1, 2, 2)\n        plt.plot(\n            result[\"history\"].history[\"root_mean_squared_error\"], label=\"Training RMSE\"\n        )\n        plt.plot(\n            result[\"history\"].history[\"val_root_mean_squared_error\"],\n            label=\"Validation RMSE\",\n        )\n        plt.title(\"Model RMSE\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"RMSE\")\n        plt.legend()\n\n        plt.tight_layout()\n        plt.savefig(\"training_curves.png\")\n        mlflow.log_artifact(\"training_curves.png\")\n        plt.close()\n\n        #Return loss for Hyperopt (it minimizes)\n        return {\"loss\": result[\"val_rmse\"], \"status\": STATUS_OK}\n\n\n#Define search space for hyperparameters\nsearch_space = {\n    \"learning_rate\": hp.loguniform(\"learning_rate\", np.log(1e-5), np.log(1e-1)),\n    \"momentum\": hp.uniform(\"momentum\", 0.0, 0.9),\n}\n\nprint(\"Search space defined:\")\nprint(\"- Learning rate: 1e-5 to 1e-1 (log-uniform)\")\nprint(\"- Momentum: 0.0 to 0.9 (uniform)\")"
  },
  {
    "objectID": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-4-run-the-hyperparameter-optimization",
    "href": "posts/2025-08-02-End-to-end-ML-with-MLflow/index.html#step-4-run-the-hyperparameter-optimization",
    "title": "End-to-end Machine Learning (MLflow + Docker + Google Cloud)",
    "section": "Step 4: Run the hyperparameter optimization",
    "text": "Step 4: Run the hyperparameter optimization\n\n#Create or set experiment\nexperiment_name = \"wine-quality-optimization\"\nmlflow.set_experiment(experiment_name)\n\nprint(f\"Starting hyperparameter optimization experiment: {experiment_name}\")\nprint(\"This will run 15 trials to find optimal hyperparameters...\")\n\nwith mlflow.start_run(run_name=\"hyperparameter-sweep\"):\n    #Log experiment metadata\n    mlflow.log_params(\n        {\n            \"optimization_method\": \"Tree-structured Parzen Estimator (TPE)\",\n            \"max_evaluations\": 15,\n            \"objective_metric\": \"validation_rmse\",\n            \"dataset\": \"wine-quality\",\n            \"model_type\": \"neural_network\",\n        }\n    )\n\n    #Run optimization\n    trials = Trials()\n    best_params = fmin(\n        fn=objective,\n        space=search_space,\n        algo=tpe.suggest,\n        max_evals=15,\n        trials=trials,\n        verbose=True,\n    )\n\n    #Find and log best results\n    best_trial = min(trials.results, key=lambda x: x[\"loss\"])\n    best_rmse = best_trial[\"loss\"]\n\n    #Log optimization results\n    mlflow.log_params(\n        {\n            \"best_learning_rate\": best_params[\"learning_rate\"],\n            \"best_momentum\": best_params[\"momentum\"],\n        }\n    )\n    mlflow.log_metrics(\n        {\n            \"best_val_rmse\": best_rmse,\n            \"total_trials\": len(trials.trials),\n            \"optimization_completed\": 1,\n        }\n    )"
  },
  {
    "objectID": "posts/2025-08-03-Web-scraping-python/index.html",
    "href": "posts/2025-08-03-Web-scraping-python/index.html",
    "title": "Web Scraping with Python",
    "section": "",
    "text": "BibTeX citation:@online{okola2025,\n  author = {Okola, Basil},\n  title = {Web {Scraping} with {Python}},\n  date = {2025-08-03},\n  url = {https://bokola.github.io/posts/2025-08-03-Web-scraping-python/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nOkola, Basil. 2025. “Web Scraping with Python.” August 3,\n2025. https://bokola.github.io/posts/2025-08-03-Web-scraping-python/.\nThis is a web scrapping task to find conflict/war related news articles from the internet. There is been quite a lot of that considering the Russia/Ukraine conflict, The South Sudan conflict, and the Palestine/Israel conflict just to mention the most reported cases."
  },
  {
    "objectID": "posts/2025-08-03-Web-scraping-python/index.html#using-beautifulsoup-requests",
    "href": "posts/2025-08-03-Web-scraping-python/index.html#using-beautifulsoup-requests",
    "title": "Web Scraping with Python",
    "section": "Using Beautifulsoup + requests",
    "text": "Using Beautifulsoup + requests\n\nUnderstanding website’s structure\nPrior to scraping inspect the HTML source code of the web page to identify the elements you want to scrape\n\n\nSet up your develpment environment\nCreate a virtual environment, follow prompts per your IDE. For VScode I pressed Ctrl+Shift+Pthen searched Python: Create Environment A beginner web scraper in Python is advised to start with requests and beautifulsoup4 librarires which is what we will use. \nimport requests\nfrom bs4 import BeautifulSoup\n\nbaseurl = \"https://news.ycombinator.com\"\nuser = \"\"\npassd = \"\"\n\ns = requests.Session()\ndata = {\"goto\": \"news\", \"acct\": user, \"pw\": passd}\nr = s.post(f'{baseurl}', data=data)\n\nsoup = BeautifulSoup(r.text, 'html.parser')\nif soup.find(id='logout') is not None:\n    print(\"Successfully logged in\")\nelse:\n    print(\"Authentication error\")\n\n\nInspect HTML element\nEach post is wrapped in a &lt;tr&gt; tag with the class athing\n\nfrom PIL import Image\nfrom IPython.display import display\n\nimport matplotlib.image as mpimg\nimage_path = \"hacker-element.png\"\n\nimage = Image.open(image_path)\ndisplay(image)\n\n\n\n\n\n\n\n\n\n\nScrape with requests + beautifulsoup4\n\nimport requests\nfrom bs4 import BeautifulSoup\n\nr = requests.get(\"https://news.ycombinator.com/\")\nsoup = BeautifulSoup(r.text, 'html.parser')\nlinks = soup.find_all('tr', class_='athing')\n\nformatted_links = []\nfor link in links:\n    data = {\n        'id': link['id'],\n        'title': link.find_all(\"td\")[2].a.text,\n        'url': link.find_all(\"td\")[2].a['href'],\n        'rank': int(link.find_all(\"td\")[0].span.text.replace('.', ''))\n    }\n    formatted_links.append(data)\n\n\n    formatted_links.append(data)\n    \nprint(formatted_links)\n\n[{'id': '44754697', 'title': 'New quantum state of matter found at interface of exotic materials', 'url': 'https://phys.org/news/2025-07-quantum-state-interface-exotic-materials.html', 'rank': 1}, {'id': '44754697', 'title': 'New quantum state of matter found at interface of exotic materials', 'url': 'https://phys.org/news/2025-07-quantum-state-interface-exotic-materials.html', 'rank': 1}, {'id': '44778936', 'title': 'Modern Node.js Patterns', 'url': 'https://kashw1n.com/blog/nodejs-2025/', 'rank': 2}, {'id': '44778936', 'title': 'Modern Node.js Patterns', 'url': 'https://kashw1n.com/blog/nodejs-2025/', 'rank': 2}, {'id': '44780353', 'title': 'So you want to parse a PDF?', 'url': 'https://eliot-jones.com/2025/8/pdf-parsing-xref', 'rank': 3}, {'id': '44780353', 'title': 'So you want to parse a PDF?', 'url': 'https://eliot-jones.com/2025/8/pdf-parsing-xref', 'rank': 3}, {'id': '44779428', 'title': 'Writing a good design document', 'url': 'https://grantslatton.com/how-to-design-document', 'rank': 4}, {'id': '44779428', 'title': 'Writing a good design document', 'url': 'https://grantslatton.com/how-to-design-document', 'rank': 4}, {'id': '44777760', 'title': 'Persona vectors: Monitoring and controlling character traits in language models', 'url': 'https://www.anthropic.com/research/persona-vectors', 'rank': 5}, {'id': '44777760', 'title': 'Persona vectors: Monitoring and controlling character traits in language models', 'url': 'https://www.anthropic.com/research/persona-vectors', 'rank': 5}, {'id': '44781523', 'title': 'A parser for TypeScript types, written in TypeScript types', 'url': 'https://github.com/easrng/tsints', 'rank': 6}, {'id': '44781523', 'title': 'A parser for TypeScript types, written in TypeScript types', 'url': 'https://github.com/easrng/tsints', 'rank': 6}, {'id': '44775563', 'title': \"If you're remote, ramble\", 'url': 'https://stephango.com/ramblings', 'rank': 7}, {'id': '44775563', 'title': \"If you're remote, ramble\", 'url': 'https://stephango.com/ramblings', 'rank': 7}, {'id': '44765562', 'title': 'Life, Work, Death and the Peasant: Family Formation', 'url': 'https://acoup.blog/2025/08/01/collections-life-work-death-and-the-peasant-part-iiia-family-formation/', 'rank': 8}, {'id': '44765562', 'title': 'Life, Work, Death and the Peasant: Family Formation', 'url': 'https://acoup.blog/2025/08/01/collections-life-work-death-and-the-peasant-part-iiia-family-formation/', 'rank': 8}, {'id': '44777766', 'title': 'How Python grew from a language to a community', 'url': 'https://thenewstack.io/how-python-grew-from-a-language-to-a-community/', 'rank': 9}, {'id': '44777766', 'title': 'How Python grew from a language to a community', 'url': 'https://thenewstack.io/how-python-grew-from-a-language-to-a-community/', 'rank': 9}, {'id': '44781116', 'title': 'Why doctors hate their computers (2018)', 'url': 'https://www.newyorker.com/magazine/2018/11/12/why-doctors-hate-their-computers', 'rank': 10}, {'id': '44781116', 'title': 'Why doctors hate their computers (2018)', 'url': 'https://www.newyorker.com/magazine/2018/11/12/why-doctors-hate-their-computers', 'rank': 10}, {'id': '44780878', 'title': 'Typed languages are better suited for vibecoding', 'url': 'https://solmaz.io/typed-languages-are-better-suited-for-vibecoding', 'rank': 11}, {'id': '44780878', 'title': 'Typed languages are better suited for vibecoding', 'url': 'https://solmaz.io/typed-languages-are-better-suited-for-vibecoding', 'rank': 11}, {'id': '44782046', 'title': 'Rising Young Worker Despair in the United States', 'url': 'https://www.nber.org/papers/w34071', 'rank': 12}, {'id': '44782046', 'title': 'Rising Young Worker Despair in the United States', 'url': 'https://www.nber.org/papers/w34071', 'rank': 12}, {'id': '44743631', 'title': 'C++: \"model of the hardware\" vs. \"model of the compiler\" (2018)', 'url': 'http://ithare.com/c-model-of-the-hardware-vs-model-of-the-compiler/', 'rank': 13}, {'id': '44743631', 'title': 'C++: \"model of the hardware\" vs. \"model of the compiler\" (2018)', 'url': 'http://ithare.com/c-model-of-the-hardware-vs-model-of-the-compiler/', 'rank': 13}, {'id': '44780540', 'title': 'How to grow almost anything', 'url': 'https://howtogrowalmostanything.notion.site/htgaa25', 'rank': 14}, {'id': '44780540', 'title': 'How to grow almost anything', 'url': 'https://howtogrowalmostanything.notion.site/htgaa25', 'rank': 14}, {'id': '44767508', 'title': 'Efficiently Generating a Number in a Range (2018)', 'url': 'https://www.pcg-random.org/posts/bounded-rands.html', 'rank': 15}, {'id': '44767508', 'title': 'Efficiently Generating a Number in a Range (2018)', 'url': 'https://www.pcg-random.org/posts/bounded-rands.html', 'rank': 15}, {'id': '44745441', 'title': \"2,500-year-old Siberian 'ice mummy' had intricate tattoos, imaging reveals\", 'url': 'https://www.bbc.com/news/articles/c4gzx0zm68vo', 'rank': 16}, {'id': '44745441', 'title': \"2,500-year-old Siberian 'ice mummy' had intricate tattoos, imaging reveals\", 'url': 'https://www.bbc.com/news/articles/c4gzx0zm68vo', 'rank': 16}, {'id': '44765730', 'title': 'Welcome to url.town, population 465', 'url': 'https://url.town/', 'rank': 17}, {'id': '44765730', 'title': 'Welcome to url.town, population 465', 'url': 'https://url.town/', 'rank': 17}, {'id': '44775700', 'title': 'Tokens are getting more expensive', 'url': 'https://ethanding.substack.com/p/ai-subscriptions-get-short-squeezed', 'rank': 18}, {'id': '44775700', 'title': 'Tokens are getting more expensive', 'url': 'https://ethanding.substack.com/p/ai-subscriptions-get-short-squeezed', 'rank': 18}, {'id': '44760583', 'title': 'Survival at High Altitudes: Wheel-Well Passengers (1996)', 'url': 'https://rosap.ntl.bts.gov/view/dot/57536', 'rank': 19}, {'id': '44760583', 'title': 'Survival at High Altitudes: Wheel-Well Passengers (1996)', 'url': 'https://rosap.ntl.bts.gov/view/dot/57536', 'rank': 19}, {'id': '44781189', 'title': 'Poorest US workers hit hardest by slowing wage growth', 'url': 'https://www.ft.com/content/cfb77a53-fef8-4382-b102-c217e0aa4b25', 'rank': 20}, {'id': '44781189', 'title': 'Poorest US workers hit hardest by slowing wage growth', 'url': 'https://www.ft.com/content/cfb77a53-fef8-4382-b102-c217e0aa4b25', 'rank': 20}, {'id': '44774104', 'title': 'Twenty Eighth International Obfuscated C Code Contest', 'url': 'https://www.ioccc.org/2024/index.html', 'rank': 21}, {'id': '44774104', 'title': 'Twenty Eighth International Obfuscated C Code Contest', 'url': 'https://www.ioccc.org/2024/index.html', 'rank': 21}, {'id': '44764696', 'title': 'A dedicated skin-to-brain circuit for cool sensation in mice', 'url': 'https://www.sciencedaily.com/releases/2025/07/250730030354.htm', 'rank': 22}, {'id': '44764696', 'title': 'A dedicated skin-to-brain circuit for cool sensation in mice', 'url': 'https://www.sciencedaily.com/releases/2025/07/250730030354.htm', 'rank': 22}, {'id': '44777055', 'title': 'This Old SGI: notes and memoirs on the Silicon Graphics 4D series (1996)', 'url': 'https://archive.irixnet.org/thisoldsgi/', 'rank': 23}, {'id': '44777055', 'title': 'This Old SGI: notes and memoirs on the Silicon Graphics 4D series (1996)', 'url': 'https://archive.irixnet.org/thisoldsgi/', 'rank': 23}, {'id': '44775830', 'title': 'How to make almost anything (2019)', 'url': 'https://fab.cba.mit.edu/classes/863.19/CBA/people/dsculley/index.html', 'rank': 24}, {'id': '44775830', 'title': 'How to make almost anything (2019)', 'url': 'https://fab.cba.mit.edu/classes/863.19/CBA/people/dsculley/index.html', 'rank': 24}, {'id': '44779839', 'title': 'Everything to know about UniFi OS Server', 'url': 'https://deluisio.com/networking/unifi/2025/08/03/everything-you-need-to-know-about-unifi-os-server-before-you-waste-time-testing-it/', 'rank': 25}, {'id': '44779839', 'title': 'Everything to know about UniFi OS Server', 'url': 'https://deluisio.com/networking/unifi/2025/08/03/everything-you-need-to-know-about-unifi-os-server-before-you-waste-time-testing-it/', 'rank': 25}, {'id': '44762397', 'title': 'Show HN: Schematra – Sinatra-inspired minimal web framework for Chicken Scheme', 'url': 'https://github.com/rolandoam/schematra', 'rank': 26}, {'id': '44762397', 'title': 'Show HN: Schematra – Sinatra-inspired minimal web framework for Chicken Scheme', 'url': 'https://github.com/rolandoam/schematra', 'rank': 26}, {'id': '44754789', 'title': 'The first lunar road trip', 'url': 'https://nautil.us/the-first-lunar-road-trip-1227738/', 'rank': 27}, {'id': '44754789', 'title': 'The first lunar road trip', 'url': 'https://nautil.us/the-first-lunar-road-trip-1227738/', 'rank': 27}, {'id': '44771808', 'title': 'Lina Khan points to Figma IPO as vindication of M&A scrutiny', 'url': 'https://techcrunch.com/2025/08/02/lina-khan-points-to-figma-ipo-as-vindication-for-ma-scrutiny/', 'rank': 28}, {'id': '44771808', 'title': 'Lina Khan points to Figma IPO as vindication of M&A scrutiny', 'url': 'https://techcrunch.com/2025/08/02/lina-khan-points-to-figma-ipo-as-vindication-for-ma-scrutiny/', 'rank': 28}, {'id': '44780552', 'title': 'Learnable Programming (2012)', 'url': 'https://worrydream.com/LearnableProgramming/', 'rank': 29}, {'id': '44780552', 'title': 'Learnable Programming (2012)', 'url': 'https://worrydream.com/LearnableProgramming/', 'rank': 29}, {'id': '44779178', 'title': 'Shrinking freshwater availability increasing land contribution to sea level rise', 'url': 'https://news.asu.edu/20250725-environment-and-sustainability-new-global-study-shows-freshwater-disappearing-alarming', 'rank': 30}, {'id': '44779178', 'title': 'Shrinking freshwater availability increasing land contribution to sea level rise', 'url': 'https://news.asu.edu/20250725-environment-and-sustainability-new-global-study-shows-freshwater-disappearing-alarming', 'rank': 30}]\n\n\n\n\nStore data as .csv\n\nimport csv\n\nfile = 'hacker_news_posts.csv'\nwith open(file, 'w', newline=\"\") as f:\n    writer = csv.DictWriter(f, fieldnames=['id', 'title', 'url', 'rank'])\n    writer.writeheader()\n    for row in formatted_links:\n        writer.writerow(row)\n\n\n\nStore data in PostgreSQL\n\nStep 1: Installing PostgreSQL\nFollow the PostgreSQL download page for downloads and installation\n\n\nStep 2: Creating a Database Table\nFirst you’ll need a table\n\n#start service\nsudo systemctl start postgresql.service\n\n#log in as a superuser\nsudo -i -u postgres\n\nCREATE DATABASE scrape_demo;\n\n\nCREATE TABLE \"hn_links\" (\n    \"id\" INTEGER NOT NULL,\n    \"title\" VARCHAR NOT NULL,\n    \"url\" VARCHAR NOT NULL,\n    \"rank\" INTEGER NOT NULL\n);\n\n\nStep 3: Install Psycopg2 to Connect to PostgreSQL\npip install psycopg2\nEstablish connection to the database\nEnsure you set password for postgres user, which logs without a password by default\n\nsudo -u postgres psql\n\npostgres=# ALTER USER postgres PASSWORD 'myPassword';\nALTER ROLE\n\n\nimport psycopg2\nimport os\nimport dotenv\ndotenv.load_dotenv()\n\np = os.getenv(\"pass\")\ntable_name = \"hn_links\"\ncsv_path = \"hacker_news_posts.csv\"\n\n\ncon = psycopg2.connect(host=\"127.0.0.1\", port=\"5432\", user=\"postgres\", password = p,database=\"scrape_demo\")\n\n# Get a database cursor\ncur = con.cursor()\n\nr = requests.get('https://news.ycombinator.com')\nsoup = BeautifulSoup(r.text, 'html.parser')\nlinks = soup.findAll('tr', class_='athing')\n\nfor link in links:\n    cur.execute(\"\"\"\n        INSERT INTO hn_links (id, title, url, rank)\n        VALUES (%s, %s, %s, %s)\n        \"\"\",\n        (\n            link['id'],\n            link.find_all('td')[2].a.text,\n            link.find_all('td')[2].a['href'],\n            int(link.find_all('td')[0].span.text.replace('.', ''))\n        )\n    )\n\n# Commit the data\ncon.commit()\n\n# Close our database connections\ncur.close()\ncon.close()\n\n/tmp/ipykernel_21147/1184676145.py:18: DeprecationWarning: Call to deprecated method findAll. (Replaced by find_all) -- Deprecated since version 4.0.0.\n  links = soup.findAll('tr', class_='athing')"
  },
  {
    "objectID": "posts/2025-08-03-Web-scraping-python/index.html#using-scapingbee-python-client",
    "href": "posts/2025-08-03-Web-scraping-python/index.html#using-scapingbee-python-client",
    "title": "Web Scraping with Python",
    "section": "Using ScapingBee Python Client",
    "text": "Using ScapingBee Python Client\nScrapingBee is a subscription API providing a way to bypass any website’s anti-scraping measures.\n\nfrom scrapingbee import ScrapingBeeClient\nimport json\nimport pandas as pd\n\ndotenv.load_dotenv()\nkey = os.getenv(\"spring_bee_api_key\")\n\nsb_client = ScrapingBeeClient(api_key=key)\nurl = \"https://www.aljazeera.com/\"\n\n\n\nclient = ScrapingBeeClient(api_key=key)\n\ndef google_news_headlines_api(country_code='US'):\n\n    extract_rules = {\n        \"news\": {\n        \"selector\": \"article\",\n        \"type\": \"list\",\n            \"output\": {\n                \"title\": \".gPFEn,.JtKRv\",\n                \"source\": \".vr1PYe\",\n                \"time\": \"time@datetime\",\n                \"author\": \".bInasb\",\n                \"link\": \".WwrzSb@href\"\n            }\n        }\n    }\n\n    js_scenario = {\n        \"instructions\":[\n            {\"evaluate\":\"document.querySelectorAll('.WwrzSb').forEach( (e) =&gt; e.href = e.href );\"}\n        ]\n    }\n\n    response =  client.get(\n        f'https://news.google.com/topics/CAAqJggKIiBDQkFTRWdvSUwyMHZNRFZxYUdjU0FtVnVHZ0pWVXlnQVAB?&gl={country_code}',\n        params={ \n            \"custom_google\": \"true\",\n            \"wait_for\": \".bInasb\",\n            \"extract_rules\": extract_rules,\n            \"js_scenario\": js_scenario, \n        },\n        retries=2\n    )\n\n    if response.text.startswith('{\"message\":\"Invalid api key:'):\n        return f\"Oops! It seems you may have missed adding your API KEY or you are using an incorrect key.\\nGet your free API KEY and 1000 free scraping credits by signing up to our platform here: https://app.scrapingbee.com/account/register\"\n    else:\n        def get_info():\n            if len(response.json()['news']) == 0:\n                return \"FAILED TO RETRIEVE NEWS\"\n            else:\n                return \"SUCCESS\"\n\n        return pd.DataFrame({\n            'count': len(response.json()['news']),\n            'news_extracts': response.json()['news'],\n            'info': f\"{response.status_code} {get_info()}\",\n        })\n#country_code: Set the news location; US, IN, etc.\ndf = google_news_headlines_api(country_code='US')\n\nprint(df.iloc[:10])\n\n   count                                      news_extracts         info\n0    263  {'title': 'Texas Democrats Leave State to Stop...  200 SUCCESS\n1    263  {'title': 'Democrats flee Texas to block Repub...  200 SUCCESS\n2    263  {'title': 'A Texas Democratic lawmaker on thei...  200 SUCCESS\n3    263  {'title': 'Texas Democrats flee state to preve...  200 SUCCESS\n4    263  {'title': 'Videos of emaciated hostages condem...  200 SUCCESS\n5    263  {'title': 'Hamas says it will allow aid for ho...  200 SUCCESS\n6    263  {'title': 'Netanyahu asks Red Cross to help ho...  200 SUCCESS\n7    263  {'title': 'Hamas says open to ICRC delivering ...  200 SUCCESS\n8    263  {'title': 'White House advisers defend Trump’s...  200 SUCCESS\n9    263  {'title': 'Trump Fired America’s Economic Data...  200 SUCCESS"
  },
  {
    "objectID": "posts/2025-08-03-Web-scraping-python/index.html#web-scraping-with-scrapy",
    "href": "posts/2025-08-03-Web-scraping-python/index.html#web-scraping-with-scrapy",
    "title": "Web Scraping with Python",
    "section": "Web scraping with Scrapy",
    "text": "Web scraping with Scrapy\nScrapy is a web scraping framework using an event-driven networking infrastracture built around an asynchronous network engine that allows for more efficiency and scalability. It is made of a crawler that handles low-level logic, and a spider that is provider by the user to help the crawler generate request, parse and retrieve data.\nIn this section we use scrapy to scrape product listings available at web-scraping.dev, but first some house-keeping.\nTo install scrapy run pip install scrapy or better still add scrapy to your project’s requirements.txt and run pip install -r requirements.txt. Start a scrapy project by running scrapy startproject &lt;project-name&gt; &lt;project-directory&gt; in terminal. Some scrapy commands below:\n\n!scrapy --help\n\nScrapy 2.13.3 - active project: webscrapingdev\n\nUsage:\n  scrapy &lt;command&gt; [options] [args]\n\nAvailable commands:\n  bench         Run quick benchmark test\n  check         Check spider contracts\n  crawl         Run a spider\n  edit          Edit spider\n  fetch         Fetch a URL using the Scrapy downloader\n  genspider     Generate new spider using pre-defined templates\n  list          List available spiders\n  parse         Parse URL (using its spider) and print the results\n  runspider     Run a self-contained spider (without creating a project)\n  settings      Get settings values\n  shell         Interactive scraping console\n  startproject  Create new project\n  version       Print Scrapy version\n  view          Open URL in browser, as seen by Scrapy\n\nUse \"scrapy &lt;command&gt; -h\" to see more info about a command\n\n\n\nCreating a spider\nrun scrapy genspider &lt;name&gt; &lt;host-to-scrape&gt;\n\n!scrapy genspider products web-scraping.dev\n\nSpider 'products' already exists in module:\n  webscrapingdev.spiders.products\n\n\n\n!scrapy list\n!tree\n\n\nproducts\n\n.\n\n├── article-class.png\n\n├── hacker-element.png\n\n├── hacker_news_posts.csv\n\n├── LICENSE\n\n├── producthunt.json\n\n├── README.md\n\n├── requirements.txt\n\n├── results.json\n\n├── scrapy.cfg\n\n├── web-scrap_files\n\n│   ├── figure-html\n\n│   │   └── cell-2-output-1.png\n\n│   └── libs\n\n│       ├── bootstrap\n\n│       │   ├── bootstrap-b9f025fa521194ab51f5de92fbd134be.min.css\n\n│       │   ├── bootstrap-icons.css\n\n│       │   ├── bootstrap-icons.woff\n\n│       │   └── bootstrap.min.js\n\n│       ├── clipboard\n\n│       │   └── clipboard.min.js\n\n│       └── quarto-html\n\n│           ├── anchor.min.js\n\n│           ├── popper.min.js\n\n│           ├── quarto.js\n\n│           ├── quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css\n\n│           ├── tabsets\n\n│           │   └── tabsets.js\n\n│           ├── tippy.css\n\n│           └── tippy.umd.min.js\n\n├── web-scrap.html\n\n├── webscrapingdev\n\n│   ├── __init__.py\n\n│   ├── items.py\n\n│   ├── middlewares.py\n\n│   ├── pipelines.py\n\n│   ├── __pycache__\n\n│   │   ├── __init__.cpython-312.pyc\n\n│   │   └── settings.cpython-312.pyc\n\n│   ├── settings.py\n\n│   └── spiders\n\n│       ├── __init__.py\n\n│       ├── products.py\n\n│       └── __pycache__\n\n│           ├── __init__.cpython-312.pyc\n\n│           └── products.cpython-312.pyc\n\n├── web-scrap.ipynb\n\n├── web-scrap.py\n\n├── web-scrap.qmd\n\n└── web-scrap.quarto_ipynb\n\n\n\n12 directories, 38 files\n\n\n\n\nif you open the generated spider - products.py, you’ll find the following\nimport scrapy\n\n\nclass ProductsSpider(scrapy.Spider):\n    name = \"products\"\n    allowed_domains = [\"web-scraping.dev\"]\n    start_urls = [\"https://web-scraping.dev\"]\n\n    def parse(self, response):\n        pass\n\n\nname is used as a reference to the spider for scrapy commands like crawl` - this would run the scraper\nallowed_domains is a safety feauture restricting this spider to crawl only particular domains.\nstart_urls indicates the spider starting point while parse() is the first callback to execute above instructions.\n\n\n\nAdding crawling logic\nWe want our start_urls to be some topic directories e.g., https://www.producthunt.com/topics/developer-tools and our parse() callback method to find all product links and schedule them to be scrapped:\n# /spiders/products.py\nimport scrapy\nfrom scrapy.http import Response, Request\n\n\nclass ProductsSpider(scrapy.Spider):\n    name = 'products'\n    allowed_domains = ['web-scraping.dev']\n    start_urls = [\n        'https://web-scraping.dev/products',\n    ]\n\n    def parse(self, response: Response):\n        product_urls = response.xpath(\n            \"//div[@class='row product']/div/h3/a/@href\"\n        ).getall()\n        for url in product_urls:\n            yield Request(url, callback=self.parse_product)\n        # or shortcut in scrapy &gt;2.0\n        # yield from response.follow_all(product_urls, callback=self.parse_product)\n    \n    def parse_product(self, response: Response):\n        print(response)\n\n\n\nAdding Parsing Logic\nPopulate parse_product()\n# /spiders/products.py\n...\n\n    def parse_product(self, response: Response):\n        yield {\n            \"title\": response.xpath(\"//h3[contains(@class, 'product-title')]/text()\").get(),\n            \"price\": response.xpath(\"//span[contains(@class, 'product-price')]/text()\").get(),\n            \"image\": response.xpath(\"//div[contains(@class, 'product-image')]/img/@src\").get(),\n            \"description\": response.xpath(\"//p[contains(@class, 'description')]/text()\").get()\n        }\n\n\n\nBasic Settings\nAdjust recommended settings:\n# settings.py\n# will ignore /robots.txt rules that might prevent scraping\nROBOTSTXT_OBEY = False\n# will cache all request to /httpcache directory which makes running spiders in development much quicker\n# tip: to refresh cache just delete /httpcache directory\nHTTPCACHE_ENABLED = True\n# while developing we want to see debug logs\nLOG_LEVEL = \"DEBUG\" # or \"INFO\" in production\n\n# to avoid basic bot detection we want to set some basic headers\nDEFAULT_REQUEST_HEADERS = {\n    # we should use headers\n    'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\",\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n    'Accept-Language': 'en',\n}\n\n\n\nRunning Spiders\nEither through the scrapy command or explicitly calling scrapy using a Python script.\n\n%%capture\n!scrapy crawl products\n\n\n\nSaving results\n\n%%capture\n\n!scrapy crawl products --output results.json\n\n\n!tree\n\n\n.\n\n├── article-class.png\n\n├── hacker-element.png\n\n├── hacker_news_posts.csv\n\n├── LICENSE\n\n├── producthunt.json\n\n├── README.md\n\n├── requirements.txt\n\n├── results.json\n\n├── scrapy.cfg\n\n├── web-scrap_files\n\n│   ├── figure-html\n\n│   │   └── cell-2-output-1.png\n\n│   └── libs\n\n│       ├── bootstrap\n\n│       │   ├── bootstrap-b9f025fa521194ab51f5de92fbd134be.min.css\n\n│       │   ├── bootstrap-icons.css\n\n│       │   ├── bootstrap-icons.woff\n\n│       │   └── bootstrap.min.js\n\n│       ├── clipboard\n\n│       │   └── clipboard.min.js\n\n│       └── quarto-html\n\n│           ├── anchor.min.js\n\n│           ├── popper.min.js\n\n│           ├── quarto.js\n\n│           ├── quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css\n\n│           ├── tabsets\n\n│           │   └── tabsets.js\n\n│           ├── tippy.css\n\n│           └── tippy.umd.min.js\n\n├── web-scrap.html\n\n├── webscrapingdev\n\n│   ├── __init__.py\n\n│   ├── items.py\n\n│   ├── middlewares.py\n\n│   ├── pipelines.py\n\n│   ├── __pycache__\n\n│   │   ├── __init__.cpython-312.pyc\n\n│   │   └── settings.cpython-312.pyc\n\n│   ├── settings.py\n\n│   └── spiders\n\n│       ├── __init__.py\n\n│       ├── products.py\n\n│       └── __pycache__\n\n│           ├── __init__.cpython-312.pyc\n\n│           └── products.cpython-312.pyc\n\n├── web-scrap.ipynb\n\n├── web-scrap.py\n\n├── web-scrap.qmd\n\n└── web-scrap.quarto_ipynb\n\n\n\n12 directories, 38 files\n\n\n\n\n\nimport json\njson_file = 'results.json'\nwith open(json_file) as f:\n    j_obj = json.load(f)\n\n\njson_fmt = json.dumps(j_obj, indent=2)\nprint(json_fmt)\n\n[\n  {\n    \"title\": \"Blue Energy Potion\",\n    \"price\": \"$4.99\",\n    \"image\": \"https://web-scraping.dev/assets/products/blue-potion.webp\",\n    \"description\": \"Ignite your gaming sessions with our 'Blue Energy Potion', a premium energy drink crafted for dedicated gamers. Inspired by the classic video game potions, this energy drink provides a much-needed boost to keep you focused and energized. It's more than just an energy drink - it's an ode to the gaming culture, packaged in an aesthetically pleasing potion-like bottle that'll make you feel like you're in your favorite game world. Drink up and game on!\"\n  },\n  {\n    \"title\": \"Red Energy Potion\",\n    \"price\": \"$4.99\",\n    \"image\": \"https://web-scraping.dev/assets/products/red-potion.webp\",\n    \"description\": \"Elevate your game with our 'Red Potion', an extraordinary energy drink that's as enticing as it is effective. This fiery red potion delivers an explosive berry flavor and an energy kick that keeps you at the top of your game. Are you ready to level up?\"\n  },\n  {\n    \"title\": \"Teal Energy Potion\",\n    \"price\": \"$4.99\",\n    \"image\": \"https://web-scraping.dev/assets/products/teal-potion.webp\",\n    \"description\": \"Experience a surge of vitality with our 'Teal Potion', an exceptional energy drink designed for the gaming community. With its intriguing teal color and a flavor that keeps you asking for more, this potion is your best companion during those long gaming nights. Every sip is an adventure - let the quest begin!\"\n  },\n  {\n    \"title\": \"Dark Red Energy Potion\",\n    \"price\": \"$4.99\",\n    \"image\": \"https://web-scraping.dev/assets/products/darkred-potion.webp\",\n    \"description\": \"Unleash the power within with our 'Dark Red Potion', an energy drink as intense as the games you play. Its deep red color and bold cherry cola flavor are as inviting as they are invigorating. Bring out the best in your gaming performance, and unlock your full potential.\"\n  },\n  {\n    \"title\": \"Box of Chocolate Candy\",\n    \"price\": \"$9.99 \",\n    \"image\": \"https://web-scraping.dev/assets/products/orange-chocolate-box-small-1.webp\",\n    \"description\": \"Indulge your sweet tooth with our Box of Chocolate Candy. Each box contains an assortment of rich, flavorful chocolates with a smooth, creamy filling. Choose from a variety of flavors including zesty orange and sweet cherry. Whether you're looking for the perfect gift or just want to treat yourself, our Box of Chocolate Candy is sure to satisfy.\"\n  }\n]"
  },
  {
    "objectID": "posts/2025-08-08-data-enginnering/index.html",
    "href": "posts/2025-08-08-data-enginnering/index.html",
    "title": "End-to-end Data Engineering with Python DuckDB, Google Cloud, dbt",
    "section": "",
    "text": "BibTeX citation:@online{okola2025,\n  author = {Okola, Basil},\n  title = {End-to-End {Data} {Engineering} with {Python} {DuckDB,}\n    {Google} {Cloud,} Dbt},\n  date = {2025-08-08},\n  url = {https://bokola.github.io/posts/2025-08-08-data-engineering/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nOkola, Basil. 2025. “End-to-End Data Engineering with Python\nDuckDB, Google Cloud, Dbt.” August 8, 2025. https://bokola.github.io/posts/2025-08-08-data-engineering/.\nThis is a ‘do it along’ following DuckDB material. We will source data from PyPi, a repository of python packages providing logs of how given libraries have been used across platforms, transform it into relevant tables, and feed it into a dashboard using SQL, dtb - a tool for building modular, maintainable data pipelines to power analytics, and DuckDB - an in-process SQL online analytical processing (OLAPs) relational database management system; then use evidence to create a dashboard utilizing SQL and markdown."
  },
  {
    "objectID": "posts/2025-08-08-data-enginnering/index.html#part-1-ingestion-pipeline",
    "href": "posts/2025-08-08-data-enginnering/index.html#part-1-ingestion-pipeline",
    "title": "End-to-end Data Engineering with Python DuckDB, Google Cloud, dbt",
    "section": "Part 1: Ingestion pipeline",
    "text": "Part 1: Ingestion pipeline\n\nSetup & requirements\n\nPython 3.12\nPoetry for dependency management\nMake to run Makefile commands\nA Google Cloud account to fetch the source data. Free tier covers any computing cost.\nA host of libraries: poetry duckdb google-cloud-bigquery google-auth google-cloud-bigquery-storage pyarrow pandas fire loguru pydantic pytest ruff\n\nThe architecture woul look like below\n\nfrom PIL import Image\nfrom IPython.display import display\n\nimg_f = \"./docs/architecture.png\"\nimg = Image.open(img_f)\ndisplay(img)"
  },
  {
    "objectID": "posts/2025-08-08-data-enginnering/index.html#exploring-the-data",
    "href": "posts/2025-08-08-data-enginnering/index.html#exploring-the-data",
    "title": "End-to-end Data Engineering with Python DuckDB, Google Cloud, dbt",
    "section": "Exploring the data",
    "text": "Exploring the data\nHead to your google cloud console, then under BigQuery search “file_downloads” and click SEARCH ALL PROJECTS.\n\nimg_f = \"./docs/big_query_search.png\"\nimg = Image.open(img_f)\ndisplay(img)\n\n\n\n\n\n\n\n\nLimit your query to a given timestamp as the data is very big.\n\nimg_f = \"./docs/query.png\"\nimg = Image.open(img_f)\ndisplay(img)\n\n\n\n\n\n\n\n\nThe ingestion part is bundled in /ingestion directory with the following primary files:\n\n!tree ./ingestion\n\n\n./ingestion\n\n├── bigquery.py\n\n├── duck.py\n\n├── models.py\n\n├── pipeline.py\n\n└── __pycache__\n\n    ├── bigquery.cpython-312.pyc\n\n    └── pipeline.cpython-312.pyc\n\n\n\n2 directories, 6 files\n\n\n\n\n\nbigquery.py has BigQuery helpers to get the client, get query results and query the public dataset\nduck.py contains DuckDB helpers to create a table from a dataframe, and write to a local or MotherDuck database\nmodels.py defines table schema\npipeline runs the ingestion pipeline use pydantic syntax\n\nThe pipeline was successfuly executed and data written locally and to motherduck:\n\nimg_f = \"./docs/motherduck_db.png\"\nimg = Image.open(img_f)\ndisplay(img)"
  },
  {
    "objectID": "posts/2025-08-08-data-engineering/index.html",
    "href": "posts/2025-08-08-data-engineering/index.html",
    "title": "End-to-end Data Engineering with Python DuckDB, Google Cloud, dbt",
    "section": "",
    "text": "BibTeX citation:@online{okola2025,\n  author = {Okola, Basil},\n  title = {End-to-End {Data} {Engineering} with {Python} {DuckDB,}\n    {Google} {Cloud,} Dbt},\n  date = {2025-08-08},\n  url = {https://bokola.github.io/posts/2025-08-08-data-engineering/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nOkola, Basil. 2025. “End-to-End Data Engineering with Python\nDuckDB, Google Cloud, Dbt.” August 8, 2025. https://bokola.github.io/posts/2025-08-08-data-engineering/.\nThis is a ‘do it along’ following DuckDB material. We will source data from PyPi, a repository of python packages providing logs of how given libraries have been used across platforms, transform it into relevant tables, and feed it into a dashboard using SQL, dtb - a tool for building modular, maintainable data pipelines to power analytics, and DuckDB - an in-process SQL online analytical processing (OLAPs) relational database management system; then use evidence to create a dashboard utilizing SQL and markdown."
  },
  {
    "objectID": "posts/2025-08-08-data-engineering/index.html#part-1-ingestion-pipeline",
    "href": "posts/2025-08-08-data-engineering/index.html#part-1-ingestion-pipeline",
    "title": "End-to-end Data Engineering with Python DuckDB, Google Cloud, dbt",
    "section": "Part 1: Ingestion pipeline",
    "text": "Part 1: Ingestion pipeline\n\nSetup & requirements\n\nPython 3.12\nPoetry for dependency management\nMake to run Makefile commands\nA Google Cloud account to fetch the source data. Free tier covers any computing cost.\nA host of libraries: poetry duckdb google-cloud-bigquery google-auth google-cloud-bigquery-storage pyarrow pandas fire loguru pydantic pytest ruff\n\nThe architecture woul look like below\n\nfrom PIL import Image\nfrom IPython.display import display\n\nimg_f = \"./docs/architecture.png\"\nimg = Image.open(img_f)\ndisplay(img)"
  },
  {
    "objectID": "posts/2025-08-08-data-engineering/index.html#exploring-the-data",
    "href": "posts/2025-08-08-data-engineering/index.html#exploring-the-data",
    "title": "End-to-end Data Engineering with Python DuckDB, Google Cloud, dbt",
    "section": "Exploring the data",
    "text": "Exploring the data\nHead to your google cloud console, then under BigQuery search “file_downloads” and click SEARCH ALL PROJECTS.\n\nimg_f = \"./docs/big_query_search.png\"\nimg = Image.open(img_f)\ndisplay(img)\n\n\n\n\n\n\n\n\nLimit your query to a given timestamp as the data is very big.\n\nimg_f = \"./docs/query.png\"\nimg = Image.open(img_f)\ndisplay(img)\n\n\n\n\n\n\n\n\nThe ingestion part is bundled in /ingestion directory with the following primary files:\n\n!tree ./ingestion\n\n\n./ingestion\n\n├── bigquery.py\n\n├── duck.py\n\n├── models.py\n\n├── pipeline.py\n\n└── __pycache__\n\n    ├── bigquery.cpython-312.pyc\n\n    └── pipeline.cpython-312.pyc\n\n\n\n2 directories, 6 files\n\n\n\n\n\nbigquery.py has BigQuery helpers to get the client, get query results and query the public dataset\nduck.py contains DuckDB helpers to create a table from a dataframe, and write to a local or MotherDuck database\nmodels.py defines table schema\npipeline runs the ingestion pipeline use pydantic syntax\n\nThe pipeline was successfuly executed and data written locally and to motherduck:\n\nimg_f = \"./docs/motherduck_db.png\"\nimg = Image.open(img_f)\ndisplay(img)"
  },
  {
    "objectID": "posts/2025-10-29-data-pub-ready-viz/index.html",
    "href": "posts/2025-10-29-data-pub-ready-viz/index.html",
    "title": "Publication ready visualization in R",
    "section": "",
    "text": "CitationBibTeX citation:@online{okola2025,\n  author = {Okola, Basil},\n  title = {Publication Ready Visualization in {R}},\n  date = {2025-10-29},\n  url = {https://bokola.github.io/posts/2025-10-29-pub-ready-viz/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nOkola, Basil. 2025. “Publication Ready Visualization in R.”\nOctober 29, 2025. https://bokola.github.io/posts/2025-10-29-pub-ready-viz/."
  },
  {
    "objectID": "posts/2025-10-29-pub-ready-viz/index.html",
    "href": "posts/2025-10-29-pub-ready-viz/index.html",
    "title": "Publication ready visualization in R",
    "section": "",
    "text": "CitationBibTeX citation:@online{okola2025,\n  author = {Okola, Basil},\n  title = {Publication Ready Visualization in {R}},\n  date = {2025-10-29},\n  url = {https://bokola.github.io/posts/2025-10-29-pub-ready-viz/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nOkola, Basil. 2025. “Publication Ready Visualization in R.”\nOctober 29, 2025. https://bokola.github.io/posts/2025-10-29-pub-ready-viz/."
  },
  {
    "objectID": "posts/2025-10-30-interactive-maps/index.html",
    "href": "posts/2025-10-30-interactive-maps/index.html",
    "title": "Interactive maps (R)",
    "section": "",
    "text": "This document explores making interactive maps using R. There are a number of R packages that can be used to generate such visualizations. We consider just a small number (not exhaustively)."
  },
  {
    "objectID": "posts/2025-10-30-interactive-maps/index.html#data",
    "href": "posts/2025-10-30-interactive-maps/index.html#data",
    "title": "Interactive maps (R)",
    "section": "Data",
    "text": "Data\nWe use gapminder dataset from {gapminder} package. It is contains data from 187 countries covering the periods 1952 - 2007 with the following columns:\n\ncountry: Country name\ncontinent: Continental territory of a country\nyear: 1952 - 2007\nlifeExp: Life expectancy in years\npop: population size\ngdpPercap: GDP per capita in infaltion-adjusted dollars\n\nThe second dataset is the meteorite landings from Tidytuesday project from the Meteoritical Society of NASA. It comes with the following variables:\n\nname: Meteorite name\nmass: Mass in grams\nlat: latitude\nlong: longitude\nfall: fall or found meteorite\n\n\npacman::p_load(\n    leaflet\n    ,gapminder\n    ,echarts4r\n    ,tidyverse\n    ,ggiraph\n    ,widgetframe\n    ,ggthemes\n    ,plotly\n    ,viridis\n    ,DT\n)\n\nFirst we organize the datasets:\n\n# 1. gapminder\n\n# country codes\ncodes &lt;- gapminder::country_codes\n# countries with info unfiltered version\ngapminder &lt;- gapminder::gapminder_unfiltered\n# join\ngapminder &lt;- gapminder %&gt;% left_join(codes) %&gt;%\n  mutate(code = iso_alpha)\n\nJoining with `by = join_by(country)`\n\n# a map of the world - Antarctica removed\nworld &lt;- map_data(\"world\") %&gt;%\n  filter(!grepl(\"antarctica\", region, ignore.case = T))\ngapminder_df &lt;- gapminder %&gt;%\n  inner_join(maps::iso3166 %&gt;% \n  select(a3, mapname), by = c(code = \"a3\")\n  ) %&gt;%\n    mutate(\n      mapname = str_remove(mapname, \"\\\\(.*\")\n    )\n\nWarning in inner_join(., maps::iso3166 %&gt;% select(a3, mapname), by = c(code = \"a3\")): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 605 of `x` matches multiple rows in `y`.\nℹ Row 2 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\nGapminder dataset:\n\ndatatable(gapminder_df)\n\n\n\n\n\n\nmeteorites &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-06-11/meteorites.csv\")\n\nRows: 45716 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): name, name_type, class, fall, geolocation\ndbl (5): id, mass, year, lat, long\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nMeteorite dataset:\n\ndatatable(meteorites)\n\nWarning in instance$preRenderHook(instance): It seems your data is too big for\nclient-side DataTables. You may consider server-side processing:\nhttps://rstudio.github.io/DT/server.html"
  },
  {
    "objectID": "posts/2025-10-30-interactive-maps/index.html#interactive-choropleth-map-with-ggiraph",
    "href": "posts/2025-10-30-interactive-maps/index.html#interactive-choropleth-map-with-ggiraph",
    "title": "Interactive maps (R)",
    "section": "Interactive choropleth map with {ggiraph}",
    "text": "Interactive choropleth map with {ggiraph}\nTo turn a static choropleth map by invoking a tooltip when we hover the pointer over a country, we can use {ggiraph} - geom_polygon_interactive() & girafe and {widgetframe} - framewidget() packages.\nIt is useful creating a reusable theming function:\n\ntheme_helper &lt;- function(){\n   theme(\n    axis.line = element_blank(),\n    axis.text = element_blank(),\n    axis.title = element_blank(),\n    axis.ticks = element_blank(),\n    plot.background = element_rect(fill = \"snow\", color = NA),\n    panel.background = element_rect(fill= \"snow\", color = NA),\n    plot.title = element_text(size = 16, hjust = 0.5),\n    plot.subtitle = element_text(size = 12, hjust = 0.5),\n    plot.caption = element_text(size = 8, hjust = 1),\n    legend.title = element_text(color = \"grey40\", size = 8),\n    legend.text = element_text(color = \"grey40\", size = 7, hjust = 0),\n    legend.position = c(0.05, 0.25),\n    plot.margin = unit(c(0.5,2,0.5,1), \"cm\")) \n}\n\n\nlife_exp_map &lt;- gapminder_df %&gt;%\n  filter(year == 2007) %&gt;%\n  right_join(world, by = c(mapname = \"region\")) %&gt;%\n  ggplot() +\n  geom_polygon_interactive(\n    color = \"white\", size = 0.01, \n    aes(long, lat, group = group, fill = lifeExp,\n    tooltip = sprintf(\"%s&lt;br/&gt;%s\", country, lifeExp))\n  ) +\n    theme_void() + \n    scale_fill_viridis(option = \"B\") +\n    labs(\n      title = \"Life Expectancy\",\n      subtitle = \"Year: 2007\",\n      caption = \"Source: gapminder.org\",\n      fill = \"Years\"\n    ) \n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\nℹ The deprecated feature was likely used in the ggiraph package.\n  Please report the issue at &lt;https://github.com/davidgohel/ggiraph/issues&gt;.\n\nlife_exp_map &lt;- life_exp_map + theme_helper() + coord_fixed(ratio = 1.3)\n\nPrint it interactively\nwidgetframe::frameWidget(girafe(code = print(life_exp_map)))"
  },
  {
    "objectID": "posts/2025-10-30-interactive-maps/index.html#interactive-choropleth-maps-with-plotly",
    "href": "posts/2025-10-30-interactive-maps/index.html#interactive-choropleth-maps-with-plotly",
    "title": "Interactive maps (R)",
    "section": "Interactive choropleth maps with {plotly}",
    "text": "Interactive choropleth maps with {plotly}\n{plotly} on top of tooltips also allows zooming, lasso/box selections or downloading the map as .png.\nlife_exp07 &lt;- gapminder_df %&gt;%\n  filter(year == 2007) %&gt;% \n  select(mapname, code, lifeExp)\n\np_07 &lt;- plot_geo(life_exp07)\n\np_07 &lt;- p_07 %&gt;% add_trace(\n  z = ~lifeExp, color = ~ lifeExp, colors = 'Oranges',\n  text = ~mapname, locations = ~ code\n) %&gt;% colorbar(title = \"Years\")\n\np_07 &lt;- p_07 %&gt;%\n  layout(\n    title = 'Life Expectancy in 2007 &lt;br&gt;Source:&lt;a href= \"https://www.gapminder.org\"&gt; gapminder.org&lt;/a&gt;', geo = p_07\n  )\n\np_07"
  },
  {
    "objectID": "posts/2025-10-30-interactive-maps/index.html#interactive-points-using-plotly",
    "href": "posts/2025-10-30-interactive-maps/index.html#interactive-points-using-plotly",
    "title": "Interactive maps (R)",
    "section": "Interactive points using {plotly}",
    "text": "Interactive points using {plotly}\n\nmeteorites_fell &lt;- meteorites %&gt;%\n  filter(fall == \"Fell\")\n\nmeteorites_map &lt;- list(\n  #scope = 'usa',\n  projection = list(type = 'Mercator'),\n  showland = TRUE,\n  landcolor = toRGB(\"grey80\")\n)\n\n\nmeteo_map &lt;- plot_geo(meteorites_fell, lat = ~lat, lon = ~long)\nmeteo_map &lt;- meteo_map %&gt;% add_markers(\n  text = ~paste(paste(\"Name:\", name), \n                paste(\"Year:\", year), \n                paste(\"Mass:\", mass), sep = \"&lt;br /&gt;\"),\n  color = ~mass, symbol = I(\"circle-dot\"), size = I(8), hoverinfo = \"text\"\n)\nmeteo_map &lt;- meteo_map %&gt;% colorbar(title = \"Mass\")\n\nWarning: Ignoring 10 observations\n\nmeteo_map &lt;- meteo_map %&gt;% layout(\n  title = 'Meteorite Landings&lt;br /&gt;(Meteorite falls)', geo = meteorites_map\n)\n\nmeteo_map"
  },
  {
    "objectID": "posts/2025-10-30-interactive-maps/index.html#interactive-maps-with-echarts4r",
    "href": "posts/2025-10-30-interactive-maps/index.html#interactive-maps-with-echarts4r",
    "title": "Interactive maps (R)",
    "section": "interactive maps with {echarts4r}",
    "text": "interactive maps with {echarts4r}\n\ndf &lt;- gapminder %&gt;% \n    mutate(Name = recode_factor(country,\n                              `Congo, Dem. Rep.`= \"Dem. Rep. Congo\",\n                              `Congo, Rep.`= \"Congo\",\n                              `Cote d'Ivoire`= \"Côte d'Ivoire\",\n                              `Central African Republic`= \"Central African Rep.\",\n                              `Yemen, Rep.`= \"Yemen\",\n                              `Korea, Rep.`= \"Korea\",\n                              `Korea, Dem. Rep.`= \"Dem. Rep. Korea\",\n                              `Czech Republic`= \"Czech Rep.\",\n                              `Slovak Republic`= \"Slovakia\",\n                              `Dominican Republic`= \"Dominican Rep.\",\n                              `Equatorial Guinea`= \"Eq. Guinea\"))\n\ndf %&gt;% group_by(year) %&gt;%\n  e_chart(Name, timeline = TRUE) %&gt;%\n  e_map(lifeExp) %&gt;%\n  e_visual_map(\n    min = 30, max = 90, type = \"piecewise\"\n  ) %&gt;%\n    e_title(\"Life expectancy by country and year\", left = \"center\") %&gt;%\n    e_tooltip(\n      trigger = \"item\", formatter = e_tooltip_choro_formatter()\n    )"
  }
]